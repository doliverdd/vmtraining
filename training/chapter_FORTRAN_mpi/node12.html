<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2012 (1.2)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>All to one</TITLE>
<META NAME="description" CONTENT="All to one">
<META NAME="keywords" CONTENT="chapter_FORTRAN_mpi">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2012">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="chapter_FORTRAN_mpi.css">

<LINK REL="next" HREF="node13.html">
<LINK REL="previous" HREF="node11.html">
<LINK REL="up" HREF="node10.html">
<LINK REL="next" HREF="node13.html">
</HEAD>

<BODY >
<!--Navigation Panel-->
<A NAME="tex2html203"
  HREF="node13.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="file:/usr/share/latex2html/icons/next.png"></A> 
<A NAME="tex2html201"
  HREF="node10.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="file:/usr/share/latex2html/icons/up.png"></A> 
<A NAME="tex2html195"
  HREF="node11.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="file:/usr/share/latex2html/icons/prev.png"></A>   
<BR>
<B> Next:</B> <A NAME="tex2html204"
  HREF="node13.html">Scatter and Gather</A>
<B> Up:</B> <A NAME="tex2html202"
  HREF="node10.html">Collective Communications</A>
<B> Previous:</B> <A NAME="tex2html196"
  HREF="node11.html">One to All</A>
<BR>
<BR>
<!--End of Navigation Panel-->

<H2><A NAME="SECTION00132000000000000000"></A>
<A NAME="subsection:reduce"></A>
<BR>
All to one
</H2> So you are probably thinking that if MPI
has an efficient method of broadcasting from one process to all of
the others that there should be some way of collecting data from
all of the nodes into one.  And there are several. The first ones
that we will look at involve reduction operations.  For example
suppose that you have a value on each processor and that you
wanted to get a global sum of these values.  If we send all of the
data to the root node by using individual calls we have created
several bottlenecks in the program. The communications must be
received sequentially at the root node.  Also, once the data is
collected the other processors must wait until the root processor
is finished with the calculation before they can proceed. (If they
need the result.)  The function MPI_Reduce takes care of the
first of these problem.

<P>
<PRE>
    CALL MPI_REDUCE(sendbuf,recvbuf,icount,
  &amp; MPI_Datatype,MPI_Op,iroot,MPI_Comm,ierr)
    integer icount,iroot,ierr
</PRE>

<P>
Lets take a look at an example. Assume that all of the variables
are defined appropriately.
<PRE>
    CALL MPI_REDUCE(val,total,1,MPI_INTEGER,
  &amp; MPI_SUM,0,MPI_COMM_WORLD,ierr)
</PRE>

<P>
Each process calls MPI_Reduce with the same parameters.  Even
though the sum is accumulated only on process 0. After the call
above, the variable total will contain the global sum.  What if
you want to do some other operation than summation? MPI predefines
several operations for the parameter MPI_Op. It can take on the
following values.
<DL>
<DT><STRONG>MPI_MAX</STRONG></DT>
<DD>maximum
    
</DD>
<DT><STRONG>MPI_MIN</STRONG></DT>
<DD>minimum
    
</DD>
<DT><STRONG>MPI_SUM</STRONG></DT>
<DD>sum
    
</DD>
<DT><STRONG>MPI_PROD</STRONG></DT>
<DD>product
    
</DD>
<DT><STRONG>MPI_LAND</STRONG></DT>
<DD>logical and
    
</DD>
<DT><STRONG>MPI_BAND</STRONG></DT>
<DD>bit-wise and
    
</DD>
<DT><STRONG>MPI_LOR</STRONG></DT>
<DD>logical or
    
</DD>
<DT><STRONG>MPI_BOR</STRONG></DT>
<DD>bit-wise or
    
</DD>
<DT><STRONG>MPI_LXOR</STRONG></DT>
<DD>logical xor
    
</DD>
<DT><STRONG>MPI_BXOR</STRONG></DT>
<DD>bit-wise xor
    
</DD>
<DT><STRONG>MPI_MAXLOC</STRONG></DT>
<DD>max value and location of maximum
    
</DD>
<DT><STRONG>MPI_MINLOC</STRONG></DT>
<DD>min value and location of minimum
</DD>
</DL>

<P>
The operators MPI_MAXLOC and MPI_MINLOC have predefined MPI
datatypes to be able to return the value and the location.

<P>
While MPI_Reduce solves the problem of efficiently communicating
the data to the root node it leaves open the question of how to
distribute the result if it is needed on all of the processors. We
could follow the call to MPI_Reduce with a call to MPI_Bcast,
but this isn't the most efficient method. Instead MPI uses a
communication structure called the <I>butterfly</I> which
communicates the results of partial operations in such a way that
at the end of the call all of the processors have the result.  The
details of this communication scheme are beyond the scope of these
notes, but it should suffice at this stage to just assume that for
doing one of the global reduction operations and having the result
be available to all of the processors, MPI_Allreduce is the
function of choice. The definition of MPI_Allreduce is given
below.

<P>
<PRE>
    call MPI_ALLREDUCE(sendbuf,recvbuf,icount,
  &amp;     MPI_Datatype,MPI_Op,MPI_Comm,ierr)
    integer icount,ierr
</PRE>

<P>
<HR>
<!--Navigation Panel-->
<A NAME="tex2html203"
  HREF="node13.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="file:/usr/share/latex2html/icons/next.png"></A> 
<A NAME="tex2html201"
  HREF="node10.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="file:/usr/share/latex2html/icons/up.png"></A> 
<A NAME="tex2html195"
  HREF="node11.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="file:/usr/share/latex2html/icons/prev.png"></A>   
<BR>
<B> Next:</B> <A NAME="tex2html204"
  HREF="node13.html">Scatter and Gather</A>
<B> Up:</B> <A NAME="tex2html202"
  HREF="node10.html">Collective Communications</A>
<B> Previous:</B> <A NAME="tex2html196"
  HREF="node11.html">One to All</A>
<!--End of Navigation Panel-->
<ADDRESS>
root
2015-12-02
</ADDRESS>
</BODY>
</HTML>
