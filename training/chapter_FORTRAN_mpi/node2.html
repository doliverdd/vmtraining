<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2012 (1.2)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>A first program</TITLE>
<META NAME="description" CONTENT="A first program">
<META NAME="keywords" CONTENT="chapter_FORTRAN_mpi">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2012">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="chapter_FORTRAN_mpi.css">

<LINK REL="next" HREF="node6.html">
<LINK REL="previous" HREF="node1.html">
<LINK REL="up" HREF="node1.html">
<LINK REL="next" HREF="node3.html">
</HEAD>

<BODY >
<!--Navigation Panel-->
<A NAME="tex2html98"
  HREF="node3.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="file:/usr/share/latex2html/icons/next.png"></A> 
<A NAME="tex2html96"
  HREF="node1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="file:/usr/share/latex2html/icons/up.png"></A> 
<A NAME="tex2html90"
  HREF="node1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="file:/usr/share/latex2html/icons/prev.png"></A>   
<BR>
<B> Next:</B> <A NAME="tex2html99"
  HREF="node3.html">Compiling and running MPI</A>
<B> Up:</B> <A NAME="tex2html97"
  HREF="node1.html">Introduction to MPI Programming</A>
<B> Previous:</B> <A NAME="tex2html91"
  HREF="node1.html">Introduction to MPI Programming</A>
<BR>
<BR>
<!--End of Navigation Panel-->

<H1><A NAME="SECTION00110000000000000000"></A>
<A NAME="section:afp"></A>
<BR>
A first program
</H1> We will start with the traditional "Hello
World" program.  This will allow us to take a detailed look at the
minimal code necessary to run an MPI program. The code for this
and the other examples in this chapter are included in the bccode
directory on the lab machines.
<PRE>
    PROGRAM HELLOWORLD

    INCLUDE 'mpif.h'

    integer my_rank
    integer NPROCS
    integer source
    integer dest
    integer tag
    character*100 message
    character*10  rank
    integer status(MPI_STATUS_SIZE)
    integer ierr

c   Start MPI
    CALL MPI_Init(ierr)

c   Find out process rank
    CALL MPI_Comm_rank(MPI_COMM_WORLD, my_rank,ierr)

c   Find out the number of processes
    CALL MPI_Comm_size(MPI_COMM_WORLD, NPROCS,ierr)

    if (my_rank.ne.0) then

c   Create Message
    write(rank,100) my_rank
100 format(I1)
    message = 'Hello from process ' //  rank
  &amp;             // '!'
    dest = 0
    tag = 0
    CALL MPI_SEND(message, 100, MPI_CHARACTER,
  &amp;             dest, tag, MPI_COMM_WORLD, ierr)
    else
    do 200 source = 1, NPROCS-1
        tag = 0
    CALL MPI_RECV(message, 100, MPI_CHARACTER,
  &amp;     source,tag, MPI_COMM_WORLD, status, ierr)
    write(*,*) message
200 continue
    endif

c   Shutdown MPI
    CALL MPI_FINALIZE(IERR)
    end
</PRE>
In Fortran, MPI programs require the include file mpif.h. Status is
an array of integers of length MPI_STATUS_SIZE which is defined
in the file mpif.h. It is used to return the status of the MPI
communication subroutines. Information on MPI_SOURCE, MPI_TAG,
and MPI_ERROR is obtainable from Status. For now, just think of
the status variable as the place where MPI puts the error codes
for its subroutines.

<P>
Now we will start looking at the MPI subroutines used in this
program. The first is MPI_INIT which has the following
definition:
<PRE>
    CALL MPI_INIT(ierr)
    integer ierr
</PRE>
 MPI_INIT must be the first MPI  call in your program, as it initializes the state of the
program for all other MPI calls. Although the call to MPI_INIT
allows the passing of the command line parameters, their use is
not defined in the MPI standard. This means that using them is not
necessarily portable, and that they should be used with caution.
Their use is beyond the scope of this class. Also note that for
most installations, you can get information on the MPI functions
by using the man command.

<P>
With the next call we begin to delve into some of the mechanisms
of MPI programming.

<P>
<PRE>
c   Find out process rank
    CALL MPI_COMM_RANK(MPI_COMM_WORLD,my_rank,ierr)

c   Find out the number of processes
    CALL MPI_COMM_SIZE(MPI_COMM_WORLD,NPROCS,ierr)
</PRE>

<P>
Both MPI_COMM_RANK and MPI_COMM_SIZE have as their first
parameter the constant MPI_COMM_WORLD, which is of type
MPI_COMM. An MPI_COMM is the data type used to reference a <I>communicator</I>. A communicator <A NAME="17"></A> is a collection
of processes. As your programs and the underlying tasks become
more complicated, you may need to set up different groups of
processors to accomplish different tasks and may need to create
your own communicators. MPI by default sets up the global
communicator MPI_COMM_WORLD.  The MPI_COMM_RANK command
returns the process number or "rank" within the communicator.
MPI_COMM_SIZE puts the size of the given communicator into the
location pointed to by the second parameter.  The rank of the
process and the communicator's size are often used to control the
local flow of the program. We can see this in the next code
segment.

<P>
<PRE>
    if (my_rank.ne.0) then
c   Create Message
    write(rank,100) my_rank
100     format(I1)
    message = 'Hello from process ' //  rank
  &amp;             // '!'
    dest = 0
    tag = 0
    CALL MPI_SEND(message, 100, MPI_CHARACTER,
  &amp;             dest, tag, MPI_COMM_WORLD, ierr)
    else
    do 200 source = 1, NPROCS-1
        tag = 0
    CALL MPI_RECV(message, 100, MPI_CHARACTER,
  &amp;     source,tag, MPI_COMM_WORLD, status, ierr)
    write(*,*) message
</PRE>

<P>
In this snippet we see two very common MPI constructions.  First
the <I>if .. else</I> construction where one (or more) process(es)
are executing one set of instructions, and another set of
processes are executing another. In this case we see that each
process other than the root process (rank = 0) is sending a
message.  Meanwhile, the root process is receiving messages in a
DO loop which runs from 1 to the size of the communicator. Note
that there is nothing in this code that forces an order on the
communications from the non-root processes.  Rather they will each
run the code asynchronously.  One might ask how is it that the
root process is able to receive the messages in the correct order?
To answer that question, we must take a more in depth look at the
<I>MPI_RECV</I> and <I>MPI_SEND</I> commands. This is the subject
of the next section, but we will take a brief look at these
commands here, paying particular attention to the parameter types
and what they mean.

<P>
The MPI_Send command has the definition:
<PRE>
    call MPI_SEND (buf,icount,MPI_Datatype,idest,
  &amp;                  itag,MPI_COMM,ierr)
    integer icount,idest,itag,ierr
</PRE>

<P>
<DL>
<DT><STRONG>buf</STRONG></DT>
<DD>This is initial address of the send buffer, in our case it is
the beginning of the string message.
   
</DD>
<DT><STRONG>icount</STRONG></DT>
<DD>number of elements in send buffer. The count is not the number of bytes
but number of items.  This allows more complex data types to be
handled easily, and maintains portability.
   
</DD>
<DT><STRONG>datatype</STRONG></DT>
<DD>Datatype of each send buffer element.  Note that this is
not a standard Fortran datatype but one of the data types defined
by MPI. Although MPI has many datatypes and functions for
manipulating them, our examples will draw from the basic types
given in the following list.
   
<OL>
<LI>MPI_CHARACTER
</LI>
<LI>MPI_REAL
</LI>
<LI>MPI_INTEGER
</LI>
<LI>MPI_DOUBLE_PRECISION
</LI>
<LI>MPI_LOGICAL
</LI>
<LI>MPI_REAL8
</LI>
<LI>MPI_REAL4
</LI>
<LI>MPI_INTEGER4
</LI>
<LI>MPI_BYTE
</LI>
<LI>MPI_PACKED
</LI>
<LI>MPI_COMPLEX
</LI>
<LI>MPI_DOUBLE_COMPLEX
   
</LI>
</OL>
   
</DD>
<DT><STRONG>idest</STRONG></DT>
<DD>Rank of the destination
   
</DD>
<DT><STRONG>itag</STRONG></DT>
<DD>The use of the message tag is left to the program. It may be
used to classify messages
   
</DD>
<DT><STRONG>comm</STRONG></DT>
<DD>Communicator
   
</DD>
<DT><STRONG>ierr</STRONG></DT>
<DD>MPI error number (0=no error)
</DD>
</DL>

<P>
The MPI_RECV has the  definition:
<PRE>
    call MPI_RECV (buf,icount,MPI_Datatype,isource,
 &amp;                   itag,MPI_COMM,istatus,ierr)
    integer  icount,isource,itag,ierr
</PRE>
Let's take a look at the differences between MPI_SEND and
MPI_RECV. The <I>buf</I> in the MPI_RECV call is the initial
address of the receive buffer instead of the send buffer.  We have
replaced idest or the destination of the message with the source
of the message. MPI_RECV also returns a Status variable as
described above.

<P>
With just a little more information we can now answer the
question, how does the root process receive the messages in order.
MPI buffers messages. Although MPI does not guarantee that the
messages from different processes arrive in any specific order,
the root process reads them  from the buffer in processor number
order. MPI does however ensure that messages from the same
processor do arrive in order.  We will explore other types of
point-to-point communications and their details in the following
section.

<P>
<BR><HR>
<!--Table of Child-Links-->
<A NAME="CHILD_LINKS"><STRONG>Subsections</STRONG></A>

<UL>
<LI><A NAME="tex2html100"
  HREF="node3.html">Compiling and running MPI programs</A>
<LI><A NAME="tex2html101"
  HREF="node4.html">A note on output</A>
<LI><A NAME="tex2html102"
  HREF="node5.html">Exercises</A>
</UL>
<!--End of Table of Child-Links-->
<HR>
<!--Navigation Panel-->
<A NAME="tex2html98"
  HREF="node3.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="file:/usr/share/latex2html/icons/next.png"></A> 
<A NAME="tex2html96"
  HREF="node1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="file:/usr/share/latex2html/icons/up.png"></A> 
<A NAME="tex2html90"
  HREF="node1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="file:/usr/share/latex2html/icons/prev.png"></A>   
<BR>
<B> Next:</B> <A NAME="tex2html99"
  HREF="node3.html">Compiling and running MPI</A>
<B> Up:</B> <A NAME="tex2html97"
  HREF="node1.html">Introduction to MPI Programming</A>
<B> Previous:</B> <A NAME="tex2html91"
  HREF="node1.html">Introduction to MPI Programming</A>
<!--End of Navigation Panel-->
<ADDRESS>
root
2015-12-02
</ADDRESS>
</BODY>
</HTML>
