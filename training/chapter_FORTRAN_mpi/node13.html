<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2012 (1.2)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Scatter and Gather</TITLE>
<META NAME="description" CONTENT="Scatter and Gather">
<META NAME="keywords" CONTENT="chapter_FORTRAN_mpi">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2012">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="chapter_FORTRAN_mpi.css">

<LINK REL="next" HREF="node14.html">
<LINK REL="previous" HREF="node12.html">
<LINK REL="up" HREF="node10.html">
<LINK REL="next" HREF="node14.html">
</HEAD>

<BODY >
<!--Navigation Panel-->
<A NAME="tex2html213"
  HREF="node14.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="file:/usr/share/latex2html/icons/next.png"></A> 
<A NAME="tex2html211"
  HREF="node10.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="file:/usr/share/latex2html/icons/up.png"></A> 
<A NAME="tex2html205"
  HREF="node12.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="file:/usr/share/latex2html/icons/prev.png"></A>   
<BR>
<B> Next:</B> <A NAME="tex2html214"
  HREF="node14.html">Exercises</A>
<B> Up:</B> <A NAME="tex2html212"
  HREF="node10.html">Collective Communications</A>
<B> Previous:</B> <A NAME="tex2html206"
  HREF="node12.html">All to one</A>
<BR>
<BR>
<!--End of Navigation Panel-->

<H2><A NAME="SECTION00133000000000000000"></A>
<A NAME="subsection:scattergather"></A>
<BR>
Scatter and Gather
</H2> We have looked at collective
communications that take the same data and communicate it to all
processors, and those that reduce data from all processes to one.
But these communication functions will not allow us to send different
information to each process.  Nor will they allow us to collect
separate bits of information from all of the processes onto one.
For these tasks we can use MPI_SCATTER and MPI_GATHER.

<P>
<PRE>
    CALL MPI_SCATTER(SNDBUF,ISCOUNT,MPI_DATATYPE,
  &amp; RECVBUF,IRCOUNT,MPI_DATATYPE,IROOT,MPI_COM,
  &amp;                  IERR)
    integer  iscount,ircount,iroot,ierr

    CALL MPI_GATHER(SNDBUF,ISCOUNT,MPI_DATATYPE,
  &amp; RECVBUF,IRCOUNT,MPI_DATATYPE,IROOT,MPI_COM,
  &amp;                 IERR)
    integer  iscount,ircount,iroot,ierr
</PRE>

<P>
The meanings of these parameters are similar to previous commands
that we have studied.  The iscount is usually the same as the
ircount, and the send datatype is usually the same as the receive
datatype. We can look at the following example to see how
MPI_Scatter works.

<P>
<PRE>
        PROGRAM SCATTER
        INCLUDE 'mpif.h'

        real vec
        dimension vec(4)
        CALL MPI_INIT(ierr)
        CALL MPI_COMM_SIZE(MPI_COMM_WORLD,NPROCS,
     &amp;                      IERR)

        CALL MPI_COMM_RANK(MPI_COMM_WORLD,MYRANK,
     &amp;                       IERR)

        if (myrank.eq.0) then
        do i=0,3
        vec(i+1)=i*2.0
        enddo
        endif
        CALL MPI_SCATTER(vec,1,MPI_INTEGER,vec,1,
     &amp;    MPI_INTEGER,0,MPI_COMM_WORLD,IERR)

        write(*,*) vec(1),myrank
        CALL    MPI_FINALIZE(ierr)
        end
</PRE>

<P>
This code will distribute the each of the entries of vector vec to
the processes in MPI_COMM_WORLD.  Figure <A HREF="#fig:scat">8.2</A> shows
the effects of the MPI_Scatter command on the memory for each
process.

<P>

<DIV ALIGN="CENTER"><A NAME="fig:scat"></A><A NAME="109"></A>
<TABLE>
<CAPTION ALIGN="BOTTOM"><STRONG>Figure 8.2:</STRONG>
The effects of MPI_Scatter</CAPTION>
<TR><TD></TD></TR>
</TABLE>
</DIV>
MPI_Gather is in some sense the inverse operation to
MPI_Scatter.  Instead of distributing values, it gathers them
together.  The following code and Figure <A HREF="#fig:gather">8.3</A> show the
effects of the MPI_Gather operation.

<P>
<PRE>
        PROGRAM GATHER
        INCLUDE 'mpif.h'

        real vec
        dimension vec(4)
        CALL MPI_INIT(ierr)
        CALL MPI_COMM_SIZE(MPI_COMM_WORLD,
    &amp;                        NPROCS,IERR)

        CALL MPI_COMM_RANK(MPI_COMM_WORLD,
    &amp;                        MYRANK,IERR)

        if (myrank.ne.0) vec(1)= myrank*2.0

        CALL MPI_GATHER(vec,1,MPI_INTEGER,vec,1,
     &amp;    MPI_INTEGER,0,MPI_COMM_WORLD,IERR)

        if (myrank.eq.0) then
        do i=0,3
        write(*,*) vec(i+1)
        enddo
        endif
        CALL  MPI_FINALIZE(ierr)
        end
</PRE>

<P>

<DIV ALIGN="CENTER"><A NAME="fig:gather"></A><A NAME="117"></A>
<TABLE>
<CAPTION ALIGN="BOTTOM"><STRONG>Figure 8.3:</STRONG>
The effects of MPI_Gather</CAPTION>
<TR><TD></TD></TR>
</TABLE>
</DIV>
<HR>
<!--Navigation Panel-->
<A NAME="tex2html213"
  HREF="node14.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="file:/usr/share/latex2html/icons/next.png"></A> 
<A NAME="tex2html211"
  HREF="node10.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="file:/usr/share/latex2html/icons/up.png"></A> 
<A NAME="tex2html205"
  HREF="node12.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="file:/usr/share/latex2html/icons/prev.png"></A>   
<BR>
<B> Next:</B> <A NAME="tex2html214"
  HREF="node14.html">Exercises</A>
<B> Up:</B> <A NAME="tex2html212"
  HREF="node10.html">Collective Communications</A>
<B> Previous:</B> <A NAME="tex2html206"
  HREF="node12.html">All to one</A>
<!--End of Navigation Panel-->
<ADDRESS>
root
2015-12-02
</ADDRESS>
</BODY>
</HTML>
