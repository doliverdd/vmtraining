<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2012 (1.2)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>A first program</TITLE>
<META NAME="description" CONTENT="A first program">
<META NAME="keywords" CONTENT="chapter_d_C">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2012">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="chapter_d_C.css">

<LINK REL="next" HREF="node6.html">
<LINK REL="previous" HREF="node1.html">
<LINK REL="up" HREF="node1.html">
<LINK REL="next" HREF="node3.html">
</HEAD>

<BODY >
<!--Navigation Panel-->
<A NAME="tex2html76"
  HREF="node3.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="file:/usr/share/latex2html/icons/next.png"></A> 
<A NAME="tex2html74"
  HREF="node1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="file:/usr/share/latex2html/icons/up.png"></A> 
<A NAME="tex2html68"
  HREF="node1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="file:/usr/share/latex2html/icons/prev.png"></A>   
<BR>
<B> Next:</B> <A NAME="tex2html77"
  HREF="node3.html">Compiling and running MPI</A>
<B> Up:</B> <A NAME="tex2html75"
  HREF="node1.html">Introduction to MPI Programming</A>
<B> Previous:</B> <A NAME="tex2html69"
  HREF="node1.html">Introduction to MPI Programming</A>
<BR>
<BR>
<!--End of Navigation Panel-->

<H1><A NAME="SECTION00110000000000000000"></A>
<A NAME="section:afp"></A>
<BR>
A first program
</H1>
We will start with the traditional "Hello World" program.  This will allow us
to take a detailed look at the minimal code necessary to run an MPI program.
The code for this and the other examples in this chapter are include in the 
bccode directory on the lab machines.  
<PRE>
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;mpi.h&gt;

main(int argc,char* argv[]){
	
	int				my_rank;
	int				p;
	int				source;
	int				dest;
	int				tag=0;
	char			message[100];
	MPI_Status		status;

	/*Start MPI*/
	MPI_Init(&amp;argc,&amp;argv);

	/*Find out process rank*/
	MPI_Comm_rank(MPI_COMM_WORLD,&amp;my_rank);

	/*Find out the number of processes*/
	MPI_Comm_size(MPI_COMM_WORLD,&amp;p);

	if(my_rank !=0){
		/* Create Message */
		sprintf(message,"Hello from process %d!",my_rank);
		dest = 0;
		/*Use strlen+1 so that '\0' gets transmitted */
		MPI_Send(message,strlen(message)+1,
					 MPI_CHAR,dest,tag,MPI_COMM_WORLD);
	}else{
		for (source =1;source&lt;p;source++){
			MPI_Recv(message,100,MPI_CHAR,
				  source,tag,MPI_COMM_WORLD,&amp;status);
			printf("%s\n",message);
		}
	}

	/* Shutdown MPI */
	MPI_Finalize();

}
</PRE>
In C MPI programs require the include file mpi.h.
MPI defines many internal datatypes, and the first one used in this program
that is <I>MPI_Status</I>. In C MPI_Status is a structure
that contains the fields MPI_SOURCE, MPI_TAG, and MPI_ERROR. It is used to 
return the status of the MPI communications functions. 
For now just think of the status variable as the place where MPI puts the 
error codes for its functions.

<P>
Now we will start looking at the MPI functions used in this program.  The first
is MPI_Init which has the following definition:

<P>
<code>int MPI_Init(int *argc, char ***argv)</code>

<P>
MPI_Init must be the first MPI function call in your program, as it initializes
the state of the program for all other MPI calls.  Although the call to 
MPI_Init allows the passing of the command line parameters, their use is not
defined in the MPI standard.  This means that using them is not necessarily 
portable, and that they should be used with caution.  Their use is beyond the
scope of this class. Also note that for most installations, you can get 
information on the MPI functions by using the man command.

<P>
With the next call we begin to delve into some of the mechanisms of MPI 
programming.  

<P>
<PRE>
   /*Find out process rank*/
   MPI_Comm_rank(MPI_COMM_WORLD,&amp;my_rank);

   /*Find out the number of processes*/
   MPI_Comm_size(MPI_COMM_WORLD,&amp;p);
</PRE>

<P>
Both MPI_Comm_rank and MPI_Comm_size have as their first parameter 
the constant MPI_COMM_WORLD, which is of type MPI_Comm. 
An MPI_Comm is the data type used to reference a <I>communicator</I>.  
A communicator <A NAME="16"></A> 
is a collection of processes.  As your programs and the underlying tasks 
become more complicated, you may need to set up different groups of processors
to accomplish different tasks and may need to create your own communicators.  
MPI by default sets up the 
global communicator MPI_COMM_WORLD.  The MPI_Comm_rank command returns the 
process number or "rank" within the communicator. 
MPI_Comm_size puts the size of the given communicator into the
location pointed to by the second parameter.  The rank of the process and the 
communicator's size are often used to control the local flow of the program.
We can see this in the next code segment.

<P>
<PRE>
 
if(my_rank !=0){
   /* Create Message */
   sprintf(message,"Hello from process %d!",my_rank);
   dest = 0;
   /*Use strlen+1 so that '\0' gets transmitted */
   MPI_Send(message,strlen(message)+1,
			MPI_CHAR,dest,tag,MPI_COMM_WORLD);
}else{
   for (source =1;source&lt;p;source++){
	  MPI_Recv(message,100,MPI_CHAR,
		 source,tag,MPI_COMM_WORLD,&amp;status);
	  printf("%s\n",message);
   }
}
</PRE>

<P>
In this snippet we see two very common MPI constructions.  First the <I>if .. else</I> construction where one (or more) process(es) are executing one
set of instructions, and another set of processes are executing another.  
In this case we see that each process other than the root process (rank = 0)
is sending a message.  Meanwhile, the root process is receiving messages
in a for loop which runs from 1 to the size of the communicator.  
Note that there is nothing in this code that forces an order on the 
communications from the non-root processes.  Rather they will each run the
code asynchronously.  One might ask how is it that the root process is able
to receive the messages in the correct order?  To answer that question, we
must take a more in depth look at the <I>MPI_Recv</I> and <I>MPI_Send</I>
commands. This is the subject of the next section, but we will take a brief
look at these commands here, paying particular attention to the parameter 
types and what they mean.

<P>
The MPI_Send command has the definition:
<PRE>
int MPI_Send( void *buf, int count, 
			   MPI_Datatype datatype, int dest,
			   int tag, MPI_Comm comm )
</PRE>

<P>
<DL>
<DT><STRONG>buf</STRONG></DT>
<DD>This is initial address of the send buffer, in our case it is
the beginning of the string message.  
   
</DD>
<DT><STRONG>count</STRONG></DT>
<DD>number of elements in send buffer. We set this to the length
of the string message +1 to account for the null terminator character. The 
count is not the number of bytes but number of items.  This allows more
complex data types to be handled easily, and maintains portability.
   
</DD>
<DT><STRONG>datatype</STRONG></DT>
<DD>Datatype of each send buffer element.  Note that this is
not a standard C datatype but one of the data types defined by MPI. Although
MPI has many datatypes and functions for manipulating them, our examples will
draw from the basic types given in the following list.
   
<OL>
<LI>MPI_CHAR
</LI>
<LI>MPI_SHORT
</LI>
<LI>MPI_INT
</LI>
<LI>MPI_LONG
</LI>
<LI>MPI_UNSIGNED_CHAR
</LI>
<LI>MPI_UNSIGNED_SHORT
</LI>
<LI>MPI_UNSIGNED
</LI>
<LI>MPI_UNSIGNED_LONG
</LI>
<LI>MPI_FLOAT
</LI>
<LI>MPI_DOUBLE
</LI>
<LI>MPI_LONG_DOUBLE
</LI>
<LI>MPI_BYTE
</LI>
<LI>MPI_PACKED
   
</LI>
</OL>
   
</DD>
<DT><STRONG>dest</STRONG></DT>
<DD>Rank of the destination
   
</DD>
<DT><STRONG>tag</STRONG></DT>
<DD>The use of the message tag is left to the program. It may be 
used to classify messages 
   
</DD>
<DT><STRONG>comm</STRONG></DT>
<DD>Communicator 
</DD>
</DL>

<P>
The MPI_Recv has the definition:
<PRE>
int MPI_Recv( void *buf, int count, 
			   MPI_Datatype datatype, int source, int tag, 
			   MPI_Comm comm, MPI_Status *status )
</PRE> 

<P>
Let's take a look at the differences between MPI_Send and MPI_Recv.
The <I>buf</I> in the MPI_Recv call is the initial address of the receive buffer 
instead of the send buffer.  We have replaced dest or the destination of the
message with the source of the message.  MPI_Recv also returns a MPI_Status
variable as described above.  

<P>
With just a little more information we can now answer the question, how does 
the root process receive the messages in order. MPI buffers messages. Although
MPI does not guarantee that the messages from different processes arrive 
in any specific order, the root process reads them  from the buffer in 
processor number order.
MPI does however insure that messages from the same processor do
arrive in order.  We will explore other types of point-to-point
communications and their details in the following section.

<P>
<BR><HR>
<!--Table of Child-Links-->
<A NAME="CHILD_LINKS"><STRONG>Subsections</STRONG></A>

<UL>
<LI><A NAME="tex2html78"
  HREF="node3.html">Compiling and running MPI programs</A>
<LI><A NAME="tex2html79"
  HREF="node4.html">A note on output</A>
<LI><A NAME="tex2html80"
  HREF="node5.html">Exercises</A>
</UL>
<!--End of Table of Child-Links-->
<HR>
<!--Navigation Panel-->
<A NAME="tex2html76"
  HREF="node3.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="file:/usr/share/latex2html/icons/next.png"></A> 
<A NAME="tex2html74"
  HREF="node1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="file:/usr/share/latex2html/icons/up.png"></A> 
<A NAME="tex2html68"
  HREF="node1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="file:/usr/share/latex2html/icons/prev.png"></A>   
<BR>
<B> Next:</B> <A NAME="tex2html77"
  HREF="node3.html">Compiling and running MPI</A>
<B> Up:</B> <A NAME="tex2html75"
  HREF="node1.html">Introduction to MPI Programming</A>
<B> Previous:</B> <A NAME="tex2html69"
  HREF="node1.html">Introduction to MPI Programming</A>
<!--End of Navigation Panel-->
<ADDRESS>
root
2015-12-02
</ADDRESS>
</BODY>
</HTML>
