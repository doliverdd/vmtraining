
end enumerate
<p>



<p>Working with the Command Line Environment
As we noted at the beginning of the last <p>, the standard interface to 
Linux systems is the command line interface. There are pluses and minuses 
for both the command line and GUI environments.  In this <p> we will 
explore two of the advantages.  The first is that the 
configuration of the environment is very flexible.  This is accomplished by 
setting the shell it environment variables. indexenvironment variables 
Not only can this facility make the system more usable, certain software 
packages require that environment variables are set correctly.
Another advantage we will be looking at is the use of shell scripts. 
indexshell scripts These are  small easy to write programs that allow you 
to do repetitive tasks by calling a simple command.  Submission scripts to 
the queuing software on many systems are often shell scripts, which makes an
ability to manipulate them (at least at a rudimentary level) necessary within
the context of HPC and HTC. s ref:EV and ref:shell_scripts
introduce you to these topics.  
Setting up your environment
label:EV
As noted above one of the advantages to a command line environment is the 
ease with which that environment can be manipulated to make computing tasks
easier.  How the environment is manipulated is determined by 
which shell you are choose.
There are several shells that may be picked to use as your interface such as,
bash, csh, tcsh, ksh, and zsh.  The two most common 
shells are bash and csh. Over
the past several years these shells have started incorporating some of each 
others functionality.  tcsh is a shell that is very similar to csh with much
of the bash functionality.  In this  we will be working exclusively
with bash.  This is not just a matter
of preference, but based on the fact that many pre-existing scripts are written
in bash's predecessor sh.  This means that learning the bash conventions in 
this  will help us in  ref:shell_scripts. 
<p>
<br> Bash Internal Commands
Bash has built-in commands that make using the environment easier.  In this  we will discuss several of the most common of these commands. This is
by no means a complete listing.  You can (and should) use "man bash" to get
more information.
begindescription
<br> [alias] indexaliasThe alias command allows you
to create a short hand or easy mnemonic for longer commands.  For example, one
repetitive task that most people need is to find out which processes they have
running.  To do this, one can issue a ps command and then 
search for his/her own processes.  For user frodo, this could be made easier 
by "piping" the output of the ps command to the grep command 
(See  ref<br>:rap) by typing :
<p>
it ps auxw  |<br> grep frodo.  
<p>
If frodo had to do this several times it might become tiresome. Issuing the 
command :
<p>
it alias psf="ps auxw  |<br>  grep frodo" 
<p>
would allow frodo to get the same results by typing:
<p>
it psf.
This command could be made part of frodo's environment by adding it to his
.bash_profile.  (See  ref<br>:bcf)
<br> [source] indexsourceSource reads and executes commands from a file.  
This command is particularly useful when  you are making changes to your 
bash configuration files and you want to use them in your current session. For 
example, to test changes in your .bashrc file you would type it source .bashrc. 
(See  ref<br>:bcf)
<br> [bg,fg,rmchar94kern-1ptz] indexbgindexfgWhen a process is running in a bash shell, you can
type in rmchar94kern-1ptz (Control-z), and the process will be suspended.  If you type bg after this
the job/process will be put in background mode which will free the shell for 
other tasks.  If you later type fg, the job will be brought back into the
foreground for this shell.  There are more options for job control in the
bash shell which you can read about on the man page.
<br> [rmchar94kern-1ptc] Abort the current process.
<br> [echo] indexechoOutput the arguments. This is useful in shell scripting, and
in checking the value of the environment variables for example 
it echo $PATH will print out the current value of your PATH variable.
<br> [export] indexexportThe arguments are marked for automatic export 
to the environment of subsequently executed commands.  This is often used 
to set environment variables so that any process called by the shell 
will be able to reference them.  (See  ref<br>:bcf)
<br> [exit] indexexit The exit command causes the shell to quit.
<br> [history] indexhistory Displays a list of past commands.  This command has many options and can be used to rerun commands.  In most bash installations, commands can be rerun, and you can scroll through this list by using the up and down arrows.  
<br> [kill] indexkill The kill command sends a signal to a process. The most common use is it kill -9 pid which will kill the process with process number pid.
<br> [printenv] Display the current state of your environment variables.
<br> [pwd] indexpwd List the current directory.
enddescription
<br> Bash Configuration files
<br>:bcf
Most modifications of the environment may be accomplished through the use of 
two bash configuration files it .bash_profile and it .bashrc.(Note the
periods at the beginning.) The .bash_profile file is a  personal profile 
that is processed every time you login, and .bashrc is a setup file that 
is processed when you open a non-login shell. In practice most setups 
have .bash_profile call .bashrc when opening a login shell. 
<p>
Traditionally, the configuration files are used to set up a users environment 
so that it is the same each time they start a shell.  Remember that a shell is
in some ways nothing more than the interface to the OS.  They can also be used
to display system information, or run scripts every time the shell starts. 
<p>
As we noted before an alias is just a key short cut for an command that is 
long or often repeated.  It can also make your environment more user friendly. 
For example, the it cp command will by default copy over a file without 
a warning.  There is a flag for the cp command it cp -i that will cause
the shell to prompt before overwriting a file.  For most people this is a 
good idea, and we can put it in the .bashrc file the line to do this is 
simply type, newline it alias cp='cp -i'. 
<p>
Much of the control that you can gain over the environment involves setting 
it environment variables.  These variables are often referred to by programs
and the OS.  For example, we use the program PBS for queuing jobs
on our systems.  This program needs to know where its configuration files are,
so to use it easily, you must set the environment variable PBS to be the location
of these files.  One of the most important environment variables is PATH.
This is the list of directories that the OS searches when a command is
typed in.  It usually contains the standard system binary directories, for
example /usr/bin and /usr/local/bin, but may be modified by each user to point
to directories that they wish to use.  It also sets the order that these
directories are searched.  Suppose that I want to add the directory
/usr/lib/mpich/bin (where the MPI commands are found) to my path.  In 
.bash_profile  I can include either of the following sets of lines.

PATH=$PATH:/usr/lib/mpich/bin
export PATH

or

export PATH=$PATH:/usr/lib/mpich/bin

<p>
Another common variable to set determines the prompt.  To change the prompt in
the bash shell, you use the command: PS1="value". Value can be almost any
string, and may include the following codes that are often useful.
begin<br>ize
<br>  \d<br> - the date (day-of-the-week month day)
<br>  \h<br> - the hostname (without the domain)
<br>  \n<br> - start a new line
<br>  \u<br> - the current username
<br>  \w<br> - the working directory 
end<br>ize
<br> Redirection and piping
<br>:rap
On Linux systems almost all files, output and even devices are at their most
basic level it streams.indexstreams Streams are the way that the OS (or kernel) 
communicate with processes.  The details of this interface are (very far)
beyond the scope of this class, however, you can think of files being read
or written from a program as just a sequential "stream" of bytes.  All of the
input and output commands for files are just methods of controlling this stream.
One interesting aspect of working with streams is
that bash allows you to easily redirect the flow of these streams.  Before
we take a look at how this is done we need a few definitions.
begindescription
<br>[pipe] If you follow a command in Linux with the  |<br> character, the output
from that command will be fed into the input of the command that follows the
pipe character.
<br>[standard in or stdin] This is the name given to the input to a program.  
This defaults to the keyboard for most programs.
<br>[standard out or stdout] By default program output goes to this  
file.  For terminal programs this is usually defined to print back to the 
terminal.
<br>[standard error or stderr] This is a default file for printing errors the 
system produces from a process.  It can be redirected or added to by a program 
or user.
<br>[redirection] The symbols  &gt;,&gt;&gt;,&lt;<br> can be used to redirect a stream to
a file or device.  The most common use is to redirect the output of some 
program to a file.
enddescription
<p>
Let's look at some examples. The grep command allows you to find the occurrences
of a string in a file.  The simple form is "grep it string filename".  The 
command wc prints the byte, word, and line count of a file.  But what if you 
want to know how many lines in a file contain a certain word? Simple,
just run the command "grep it string file  |<br> wc".  In this command the 
output of grep is used as the input for wc.  (This is a trivial example, 
but adding the full capabilities of grep makes this combination very useful.)
<p>
The  &gt;<br>  redirects output to a file,  &lt;<br> redirects a file to the standard input of a program, and  &gt;&gt;<br> appends the output to the end of
a file.  For example, assume that you have a program proj1 that outputs some
data and you want to capture that data to use as input for program proj2,and you
want to keep a copy on the disk. (Note that "proj1  |<br> proj2" would 
not work since it 
would not store a copy of the output.) One way to accomplish this task would 
be to use the following commands.

frodo@mimir:proj1&gt; output
frodo@mimir:proj2&lt; output

If instead, you want to capture the output of multiple runs of proj1, you 
could replace the  &gt;<br> with  &gt;&gt;<br> in the first line so that the output 
from proj1 would be appended to the file output.
<br>Exercises
<br>:shellEx
<br>
<br> Change your prompt to be the date, working directory, and username 
followed by a $. Now add these changes to your .bash_profile to make them
permanent.
<br> Find out all of the aliases that are set in your shell.  Read the man
page on alias to find out how. 
<br> Some interesting bash functions.  If you have run at least a few
commands from your shell, try hitting the up and down arrows on the keyboard.
What is happening?  Now try to type the first couple of characters in 
the name of a file in your directory and then hit the tab key.  
<br> If you are used to DOS, you may have used the rename, del, and dir 
commands.  Set up aliases in your .bashrc file so that typing these
commands results in the appropriate actions.
<br> What is the current value of your PATH variable? Create a directory
bin in your home directory.  Now add this directory to your PATH in 
.bash_profile.  Check your PATH variable again. Is the new directory there?
If not, try using the command source to enable the changes that you made to 
.bash_profile.  Where in the PATH is your new directory? Can you put it at 
the end of the path? At the beginning?
<br> Try the who command. What does it show? Now suppose that you want to know
if root is logged on to the system.  Use the who,  |<br>, and grep commands
to only list occurrences of root being logged on.
<br> Advanced Exercise - Now suppose that you are a system administrator 
with a system that is having problems.  You want a quick command that will 
capture the time and date, and who was logged on. You  also want to log this
information to the file logon.log.  Use the touch
command to create the logon.log file.  If you want to run two commands at one
time in bash you can put a ; between them.  Run the date command followed by
the who command and redirect the output to the logon.log file.  In bash you
can rerun an earlier command by using the up and down arrows.  Try this to 
rerun the date/who command.  Run the cat command on the logon.log file.  Are
both listings there? Do you need to change the command so that you keep all of
the entries in the log file?
<br> Advanced Exercise - Compile and run the program HelloWorldError in
the code directory.  Now try running the program with the command

HelloWorldError &gt; output

Can you explain what happens? Read the man page on bash and find out how to 
redirect both standard out and standard error to a file, and then try what you
find with this program.

Shell Scripts
label:shell_scripts
Shell scripts can be a very powerful and useful tool in the Linux environment.
Experienced systems administrators can automate most of their configurations
using this type of programming.  Users will find that many repetitive tasks
are easily transformed into scripts.  Shell script programming has several 
advantages.  For small tasks it is generally quicker than writing 
higher level code and shell scripts can easily call on the power of built-in
functions.  Well written scripts are often trivial to modify, so future tasks 
are not hard to automate. Writing the script can be faster than manually 
performing the task, and once debugged, scripts less error prone than manually
performing the tasks. (That last caveat is very important, remember the 
old adage "To error is human, to really screw things up you need a computer")
<p>
Each shell has its own scripting language.  These are not feature
rich languages, but they have sufficient power for automating many tasks. We
have picked bash for this course  because its syntax is practically the same
as sh which is a shell that is on all Linux systems.  For the rest of this 
 when we refer to shell scripts, we will be specifically referring
to bash shell scripts. 
<p>
We will go over some of the basics of the bash scripting language, and then 
turn our attention to 
some practical examples and exercises. Before you try to run any of the
scripts, you must change the file permissions so that the script is executable.
Setting the script so that your user can read, write, and execute while others
have no access or can just read the script is a good starting place.  How
to do this is left as an exercise for the reader. All of the example scripts
are contained in the code directory. For convenience many of the bash operators
are listed in Appendix ref<p>:bashref.
<p>
<br>Some Simple Scripts

#!/bin/bash
export MY_NAME=frodo
echo Hello from $MY_NAME

While this script doesn't do very much, it contains enough of the basics to begin
discussing this type of programming.  One way
of thinking about shell scripts is that they exist to automate the running
of Linux commands.  To do that we need some of the usual programming 
constructions, setting variables, looping, if then else clauses, and I/O.  
While all of these things exist in shell scripts, they are somewhat more
rudimentary than in other languages, and the syntax may seem unusual.  Just 
remember that the lines in the shell script run just as if you had typed them
into the shell manually.  
<p>
The first line of the script invokes the shell that the script will use, in 
this case bash.  This should always be the first line of your scripts.
then we set a variable just as if we were setting an environment variable
in our shell.  In this case we use the syntax it export name=value, which
not only assigns the value to the variable, but also makes it available for
other programs that may be run from within this script.  The next line just 
executes that echo command with the new variable and some other text.
<p>
Variables in shell scripts are not typed.  That means that you can assign any
type of value to the variable at any time.  If you reference a variable that
has not been initialized, you get the null string.  That's all, the shell gives
no warnings or errors.  Note that just because variables are not typed doesn't
mean that you can add a number to an integer. You have to be careful to use 
assigned variables correctly.  
<p>
Now that we know how to get output from a shell script, let's take a look 
at how to get input.  To do this we use the read command, as in the script 
below.  This example also illustrates some intricacies of working
with variables. The  #<br> character is the comment character in shell scripts.
<p>

#!/bin/bash
export MY_NAME=frodo
echo Please tell me your name:
read USERNAME
echo Hello $USERNAME my name is $MY_NAME
#The next line split for space reasons
echo I will create you a file. Please tell me the 
		 name of your file 
read FILENAME
touch ${FILENAME}_1

<p>
In this script we read the name of the user in with the command read.  Once
we get the name of the file from the user we use the touch command to create
a file it name_1.  But notice the difference in how the variable is 
handled in the touch command line.  The { and } are used to delineate the
variable name from the rest of the line.  If we don't do this the script ends
with an error.
<p>
<br>Loops
There are three types of loop structures in bash; for, while, and until.  For
almost all cases until and while are interchangeable, so we will deal only with
for and while in these notes.
<p>
The for loop in bash is not the same as C or pascal where the value of 
some variable is set and then incremented.  In bash the for statement is used
to loop through some list.  As an example, suppose that you had some data
files in a directory called data.  If you wanted to create a subdirectory of 
the results directory for each of these files you could use the next script.  
This script is meant to be run from the directory containing 
subdirectories data and results.

#!/bin/bash
# create a directory for each data file
                                                                                
for dir in `ls data` ; do
mkdir results/$dir
done

<p>
This script introduces another interesting aspect of the bash shell, the  `<br>
character.  This is not an apostrophe, but a backtick.  On most keyboards it is
above the  ~<br> character on the upper left. When bash encounters these marks
it evaluates the expression inside and substitutes the results in place.  So
this script in effect lists the files in data and then loops through the list
assigning one at a time to the variable dir.  
While loops in bash are closer to typical programming loops.
But before we look at a while loop example, we need to discuss the 
 [<br> operator.  In bash  [<br> is a link to the test program.  This 
program is most often used to check arithmetic conditions, logical conditions 
or file characteristics. 
Here is how  [<br>  can be used in a while loop to create datafiles
with numerical extensions.

#!/bin/bash
export i=1
while [ $i -lt 10 ]; do
touch data/datafile_${i}
echo file data/datafile_${i} created 
i=$(( $i+1 ))
done

<p>
Here  [<br> is used to test to see if the value of  $i<br> is less than
10.  If not it creates a file in the data subdirectory. (Of course in 
real situations you may want to be more flexible with the path.  How could we
do that?) This script uses echo to announce the creation of the file.  While
this is not necessary in this simple example, it is a good habit to get into.
Shell scripts can become quite complex, and may run for some time.  Echoing the
completion of tasks can make debugging easier.  Also if the script will run for
a long time the output can be redirected to a file for logging purposes.
This script also introduces the arithmetic operator  $(( ))<br>.  Since bash 
is not a typed language, it uses this construction to delineate arithmetic 
operations.
<p>
<br>if ... then ... else
No language would be viable without some manner of conditional control of flow.
In bash this construction takes the form below.

if [ ... ]
then
  # if-code
else
  # else-code
fi

<p>
So lets change the script that creates a directory for each data file to check
and see if the directory exists. 

#!/bin/bash
# create a directory for each data file

for dir in `ls data` ; do
   if [ -e results/${dir} ]
   then
      echo The file/directory results/${dir} exists
   else
      mkdir results/$dir
   fi
done

<p>
This example also introduces the file attribute operator. We alluded to this
before when discussing the test operator.  In this case we are using the 
operator  -e<br> which test to see if a file exists.
If the file does exist we print a message, and if not we create the directory.  
A listing of the other file attribute operators can be found in Appendix 
ref<p>:bashref.
<br>case
The case statement in bash let's you test strings against patterns that may
contain wildcard characters.  Although this is different than the case or switch
statement in other languages, the effect of compressing a series of if ... then 
... elses is the same.  The syntax is listed below.
<p>

case expression in
   pattern 1 ) 
              statements ;;
   pattern 2 )
              statements ;;
   ...
esac

<p>
To extend the example on creating results directories, suppose that you have
data files with the extensions abc, cde, and fgh.  Now suppose that you
want the results from each type of file to go to a different directory,
namely results_abc, results_cde, or results_fgh.  You can do this with the
following script.(This script assumes that directories results_??? exist.)

#!/bin/bash
# create a directory for each data file

for dir in `ls data` ; do
   case $dir in
      *.abc ) mkdir results_abc/$dir ;;
      *.cde ) mkdir results_cde/$dir ;;
      *.fgh ) mkdir results_fgh/$dir ;;
      * ) echo $dir unknown file type ;;
   esac
     
done

<p>
This script flows goes through the case statement until a match is found, and
if no match is found then nothing happens.  In bash the case statement exits 
after performing the statements that follow a match. In this example, the last 
case catches everything that does not match one of the other rules. This lets 
us know if some file does not get a directory created. In general it is a good
idea to include a default case.
<p>
<br>Exercises
<br>
<br>
<br> Write a script that searches through a set of files in a directory and
processes the names according to the following rules
   begin<br>ize
	  <br> If the file is a directory print a warning message.
	  <br> If the file is not empty create a file in the results subdirectory
of the directory from which the script is written.  Do not create the results
directory ahead of time, but check to see if it exists and create it if 
necessary.
	  <br> If the file is empty delete it.
	  <br> keep track of how many of each type of file is encountered.
   end<br>ize
   Create a directory and some subdirectories, empty files and non-empty files
to check your script.  Be careful that you don't delete too much. On trick
is to write the script with just warning messages first to see if it will do
what you think it will.
<br> Check the Pattern-Matching Operators in Appendix ref<p>:bashref
and see if you can write a script that will take a PATH variable and print
each directory on a line.  (Hint: substitute a  \n<br> character for the :
delimiter) Can you extend this to a script that will check to see if the
directory exists?
<br> Read the name of a file from the keyboard.  Assume that the file contains
a string on each line of the file.
Find the first and last strings alphabetically and put them
in a file called first_last.  
Check Appendix ref<p>:bashref for string operators. Make up 
a file and test your script.
Try the same thing but get the whole file in alphabetic order by using the 
sort command.
<br> Advanced Exercise - Command line parameters to the script may be accessed
by using $#. The name of the script is located in $0, the next word on
the command line is $1 and so on. Write a script that takes four parameters
The first is a maximum size.  The second, third and fourth parameters are 
names of files.  If the combined size of the file named by the second parameter
and the file named by the third parameter is less than the maximum size, then
they are combined into the name given in the fourth parameter.  The original 
files are then removed.
<p>

<p>


 
<p>Compiling Basics and Make
Researchers must often write their own code, or at the very least modify code
that someone else has written.  In the Linux world this means working with
one of the compilers that are available, and more often than not, working
with a makefile (or Makefile).  While
we have several different compilers available on our systems we will concentrate
on gcc for this .  Although different compilers have different options,
the basic usage is the same. Concentrating on one compiler will allow us to avoid several complications.  The material should  
easily translate to other compilers, and even other languages.
<p>
Once programs reach a certain complexity, they may require certain libraries,
include files, or may be contained in more than one file.  The command to
compile such a program may consist of several lines.
These problems are 
further complicated by trying to remember which files where changed and what
are the interdependencies.  Fortunately, in the Linux world we have the command
it make. indexmake With a properly formated it makefile, all of these
problems are handled automatically.  indexmakefile 
<p>
Compiling Basics
label:compilingBasics
Lets start with a simple example. Let the following program be in the 
file HiMom.c. 

main(){
   printf("Hi Mom\n");
}

To compile this program we could just type it gcc HiMom.c.  This would
produce a file called it a.out.  While this file would execute correctly,
the next time you compiled a program in this directory you would loose this 
compiled version.  Or worse yet when you came back two days latter would 
you remember what program was compiled into a.out?  It is best to always use
the it -o option which lets you name the binary. Compiling the program with
the -o option would look like it gcc -o HiMom HiMom.c. This would create the
executable it HiMom. 
<p>
Gcc does not print out all of the warnings that it finds compiling the code. 
It will print out warnings if the code is so questionable that it seems as if
the program will not execute correctly.  If you want the code to be portable,
or if you may want to compile with a different compiler it is a good idea to
compile your code with the it -Wall flag.  This tells the compiler to 
print all warning messages.  Compiling the HiMom.c code with the command it
gcc -Wall -o HiMom HiMom.c produces the following.

HiMom.c:1: warning: return type defaults to `int'
HiMom.c: In function `main':
HiMom.c:2: warning: implicit declaration 
						   of function `printf'
HiMom.c:3: warning: control reaches end of 
						   non-void function

<p>
None of these warnings will cause problems with this program, but they might
be of concern if they were coming from compilation of a subprogram.  Because 
this is not a class on C programming we won't go into the details of the 
warning messages, but the "fixed" code is given here for completeness.

#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
int main(){
   printf("Hi Mom\n");
   exit(0);
}

<p>
The characteristics of the executable produced by the compiler may be 
determined by the use of various flags.  There are too many to cover in any
depth in a short introductory class, but we will examine a few of the more
common flags.  There are trade-offs to be made by using different flags.  Be 
aware that the nature of your code will determine if a particular flag will 
be of benefit.  This is a matter of experience and sometimes brute force trial
and error.
<p>
There are two very useful flags it -O and it -g that you should use on
a regular basis. The -g flag produces extra debugging information that can
be used when running a debugger or program tracer.  This make larger and 
possibly slower code so it is often used during development and turned off
for production code.  Most compilers can create binaries with different
levels of optimization.  To increase the level of optimization use the -O
flag.  With this flag turned on gcc tries to reduce size and execution time.
There are various levels of optimization, and you should read the man page for
more information.
<p>
So far we have dealt with compiling a single file that only requires the 
standard libraries.  This is not usually the case for research codes.  They
usually need external libraries, and are often contained in multiple files.
Although the topic of libraries will be discussed in <p> ref<p>:libs,
we will give a brief introduction here.  
<p>
Most larger programs have code in several files that are compiled separately,
and then linked into the final code. We will use the following example that
calls a function from another source file.  This is a trivial example that 
we will build on later. 
<p>

----------------------------------------------
File mainProgram.c

/* This program calls a function that returns*/
/* a value.  It is just for demonstration*/ 
/* purposes */

#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include "subProgram.h"
int main(){
   double myDouble;
   myDouble=calcValue(5);
   printf("calculated value %10.5f\n",myDouble);
   exit(0);
}
----------------------------------------------

----------------------------------------------
File subProgram.h

/* a trivial function to use as an example */

double calcValue(int input);

----------------------------------------------

----------------------------------------------
File subProgram.c

/* Code for a trivial function */

#include "subProgram.h"

double calcValue(int input){

   double returnVal;
   
   returnVal = (input * input) - 3.14;
   
   return(returnVal);
}
----------------------------------------------

<p>
These files can be compiled with the commands.
<p>

gcc -c -o subProgram.o subProgram.c
gcc -c -o mainProgram.o mainProgram.c
gcc -o myCalculation mainProgram.o subProgram.o

<p>
On the first two lines the it -c option tells gcc not to run the linker.
Or in other words compile the file and assume that the undefined (but not 
undeclared) functions will be available when the files are linked.
The third command links the .o files and calls the program it myCalculation.
<p>
But what if we want to call a function from a library that is not in our 
standard path. For this example we will use the it gsl or it Gnu Scientific 
Library. These are freely available libraries of various math functions.  They
are not always the fastest, but they are widely available.  Lets assume that
we want to replace our trivial function with a call to the library function that
computes the Bessel Function of order 0.  Then the subProgram.c file becomes:
<p>

/* Code for a trivial function */

#include "subProgram.h"
#include &lt;gsl/gsl_sf_bessel.h&gt; 

double calcValue(double input){

   double returnVal;
   returnVal = gsl_sf_bessel_J0 (input);
   
   return(returnVal);
}

<p>
If we compile it with the same line as above we get the error message, "gsl/gsl_sf_bessel.h : No such file or directory".  This is because the gsl include
files are not in one of the standard include paths. (Although in a default 
installation they are put into /usr/local/include.) 
Assume that the gsl libraries are in /opt/gsl/lib and
the includes are in /opt/gsl/include. We can use the it -I option
to add this new include directory to the include path for this program. We would
change the line above to "gcc -c -I /opt/gsl/include -o subProgram.o 
subProgram.c" and then the compile will work.  But now when we try to link the
programs with the last gcc line above we get this output.
<p>

subProgram.o(.text+0x1c): In function `calcValue':
: undefined reference to `gsl_sf_bessel_J0'
collect2: ld returned 1 exit status

<p>
This is because the library file that contains the newline gsl_sf_bessel_J0 function
is not is one of the standard library paths.  To get a compile without errors
we must change the last gcc line to be:
<p>

 gcc -L /opt/gsl/lib -lgsl -lgslcblas 
		 -o myCalc mainProgram.o subProgram.o

<p>
The line is broken only for space considerations and would be typed as one command.
We have added the it -L and the it -l flags. The -L flag is to libraries
what the -I flag is to include files.  It says to add the directory, in this
case /opt/gsl/lib, to the paths that are searched for the library files.
the -l flag names library files that are to be used to find functions.  Althoughwe are not only linking the gsl library, but the gslcblas library file. 
This must be done because the gsl library makes calls to the gslcblas libraries.
Make
As you can see by now the compiling and linking of large research codes can
begin to be quite complicated.  Imagine that instead of three files in the
last  we had been dealing with one hundred source and header files. 
Added to this problem is that if you change one of the source files it is 
inefficient to have to recompile all of them.  But how can you keep track of 
the dependencies and how can you know when a file was last compiled?  The 
answer on Linux systems is it make indexmake.  
Make is a program that is designed specifically to handle these problems. Among
other things make can, keep track of 
dependencies, and determine when program files need to be recompiled.  The
syntax of a make file is not always clear, but with a little experience it
can be very helpful.  
<p>
Make must know what you want to compile (it targets), what other files
the program needs (it dependencies), and how you want to compile the
program (it commands). indextargets indexdependencies indexcommands
Lets suppose that we have the following rather typical scenario. We want to
make a program myProgram.  The main file is called myProgram.c and it
is dependent on two other source files sub1.c and sub2.c.  The programs
use the math library (-lm) and a library abc which is contained in the /opt/lib
directory.  The include files for abc are in the /opt/include directory.  
Additionally each of the c files has an associated header file, and they
may call functions from each other.  The make file for this may look like:
<p>

# Makefile for myProgram
myProgram: sub1.o sub2.o myProgram.o 
   gcc -o myProgram -L/opt/lib -I/opt/include  \ 
	  -labc -lm sub1.o sub2.o myProgram.o

myProgram.o: myProgram.c sub1.h sub2.h
   gcc -c -o myProgram.o -L/opt/lib -I/opt/include \
      -labc -lm myProgram.c

sub1.o: sub1.c sub1.h
   gcc -c -o sub1.o -L/opt/lib -I/opt/include \
      -labc  -lm sub1.c

sub2.o: sub2.c sub2.h
   gcc -c -o sub2.o -L/opt/lib -I/opt/include \
      -labc -lm sub2.c

<p>
Let's look at this file in some detail. The first line is a comment.
The lines that begin with a program 
name and a colon followed by a list of files is a target-dependency line.
What goes before the colon is the it target and the other files are the
it dependencies.  Make will keep track of which of these files have
changed since the target was last compiled, and will recompile them before 
compiling the target. It will not recompile a target if none of the dependencies
have changed.  Notice the  \<br> at the end of the first gcc line.  This is
the continuation character for make so these two lines are treated as one. 
Extra white space is deleted when a continuation character is used.  This 
leads to one of the most frustrating aspects of the makefile for new users.
The gcc lines must begin with a tab or make has a problem.  By default, make
compiles the first target.  If you want to compile one of the other targets
you can (for example) just type make sub1.o and only that target and it's 
dependencies (if necessary) will be compiled.
<p>
Make allows assigning values to variables.  This can clean up the lines of 
a make file.  We've done this in the second version of the make file for
myProgram.

# Makefile for myProgram
LIBS = -L /opt/lib -labc -lm
INCLUDES = -I /opt/include
OBJECTS = sub1.o sub2.o myProgram.o
myProgram: $(OBJECTS) 
   gcc -o myProgram $(INCLUDES) $(LIBS) $(OBJECTS)  
	  

myProgram.o: myProgram.c sub1.h sub2.h
   gcc -c -o myProgram.o $(LIBS) $(INCLUDES)\
      myProgram.c

sub1.o: sub1.c sub1.h
   gcc -c -o sub1.o $(INCLUDES) $(LIBS) sub1.c

sub2.o: sub2.c sub2.h
   gcc -c -o sub2.o $(INCLUDES) $(LIBS) sub2.c

clean:
   /bin/rm *.o

<p>
In this version we assign the library directories, include directories, and
object files to variables, which are then used in the dependencies and 
command lines.  This not only makes the file more readable, it also helps 
when adding or removing files from the project. We've also added a dummy target
clean.  This is a common practice.  In this case we've removed all of the 
.o files.  This feature makes it easier to move projects between systems and
to handle changes in the library files.  In effect what it does is remove all
of the .o files so that the next run will recompile everything. This dummy
target adds significant flexibility to make.
<p>
Exercises
<br>
<br> Add a loop to the main program file in the example that calculates the
Bessel function to call the sub-program a large number of times.  Experiment
with the size of the loop so that the executions takes some significant amount
of time. (Say  &gt;<br> 3 minutes) Use the Linux "time" command to time your 
runs.(Check
the man page.) Now try compiling the program with different levels of 
optimization to see if the code gets faster.
<br> Create a makefile so that the code from the Bessel function example can
be compiled with make.  Extend the file so that if you want to compile with
the -g option you make one target, and if you want to compile without the
-g option you make another target. 
<br> Advanced Exercise - Look up the the it -D option for gcc and see
if you can use this to compile the program with the Bessel option sometimes
and with some other function of your choice at other times.  (Even just square
root as an example) Put these changes into the makefile so that you can
make the version that you want with no changes to the code or makefile.

<p>



<p>High-performance libraries
label<p>:libs
What are high-performance libraries?
<p>
Almost all scientific programs include a small number of repeating, common
snippets of code which perform certain core, usually simple, mathematical
operations such as multiplying a matrix by a vector.  It is
therefore reasonable to consider these common snippets separately and mix
them into a main program as needed, like, using a culinary metaphor,
when baking a fruit cake.
<p>
Moreover, if you profile a scientific code you will notice that these snippets
are, in fact, where the program spends most of its time.  It is therefore
desirable to have them written very professionally, with precision
and efficiency in mind, and have them tested thoroughly.
<p>
These two requirements are met, at least in principle, by libraries which are
sets of object files that implement specific functions and routines, and which
can be linked with the main object files to produce executables.
<p>
In general, we distinguish between two types of libraries:
<br>
<br> Static libraries are linked with the rest of the code when you build
your executable.  In Unix (and Linux) these libraries have extension .a
<br> Dynamic libraries are linked with the ``executable" image at runtime.
In Unix these libraries have extension .so

<p>
It is important to realize that you need only one copy of a given
library on a machine to which each user has an access but which they should
not be able to modify.  It is usually a bad idea to try to modify a 
``standard library".
<p>
In addition to ``standard" libraries, which are produced (and 
often sold) by reputable organizations and companies, and which are 
not supposed to contain errors, a user can create his/her own libraries.  
Say you are a computational chemist, and your favorite algorithm involves 
some kind of a spatial mesh and you like spherical harmonics.  Chances are, 
most of the codes you will write in your career will include subroutines 
which facilitate integration of spherical harmonics on a spatial mesh of
one kind or another.
You will probably want to include these routines in your library and use
them repeatedly, without having to recompile, provided this library,
too, is error free.  Later we will tell you how to create such a library.
<p>
Before we get to that we want to tell you how to build and use existing
libraries because this is the scenario you will be most often confronted
with.  We will concentrate and practice on one type (arguably the one 
that is most widely used in today's scientific computations) of high 
performance libraries - the linear algebra library.  
After we go through this exercise you may assume that you have a
general knowledge how to build and link to any library you find 
useful in your work.  This, of course, if you stick around through
this entire session.
<p>
LAPACK, BLAS, ATLAS and all that
<p>
Linear Algebra PACKage (LAPACK) is a transportable library of standardized
FORTRAN 77 routines for solving systems of simultaneous linear equations,
eigenvalue problems, and singular value problems.  It is used in a vast 
majority of large-scale
scientific programs.  The most up-to-date version is held in the online
repository linebreak
textttwww.netlib.org together with a number of other widely used
libraries.  LAPACK has been around since 1991 when the National Science
Foundation and the US Department of Energy decided to sponsor an effort
to make EISPACK and LINPACK libraries run efficiently on shared memory vector
and parallel processors. Without going into details this is achieved by moving
the block matrix operations to the innermost loops which can then be optimized
for a specific architecture to account for the memory hierarchy (this is
mostly done in BLAS - read on...), thus achieving the highest cache reuse.
Functionality is provided for real and complex matrices, in both single and
double precision.  FORTRAN 95 (LAPACK95) interface is also available as well
as the C version of LAPACK (CLAPACK) and its C++ (lapack++) interface (now
superseded by the Template Numerical Toolkit (TNT)).
<p>
The structure of LAPACK and how to find a needed routine is described in
the emphLAPACK User's Guide at 
textttnetlib.org/lapack/lug/<br>.html 
Just one look at emphDriver Routines and emphComputational Routines
s of this guide will give you a good sense of what tasks can be 
accomplished with this library.  For a set of LAPACK emphWorking Notes
go to 
textttwww.netlib.org/lapack/lawnsnewline
/downloads
Within LAPACK, each driver (which solves a complete problem and whose use is
always recommended) and computational routine (which solves a smaller 
algebraic
task) are classified according to the scheme XYYZZZ, where X is equal to S,D,C 
and Z (single, double precision, complex and complex *16), YY indicates the 
matrix type, and ZZZ indicates the operation performed.  LAPACK also
contains a certain number of auxiliary routines (usually low-level
computations not included in BLAS).  Refer to the emphLAPACK User's Guide
for a naming scheme of these routines.  In order to illustrate the process
of selecting the needed routine from LAPACK let us assume for the moment
(we will come back to this example later on) that we need to solve a
real generalized symmetric-definite eigenproblem.  Say, we want to 
use double precision and therefore the first symbol of XYYZZZ is D.
SY stands for symmetric so the YY is SY.  Finally, inspection of the 
emphLAPACK User's Guide <!-- MATH
 $rightarrow$
 -->
<IMG
 WIDTH="286" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img7.png"
 ALT="$ rightarrow$">
 Generalized Eigenvalue and Singular 
Value Problems <!-- MATH
 $rightarrow$
 -->
<IMG
 WIDTH="286" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img7.png"
 ALT="$ rightarrow$">
 Generalized Symmetric Definite Eigenproblems 
(GSEP) tells us that the proper choice of ZZZ is, in our case, GV (ZZZ may
or may not contain all three symbols).  To verify this choice the user
should look directly at the dsygv.f code in 
texttt/usr/local/netlibtest1/LAPACK/SRC
or type textttman dsygv  A word of caution:  the man pages for
LAPACK are not included with the textttnetlib.org distribution.  
The easiest way of acquiring them on a RedHat system is through a 
textttrpm  To do this point your browser to 
textttrpmfind.net/linux/RPM/ASP/i386/RPMS.9/ newline
lapack-man-3.0-20.i386.html
and download the textttrpm  Become a superuser and issue the 
command textttrpm -ivh lapack-man-3.0-20.i386.rpm
This will install the LAPACK man pages under texttt/usr/share/man/manl
While we are at it go ahead and install BLAS man pages from 
textttrpmfind.net/linux/RPM/ASP/i386/RPMS.9/blas-man-newline
3.0-20.i386.html
You will learn about BLAS very soon! (Beware, these BLAS man pages are
incomplete.) 
<p>
LAPACK works by making dense calls to the Basic Linear Algebra Subprograms
(BLAS) - a collection of routines that perform specific vector and matrix
operations.  For that reason the performance of LAPACK depends on BLAS.
BLAS, you guessed it, must be highly optimized and run very fast on YOUR
computer.  You have two good options to accomplish this:
<p>
<br>
<br> Get vendor or ISV (Independent Software Vendor) BLAS optimized for your
processor architecture.  This library is usually already compiled for you.
In our experience these BLAS are the fastest.
<br> Compile the Automatically Tuned Linear Algebra Software (ATLAS) on your
machine.  This contains BLAS and some LAPACK.  BLAS themselves consist of 
``levels":
<br>
<br> <!-- MATH
 $bf{level 1}$
 -->
<IMG
 WIDTH="81" HEIGHT="27" ALIGN="MIDDLE" BORDER="0"
 SRC="img8.png"
 ALT="$ bf{level 1}$">
 BLAS include elementary linear algebraic vector 
operations, 
usually involving just one level of looping and therefore having complexity of 
emphO<IMG
 WIDTH="59" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img9.png"
 ALT="$ (N)$">
. Dot product, constant times a vector plus vector, plane rotation,
vector copy
and swap, belong to such operations.  At the time of the development of these
original BLASfootnoteC.L.Lawson, R.J.Hanson, D.R.Kincaid and F.T.Krogh, 
ACM Trans.
Math. Software bf 5, 308 (1979).
(late 1970's) it was believed that almost all most common linear 
algebra algorithms can be built from these highly portable and efficient routines
without much penalty.
<br> <!-- MATH
 $bf{level 2}$
 -->
<IMG
 WIDTH="28" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img10.png"
 ALT="$ bf{level 2}$">
 BLAS.  It soon become clear that for higher-level 
algorithms
optimization on at least matrix-vector level becomes necessary.  This
could not be provided by level 1 BLAS and therefore level 2 BLAS were 
implemented 
in the late 1980's.footnoteJ.J.Dongara, J.Du Croz, S.Hammarling and R.J.Hanson, 
ACM Trans. Math. Software bf 14, 1 (1988).
 These BLAS include an extended but limited set of 
highly portable and 
efficient matrix-vector operations which occur frequently in most common 
linear
algebra algorithms.  These operations consist of  matrix-vector multiply, 
rank-1 and
rank-2 updates and solutions of triangular equations of certain forms. All
these operations have complexity of emphO<IMG
 WIDTH="59" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img11.png"
 ALT="$ (N^2)$">
. Of course, with the 
increase
of complexity grows the need for a selective optimization for various, 
sometimes very diverse computer architectures.  
<br> <!-- MATH
 $bf{level 3}$
 -->
<IMG
 WIDTH="36" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$ bf{level 3}$">
 BLAS.  This need becomes even more urgent for the level 3
BLAS which, as you properly guessed, concern matrix-matrix operations such as 
matrix-matrix multiply-and-add operations, rank-k and rank-2k updates of
a symmetric matrix, matrix-triangular matrix multiply operations, solving
triangular systems of equations with multiple right-hand sides of certain
forms, and their complex analogues.  If these operation names appear cryptic 
to you 
or you want to learn more about them please refer to the cited paper 
which contains 
description of what they are.footnoteJ.J.Dongara, J.Du Croz, 
S.Hammarling and R.J.Hanson, ACM Trans. Math. Software bf 16, 
1 (1990). 
All of these operations have, of course,  
complexity of emphO<IMG
 WIDTH="59" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img13.png"
 ALT="$ (N^3)$">
 and, similarly to level 2 BLAS, are 
limited in scope.
We will not be concerned in this bootcamp about the naming convention
for BLAS since this aspect is transparent for users like us who mostly 
employ higher-level LAPACK driver routines rather than call particular 
members of BLAS.
<p>
With the advent of cache and computers with hierarchical memory it was realized 
that a usual approach of using modified level 2 BLAS routines developed for
vector machines cannot be used efficiently on these type of architectures and 
the paradigm of coding long vectors started being replaced by 
emphcache blocking.  Modern level 3 BLAS fully exploit this 
paradigm. 
<p>
beginfigure[hbt]
hspace1.3cm
centering
includegraphics [width=7.0cm] memory.eps
caption Rate of memory access
labelfig:memory
endfigure
<p>
A cache-based memory system has small blocks of very
fast, low-latency, memory called cache (L1, L2, L3,
etc., depending on how close to the CPU it is located) placed between
main memory and CPU registers.  The idea is to have a quick access to 
most frequently referenced data and sets of instructions.  However, cache
memory is not cheap and there are other design constraints (usually cache
is put directly on die) that do not 
allow it to grow above a certain size.  Today, on desktop machines,
this size is somewhere in the order of 32 kB-128 kB and 256 kB-8,192 kB 
for L1 and L2 cache, respectively.  Intel P4 Extreme Edition 
adds a 2,048 kB L3 cache. Typically, data transfer rate between
memory subsystems scales as shown in Fig. (reffig:memory).
<p>
When ALU needs new data or a new instruction from a code stream the 
processor CPU's front-end first checks whether this <br> is already stored 
in the cache. If not, it is fetched from the main memory in a block 
equal to the size of a emphcache line and placed in cache according to a 
predefined cache placement policy.  This method of cache operation design 
corresponds to the principle of spatial locality which 
states that, as a general rule, if the CPU needs an <br> from memory at 
any given moment it most likely needs its neighbors, too.
<p>
Such hierarchical memory system has problems of its own.  A lot of
industrial resources as well as research is being dedicated to study
associativity (cache placement policy), optimal cash line size, 
replacement strategy and, in
particular, data prefetching techniques to minimize cache pollution
and main memory traffic, and to maximize cache hit rates.  In large-scale
scientific programs, especially the ones which involve large, dense 
matrices or other large dynamical sets of data, it is difficult to
arrive at one, efficient, generally applicable caching strategy.
For instance, when you think about
simple multiplication of two large matrices (rows vs. columns) 
then normal caching, even when combined with loop unrolling, software 
pipelining, or more sophisticated hardware prefetchingfootnote
J.L.Hennessy and D.A.Patterson emphComputer
Architecture. A Quantitative Approach (Morgan Kaufmann Publishers,
New York 2003). 
schemes,
will not prevent us from inevitable instances of cache data
eviction. As a result it is not uncommon to observe for such programs
a performance drop by up to 50%. Here, the technique which comes
to our rescue is cache blocking.  
<p>
Cache blocking can be described as
dividing a large problem into smaller blocks, resulting in reduced
data granularity and therefore reduced number of requests to fetch 
data from the main
memory.  Such transportable ``on-die" calculations are the target of 
Automated Empirical Optimization of Software (AEOS) approach described
later in this <p>.
It should be mentioned that in addition to ``controlling/enabling" 
the utilization of the spatial locality principle, cache blocking can 
improve the utilization of the 
temporal locality principle (general rule that if an <br> in memory
is accessed it will likely be accessed in a near future), as well.
Of course, cache blocking and all the other cache optimization techniques
mentioned above are NOT transparent to processor architecture.    
<p>
beginfigure[hbt]
hspace1.3cm
centering
includegraphics [width=7.0cm] tiling.eps
caption This cache blocking technique sets the
inner loop to be traversed only tile_size times at a time.
labelfig:tiling
endfigure
<p>
An almost trivial example of cache blocking (or rather, in this
instance, cache optimization) is a
loop interchange in a matrix operation code.  Depending whether you
use FORTAN (row-major ordering) or  C (column-major ordering) 
you will want to interchange outer
indices with inner indices to minimize cache misses.  (Usually a compiler
is able to perform this task automatically.) A simple example
of a non-trivial cache blocking coding is shown in Fig. (reffig:tiling)
where the inner loop is traversed only tile_size times at a time. 
<p>
As mentioned above level 3 BLAS have achieved their remarkable efficiency 
through extensive, but at the same time very shrewd, use of cache blocking 
techniques.  In fact, level 3 BLAS are so efficient that, as mentioned earlier,
employing them in a matrix-matrix problem significantly improves code
performance as compared to the same problem solved with the exclusive use of 
level 1 and level 2 BLAS routines.
<p>

<p>
We have already hinted that optimal cache blocking can be a very challenging
issue, especially when we need
to ensure a high level of code transferability.  The latest and greatest 
tool which addresses this challenge in the area of BLAS software is ATLAS, 
a member of the AEOS family.  AEOS codes
self-adapt to various architectural parameters such as memory hierarchies, 
number and types of functional units and registers, cache sizes, CPU latencies,
etc.  They do it by isolating performance-critical routines 
and by adapting them 
to differing environments by parameters adjustment, multiple implementation 
and source generation. The best codes are then selected by robust, 
context-sensitive, empirical timers.  The AEOS paradigm requires, 
that the entire process of optimization be accessible not only to an
expert but also to a regular user.  ATLAS meets these requirements by
performing automatic parameterization and multiple implementation of the 
level 1 and 2 BLAS routines 
and by performing automatic parameterization, multiple implementation and 
source code adaptation 
(which involves generation of different code implementations for a given 
operation) of a level 3 BLAS simple building-block matmul kernel.footnote
R.C.Whaley, A.Petiet and J.J.Dongarra, Parallel Comput. 
bf 27, 3 (2001). 
From this optimized matmul kernel (actually, occurring in three slightly 
different forms) the full level 3 BLAS operation for general
matrix multiply and add (GEMM) can be built.  It can be shown that all 
level 3 BLAS operations can be further constructed from GEMM and some 
level 1 and level 2 BLAS (GEMV) operations.footnoteB.Kaagstr&#246;m, 
P.Ling and C.Van Loan, ACM Trans.
Math. Software bf 5, 268 (1998).
<p>

<p>
NOTE:  If you do not follow one of the above paths the emphModel 
Implementation of
BLAS, which is included in the LAPACK distribution (both source and binaries),
will be used by LAPACK.  However, in this case you will suffer a high and,
unless you are doing something trivial, unacceptable performance penalty.
<p>
Building LAPACK
Enough talking. Let's us now prepare our own version of
LAPACK library from scratch.  To that end we will use ATLAS software since
taking this route covers the broadest array of circumstances.  
By the end of this
unit I will tell you what is available in terms of vendor or ISV optimized
BLAS.  We will proceed as follows:
<p>
<br>
<br> Make directory textttusr/local/sourceforge 
<br> Download the latest, stable, platform-independent ATLAS 
software from  newline
textttmath-atlas.sourceforge.net to this directory.
<br> Unzip and untar the file (you already know how to do this).
<br> textttcd to the texttt./ATLAS directory and apply fixes (string
overrun) from the newline
textttmath-atlas.sourceforge.net/errata.html
page.  Read this page carefully, it will save you a lot of time and work 
down the road.
<br> Type textttmake config CC=gcc (using the Portland linebreak
Group compilers will result
in error) and follow the script using the express setup, if possible.
<br> Type textttmake install arch= newline
&lt;architecture_as_detected_by_config&gt;
<br> Type textttmake sanity_test arch= newline
&lt;architecture_as_detected_by_config&gt; newline
and textttmake ptsanity_test arch= newline 
&lt;architecture_as_detected_by_config&gt; newline
if you chose to build the threaded library.
<br> Time and test ATLAS as described in newline
texttt&nbsp;/ATLAS/doc/TestTime.txt 
<br> CacheEdge - more about memory layout and cache blocking.
<p>
As you now know, cache misses result in costly access to high 
level memory.  To minimize the
penalty ATLAS detects the emphactual size of the L1 data cache and 
cache-blocks matmul kernel appropriately. linebreak
Then, the rest of the code is further tuned
to block for the L2 cache by an empirically determined parameter
called CacheEdge. You can find the optimum value of CacheEdge by:
<br>
<br> textttcd ./tune/blas/gemm/ newline
&lt;architecture_as_detected_by_config
<br> textttmake xdfindCE
<br> texttt./xdfindCE -m 3500 -n 3500 -k 3500 newline
to find out what your CacheEdge might be.
<br> textttcd ../../../.././include/ newline
&lt;architecture_as_detected_by_config&gt;
<br> textttvi atlas_cacheedge.h and substitute the linebreak
value in the third 
line by 524288. (524288 Bytes = 512 kBytes * 1024 Bytes/KBytes)
<br> you can now fine-tune CacheEdge in the following manner:
<br>
<br> textttcd ../../../../
<br> edit textttMake. newline
&lt;architecture_as_detected_by_config&gt; and define
symbol BLASlib.  In the best scenario it should point to a location where you 
have another, working copy of complete libblas.a  Otherwise get the 
emphFORTRAN 77 reference BLAS file and build it into libblas.a or use
ATLAS own C reference.
<br> textttcd ../.././include/ newline
&lt;architecture_as_detected_by_config&gt;
<br> change the value of CacheEdge in newline
atlas_cacheedge.h
<br> textttcd ../.././bin/ newline
&lt;architecture_as_detected_by_config&gt;
<br> textttmake x[s,d,c,z]l[1,2,3]blastst
<br> textttx[s,d,c,z]l[1,2,3]blastst save the results and repeat
steps textttiii-vii
<br> once the best value for CacheEdge is found recompile ATLAS 
by issuing newline
textttmake xdl3blastst xsl3blastst newline
xcl3blastst xzl3blastst newline
This will rebuild all libs with the new newline
CacheEdge setting. newline
(textttxdl3blastst xsl3blastst newline 
xcl3blastst xzl3blastst have the 
correct dependencies to ensure a proper rebuild.)

<br> If you have not done it already as a part of CacheEdge fine-tuning
procedure described linebreak
above recompile ATLAS by issuing newline
textttmake xdl3blastst xsl3blastst newline
xcl3blastst xzl3blastst

<p>
<br> You have probably forgot by now that installing ATLAS was not our
ultimate goal.  Our ultimate goal was to build the fastest implementation
of LAPACK available for your machine.  Some more work remains to be done:
<br>
<br> Get from textttnetlib.org and build LAPACK.  Here are the procedures:
<br>
<br> Make a directory texttt/usr/local/netlib 
<br> Go to textttnetlib.org <!-- MATH
 $rightarrow$
 -->
<IMG
 WIDTH="286" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img7.png"
 ALT="$ rightarrow$">
 browse <!-- MATH
 $rightarrow$
 -->
<IMG
 WIDTH="286" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img7.png"
 ALT="$ rightarrow$">
 lapack 
<!-- MATH
 $rightarrow$
 -->
<IMG
 WIDTH="286" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img7.png"
 ALT="$ rightarrow$">
 scroll down and download file textttlapack.tgz into
directory newline
texttt/usr/local/netlib 
<br> textttgunzip lapack.tgz 
<br> texttttar -xvf lapack.tar 
<br> textttcd LAPACK and read file textttREADME  You will notice
a link to a list of known problems, bugs and compiler errors for
LAPACK linebreak
maintained on textttnetlib.org 
Open your browser again and go
to that address.  You will notice a LARGE errata.  Normally you
would like to incorporate all the errata files in your original
distribution to compile your working version of LAPACK.  In this
bootcamp, since our time is limited, we will only correct
the Makefiles.  This is necessary to compile the programs properly.
<br> Download the errata files newline
textttLAPACK/INSTALL/make.inc.LINUX newline
and textttLAPACK/TIMING/Makefile newline to the
texttt/usr/local/netlib
directory.  You could, of course, download these files right into
their proper directories.  The reason why we download the errata 
files to 
texttt/usr/local/netlib is that if the compilation fails and for
some reason we want to untar a brand new tree of LAPACK we do not
need to download them again.
<br> Copy the errata files from newline
texttt/usr/local/netlib to their
proper directories: newline
textttcp /usr/local/netlib/make.inc.LINUX /usr/local/netlib/LAPACK/INSTALL
newline 
textttcp /usr/local/netlib/Makefile newline
texttt/usr/local/netlib/LAPACK/TIMING
<br> 

cp ./INSTALL/make.inc.LINUX . 
cp make.inc.LINUX make.inc

<br> 
 vi Makefile<br>  newline
You should now substitute

lib: lapacklib tmglib
#lib: blaslib lapacklib tmglib

by

#lib: lapacklib tmglib
lib: blaslib lapacklib tmglib

to make BLAS, LAPACK and to time them.
<br> textttmake

<p>
<br> in directory texttt&nbsp;/ATLAS/lib/ newline
&lt;architecture_as_detected_by_config&gt;
    issue following commands:

    mkdir tmp
    cd tmp
    ar x ../liblapack.a
    cp &lt;the_path_of_your_just_built
       _LAPACK&gt;/liblapack.a 
       ../liblapack.a
    ar r ../liblapack.a *.o
    cd ..
    rm -rf tmp

Congratulations!, you have just replaced some netlib LAPACK
routines with the ATLAS' optimized LAPACK routines.

<br> In the texttt&nbsp;/ATLAS/lib/ newline 
&lt;architecture_as_detected_by_config&gt;
    directory you should at least have:
begin<br>ize
<br>    libatlas.a - the main ATLAS library
<br>    liblapack.a - the full ATLAS optimized LAPACK library - the
                  on you have just built
<br>    libcblas.a - the C interface to BLAS
<br>    libf77blas.a - the F77 interface to BLAS
<br>    libptcblas.a - the C interface to threaded BLAS
<br>    libptf77blas.a - the F77 interface to threaded BLAS
end<br>ize
<br> Linking to these libraries must be done in a specific order:
texttt-LLIBDIR -llapack -lcblas -lf77blas newline -latlas, 
where LIBDIR is the path to your libraries.

<p>
Physics example: using LAPACK in computing the ground 
state energy and wavefunction of the hydrogen atom
<p>
We are now ready to use the fruit of our work in a real-life  scientific
problem.  The code which we gave you calculates the ground state (lowest)
energy and wavefunction of the hydrogen atom.
Hydrogen atom consists of only one proton and only one electron.  
The quantum theory tells us how to calculate energies and wave 
functions for any atom, although only in an approximate manner.  The reason
why we picked a hydrogen atom is two-fold.  First, this is the simplest 
atomic system whose variational treatment leads to a set of linear-algebraic 
equations.  If you forgot your quantum mechanics and do not understand what 
``variational" means, do not panic - we will explain it 
to you in a moment.  The important point is that the set of linear-algebraic
equations just mentioned can be completely solved using
just one LAPACK driver routine.  The second reason why we picked
the hydrogen atom for our example is that in this particular case we
are lucky enough to know the analytical solution and thus
can check our work.  I want you to keep in mind, however, that for the great
majority of current scientific problems the analytical solutions are not
known and we have to rely on approximate ones, best of which can
usually be obtained numerically, more often then not with the help of
efficient scientific libraries.  In case of atomic physics these libraries 
include 
mainly LAPACK, fast Fourier transform, and various integration routines.
<p>
So how do we solve for the ground state of the hydrogen atom?  You have 
probably heard about the Schr&#246;dinger equation for a wavefunction 
<IMG
 WIDTH="36" HEIGHT="32" ALIGN="BOTTOM" BORDER="0"
 SRC="img14.png"
 ALT="$ Psi$">
.  Function <IMG
 WIDTH="36" HEIGHT="32" ALIGN="BOTTOM" BORDER="0"
 SRC="img14.png"
 ALT="$ Psi$">
 contains all information about a quantum
system and the Schr&#246;dinger equation is a differential equation
which describes its behavior.  The
Schr&#246;dinger equation is in quantum mechanics what the familiar (Newton,
Lagrange or Hamilton) equation of motion is in classical mechanics.  
If we wrote down this equation  for the hydrogen atom we could then solve 
it analytically and find its exact solution.  We could also emphattempt 
to integrate this equation numerically on a preconceived grid of points in 
space obtaining physically exact but mathematically approximate solution.
However, there exists another approach, based on the Ritz variational 
principle, which simply states that the true ground state energy of a quantum 
system is a lower bound to a certain expression involving emphtrial
wavefunctions.  We are free to chose these trial wavefunction at will as
long as they fulfill certain criteria. Mathematically, we talk about
infinitely dimensional linear vector space, called the Hilbert space to
which these trial wavefunctions belong.  In practice, we work with subspaces
of the Hilbert space and build our trial functions out of truncated (at <IMG
 WIDTH="27" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img15.png"
 ALT="$ N$">
)
series of so-called basis functions <IMG
 WIDTH="16" HEIGHT="15" ALIGN="MIDDLE" BORDER="0"
 SRC="img16.png"
 ALT="$ phi_{i}$">
:
begineqnarray
Psi(bf r) = sum_i=1^NC_i phi_i(bf r), 
phi_i(bf r)=e^-alpha_ir^2
labelequ:basis
endeqnarray
<p>
It is then possible to rewrite the Ritz variational principle in terms of
a set of algebraic equations which in matrix notation takes on a form of
the eigenvalue problem
beginequation
underlineH cdot vecC=epsilon cdot vecC
labelequ:eing
endequation
<p>
or, if the basis functions are not orthonormal
<p>
beginequation
underlineH cdot vecC=epsilon cdot
underlineS cdot vecC
labelequ:geneing
endequation
<p>
Equation (<!-- MATH
 $ref{equ:geneing}$
 -->
<IMG
 WIDTH="31" HEIGHT="27" ALIGN="MIDDLE" BORDER="0"
 SRC="img17.png"
 ALT="$ ref{equ:geneing}$">
) represents a so-called 
generalized eigenvalue problem.
Here, <!-- MATH
 $underline{H}$
 -->
<IMG
 WIDTH="119" HEIGHT="28" ALIGN="BOTTOM" BORDER="0"
 SRC="img18.png"
 ALT="$ underline{H}$">
 is the Hamiltonian matrix, which contains 
information about kinetic and potential energies of a quantum system
and <!-- MATH
 $underline{S}$
 -->
<IMG
 WIDTH="86" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img19.png"
 ALT="$ underline{S}$">
 is the so-called overlap matrix which arises
from nonorthonormality of the basis functions.  In the current
example we will not be concerned about these matrices.   Their
elements can be easily calculated and they are, in fact, given to us 
in the program.  The scalar <IMG
 WIDTH="82" HEIGHT="16" ALIGN="MIDDLE" BORDER="0"
 SRC="img20.png"
 ALT="$ epsilon$">
 is the unknown ground state 
energy of the hydrogen atom and the corresponding elements of
the vector <IMG
 WIDTH="52" HEIGHT="27" ALIGN="BOTTOM" BORDER="0"
 SRC="img21.png"
 ALT="$ vec{C}$">
 are the unknown expansion coefficients in
equation (<!-- MATH
 $ref{equ:basis}$
 -->
<IMG
 WIDTH="37" HEIGHT="16" ALIGN="MIDDLE" BORDER="0"
 SRC="img22.png"
 ALT="$ ref{equ:basis}$">
), for this energy.
<p>
The code is written in FORTRAN 90 and consists of two main modules.
The module textttmatrix_elements (file linebreak
textttmatrielements.f90) casts the problem
of solving a Schr&#246;dinger equation for the hydrogen atom into a
linear algebraic problem of solving a generalized eigenvalue equation,
as described above.
Matrices <!-- MATH
 $underline{smatrix}(N,N)$
 -->
<IMG
 WIDTH="99" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img23.png"
 ALT="$ underline{smatrix}(N,N)$">
 and <!-- MATH
 $underline{hmatrix}(N,N)$
 -->
<IMG
 WIDTH="176" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img24.png"
 ALT="$ underline{hmatrix}(N,N)$">
,
where we now explicitly show their dimensions, enter the equation in the
following manner:
beginequation
underlinehmatrix(N,N) cdot vecC(N)=epsilon cdot 
underlinesmatrix(N,N) cdot vecC(N)
labelequ:geneing1
endequation       
Here, <IMG
 WIDTH="178" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img25.png"
 ALT="$ C(N)$">
 are eigenvectors corresponding to <IMG
 WIDTH="82" HEIGHT="16" ALIGN="MIDDLE" BORDER="0"
 SRC="img20.png"
 ALT="$ epsilon$">

Finding eigenvalues <IMG
 WIDTH="82" HEIGHT="16" ALIGN="MIDDLE" BORDER="0"
 SRC="img20.png"
 ALT="$ epsilon$">
 and corresponding 
eigenvectors <!-- MATH
 $vec{C_{epsilon}}(N)$
 -->
<IMG
 WIDTH="41" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img26.png"
 ALT="$ vec{C_{epsilon}}(N)$">
 forms the
solution to eq. (<!-- MATH
 $ref{equ:geneing1}$
 -->
<IMG
 WIDTH="105" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img27.png"
 ALT="$ ref{equ:geneing1}$">
). This is accomplished in the 
second of the two main modules - module linebreak 
texttteigenproblem
(in file texttteigenproblem.f90).   Since the hydrogen 
atom has only one electron we are interested in
the emphlowest eigenvalue and its corresponding eigenvector.
Only these solutions have a physical meaning and correspond to the 
hydrogen atom ground state energy and the ground state wavefunction.
<p>
Some of you are probably guessing that the number
N is arbitrary and that perhaps the larger it is the better the
corresponding solution to our original problem may be.  RIGHT!.  BUT,
there is a big price to pay if we try to increase <IMG
 WIDTH="27" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img15.png"
 ALT="$ N$">
 above a
certain, reasonable number. The number of numerical operations
to solve Eq. (<!-- MATH
 $ref{equ:geneing1}$
 -->
<IMG
 WIDTH="105" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img27.png"
 ALT="$ ref{equ:geneing1}$">
) is at least <IMG
 WIDTH="127" HEIGHT="28" ALIGN="BOTTOM" BORDER="0"
 SRC="img28.png"
 ALT="$ N cdot N$">
 - we say that 
the algorithm which
solves Eq. (<!-- MATH
 $ref{equ:geneing1}$
 -->
<IMG
 WIDTH="105" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img27.png"
 ALT="$ ref{equ:geneing1}$">
) scales as emphO<IMG
 WIDTH="59" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img11.png"
 ALT="$ (N^2)$">
 with the size <IMG
 WIDTH="27" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img15.png"
 ALT="$ N$">
 
of a problem.
Imagine, if your <IMG
 WIDTH="27" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img15.png"
 ALT="$ N$">
 is 10 you have (roughly!, and this number
should be understood only logarithmically)&nbsp; <!-- MATH
 $10 cdot 10 = 100$
 -->
<IMG
 WIDTH="59" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img29.png"
 ALT="$ 10 cdot 10 = 100$">
 operations
to do.  If your <IMG
 WIDTH="27" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img15.png"
 ALT="$ N$">
 is 100 you are looking at 10000 operations,
if your <IMG
 WIDTH="27" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img15.png"
 ALT="$ N$">
 is 1000 you have 1000000 operations, etc.  Scary, isn't it?
And what if I tell you that solving Eq. (<!-- MATH
 $ref{equ:geneing1}$
 -->
<IMG
 WIDTH="105" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img27.png"
 ALT="$ ref{equ:geneing1}$">
)
involves a very
cumbersome and difficult to program algebra? What about optimization?
Does this all seem hopeless?
Before you decide not to become a quantum chemist let me share
the good news with you.  We do not have to do all the work
by ourselves! We have LAPACK, which you have just compiled,  at
our disposal.  It will solve, as you will see in our next lab, 
our problem expressed by Eq. (<!-- MATH
 $ref{equ:geneing1}$
 -->
<IMG
 WIDTH="105" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img27.png"
 ALT="$ ref{equ:geneing1}$">
)
in emphjust one call to one of its driver routines.
<p>
We will now work with you through the code example to show you 
how this is done.
<br>
<br> Login as a regular user. 
<br> Copy the FORTRAN 90 source code, as well as the input file
textttALPHA_IN_5, and textttcompile.hatomf90 and 
textttlink.hatomf90 files into a directory of your choice.
<br> You should have:
<br>
<br> textttALPHA_IN_5
<br> textttcompile.hatomf90
<br> textttconstants.f90
<br> textttdeclarations.f90
<br> textttdsygvf77wrapper.f90
<br> texttteigenproblem.f90
<br> texttthydrogenmain.f90
<br> textttioerrors.f90
<br> textttlink.hatomf90
<br> textttmatrixelements.f90
<br> textttprecision.f90

in your directory.  That's 11 files.
<br> The input file textttALPHA_IN_5 contains 5 exponent
coefficients <IMG
 WIDTH="107" HEIGHT="15" ALIGN="MIDDLE" BORDER="0"
 SRC="img30.png"
 ALT="$ alpha$">
 (cf. Eq. (<!-- MATH
 $ref{equ:basis}$
 -->
<IMG
 WIDTH="37" HEIGHT="16" ALIGN="MIDDLE" BORDER="0"
 SRC="img22.png"
 ALT="$ ref{equ:basis}$">
)) and 3 parameters
used in plotting the results.
<br> Refer to the code for explanations of the modules and subroutines.
It is a good programming habit to comment the code itself so that the 
explanations are always there when you need them.  In particular,
make sure that you understand how the call to the general eigenproblem
solver dsygv is made and what it returns.  This call is made in subroutine 
textttsolve_eigenproblem in file texttteigenproblem.f90
<br> Compile and link the code as suggested in newline
textttcompile.hatomf90
and textttlink.hatomf90 using the Portland Group Compiler.  
Notice the flag texttt-g77libs when linking using pgf90 driver
with libraries created by g77. By default pgf90 does not look
for g77 support libraries to resolve references specific to g77 
objects! 
<br> textttcp ALPHA_IN_5 ALPHA_IN
<br> Run the program by typing texttthatom
<br> You should now find the following output files in your
directory:
<br>
<br> textttCOMPUTED_WAVEFUNCTION - this file contains the values
of the hydrogen atom ground state electronic wavefunction (1S) as a function 
of the electron's distance from the nucleus, computed approximately
from the Ritz variational principle.   Remember this state is
emphspherically symmetric.
<br> textttEXACT-COMPUTED_WAVEFUNCTION - this file contains the
difference values between the linebreak
textttEXACT_WAVEFUNCTION described below
and newline
the textttCOMPUTED_WAVEFUNCTION described above.  Notice 
that by expanding our trial wavefunction, (cf. Eq. (<!-- MATH
 $ref{equ:basis}$
 -->
<IMG
 WIDTH="37" HEIGHT="16" ALIGN="MIDDLE" BORDER="0"
 SRC="img22.png"
 ALT="$ ref{equ:basis}$">
)),
into only 5 primitive gaussians we get a very good agreement with
the true ground state wavefunction.  Due to the shape of Gaussian 
functions (which we emphchose to use) the agreement at <IMG
 WIDTH="41" HEIGHT="27" ALIGN="BOTTOM" BORDER="0"
 SRC="img31.png"
 ALT="$ r=0$">

is worse.  Physicists know how to improve this agreement by
employing more sophisticated basis sets.
<br> textttEXACT_WAVEFUNCTION - remember when we linebreak 
told you that
the hydrogen atom is one of the very few systems for which the 
analytical solution is known?  This file contains exact values
of the hydrogen atom ground state electronic wavefunction as a function
of the electron's distance from the nucleus - 1S state, tabulated from the
analytical expression:
beginequation
Psi(bf r) = frac1sqrtpie^-r
labelequ:analytic
endequation
<br> textttGROUND_STATE_ENERGY - this file contains the value
of the lowest eigenvalue <IMG
 WIDTH="82" HEIGHT="16" ALIGN="MIDDLE" BORDER="0"
 SRC="img20.png"
 ALT="$ epsilon$">
 which is the ground state energy
of our hydrogen atom system.
<br> textttHAMILTONIAN_MATRIX - this file contains elements of
the <!-- MATH
 $underline{hmatrix}(N,N)$
 -->
<IMG
 WIDTH="176" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img24.png"
 ALT="$ underline{hmatrix}(N,N)$">
 described earlier.
<br> textttOVERLAP_MATRIX - this file contains elements of
the <!-- MATH
 $underline{smatrix}(N,N)$
 -->
<IMG
 WIDTH="99" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img23.png"
 ALT="$ underline{smatrix}(N,N)$">
 described earlier.
<br> textttLOWEST_ENERGY_EIGENVECTOR - this file contains elements
of the eigenvector <IMG
 WIDTH="52" HEIGHT="27" ALIGN="BOTTOM" BORDER="0"
 SRC="img21.png"
 ALT="$ vec{C}$">
 corresponding to the lowest eigenvalue 
<IMG
 WIDTH="82" HEIGHT="16" ALIGN="MIDDLE" BORDER="0"
 SRC="img20.png"
 ALT="$ epsilon$">
.  
These elements are the expansion coefficients in Eq. (<!-- MATH
 $ref{equ:basis}$
 -->
<IMG
 WIDTH="37" HEIGHT="16" ALIGN="MIDDLE" BORDER="0"
 SRC="img22.png"
 ALT="$ ref{equ:basis}$">
).

<br> Simply looking at numbers in newline
textttCOMPUTED_WAVEFUNCTION, newline
textttEXACT_WAVEFUNCTION newline
or textttEXACT-COMPUTED_WAVEFUNCTION newline
is usually inferior to having these results properly graphed.  As an added 
exercise we will now install one of the best 2-D graphing Linux 
tools (complete, with TeX interface!).  The best part is 
that it is freely available on the net. 
<p>
beginfigure[hbt]
vspace7mm
centering
includegraphics [width=9.0cm] 300grace.eps
caption Calculated (<IMG
 WIDTH="38" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img32.png"
 ALT="$ N=5$">
) and exact values for the 1S state of the hydrogen
atom.  Notice a very good agreement for all points but the ones close to
the nucleus.
labelfig:500one
endfigure
<p>
beginfigure[hbt]
vspace7mm
centering
includegraphics [width=9.0cm] 300diffgrace.eps
caption Difference between the exact and calculated (<IMG
 WIDTH="38" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img32.png"
 ALT="$ N=5$">
)
values for the 1S state of the hydrogen atom.
labelfig:500two
endfigure
<p>
<br>
<br> Become a superuser.
<br> textttmkdir /usr/local/grace 
<br> Download grace from newline
texttthttp://plasma-gate.weizmann.ac.il/ newline 
Grace to texttt/usr/local/grace
<br> Unzip and untar textttgrace-5.1.16.tar.gz
<br> textttcd grace-5.1.16
<br> texttt./configure -enable-grace-home= newline 
/usr/local/grace
<br> textttmake
<br> textttmake tests and close all the test instances of 
xmgrace - be patient.
<br> textttmake install
<br> textttmake links
<br> Login as a regular user and go to your hatom example directory.
<br> Start grace by typing newline
texttt/usr/local/grace/bin/xmgrace
<br> Graph data in files newline
textttCOMPUTED_WAVEFUNCTION, newline
textttEXACT_WAVEFUNCTION newline
and textttEXACT-COMPUTED_WAVEFUNCTION newline
You should obtain figures similar to Figs. (reffig:500one)
and (reffig:500two). 
<p>


<p>
Building your own libraries
<p>
As you develop your own suite of software you may find yourself
reusing certain routines over and over.  In this case, instead 
of recompiling them every time, you will want to include them 
in one object file which you will compile once and for all (at least until
you find a bug in one of these routines) and which you can then link 
with your main program.  Returning to our example: say, you discovered 
that the routines
included in files textttprecision.f90 and textttioerrors.f90
in our example look sort of like utilities which you want to use 
every time with all the code you write.  You want to create a library
called libutilities.a and use it every time you link your
objects.  You can proceed as follows: 
<br>
<br> textttar rcs libutilities.a precision.o newline
ioerrors.f90
<br> ...and link with newline
texttt-L/&lt;directory_where_libutilities.a newline
_resides&gt; -lutilities

Try this as an exercise.
<p>


<p>

<p>Introduction to MPI Programming in C
<p>
it MPI or it Message Passing Interface indexMPI is not a programming 
language.  It is a standard for implementing message passing parallel 
programming. It is in most implementations a library of functions or 
subprograms that can be called from other languages, most often C or Fortran.
MPI grew out from the frustrations of programmers and researchers whose 
parallel code was not portable due to proprietary programming interfaces.  
It was developed by a forum with members from academia, government, and 
industry.  MPI allows development of portable and efficient parallel code.  
These libraries make many of the lower level intricacies of parallel 
programming transparent to the programmer.  
<p>
MPI is a form of message passing parallel programming, which is a very popular
parallel programming model. In this paradigm, the processes run 
independently, and use communications for synchronizing their efforts.
For those of you with an interest in the theory
behind parallel programming, MPI is a form of SIMD or SPMD parallel 
architecture.  This means that all of the processors run the same program, 
but on different data.  Unlike a pure SIMD architecture, MPI does not force
the program to run exactly the same on all processors, but allows for 
differential execution.
<p>
These notes are not intended to be an in depth exposition of
even the introductory topics that are covered. Rather they are intended to
lead you through the examples which should give you adequate exposure to 
begin using MPI. For more in depth materials please see  
ref:addmat which lists additional pedagogical materials.
<p>
There are several commercial and free versions of these libraries.  The 
two most popular non-commercial implementations are it LAM-MPI and 
it MPICH.  We will be using MPICH for this class. 
<p>
 A first program
label:afp
We will start with the traditional "Hello World" program.  This will allow us
to take a detailed look at the minimal code necessary to run an MPI program.
The code for this and the other examples in this <p> are include in the 
bccode directory on the lab machines.  

#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;mpi.h&gt;

main(int argc,char* argv[]){
	
	int				my_rank;
	int				p;
	int				source;
	int				dest;
	int				tag=0;
	char			message[100];
	MPI_Status		status;

	/*Start MPI*/
	MPI_Init(&amp;argc,&amp;argv);

	/*Find out process rank*/
	MPI_Comm_rank(MPI_COMM_WORLD,&amp;my_rank);

	/*Find out the number of processes*/
	MPI_Comm_size(MPI_COMM_WORLD,&amp;p);

	if(my_rank !=0){
		/* Create Message */
		sprintf(message,"Hello from process %d!",my_rank);
		dest = 0;
		/*Use strlen+1 so that '\0' gets transmitted */
		MPI_Send(message,strlen(message)+1,
					 MPI_CHAR,dest,tag,MPI_COMM_WORLD);
	}else{
		for (source =1;source&lt;p;source++){
			MPI_Recv(message,100,MPI_CHAR,
				  source,tag,MPI_COMM_WORLD,&amp;status);
			printf("%s\n",message);
		}
	}

	/* Shutdown MPI */
	MPI_Finalize();

}

In C, MPI programs require the include file mpi.h.
MPI defines many internal datatypes, and the first one used in this program
is it MPI_Status. MPI_Status is a structure
that contains the fields MPI_SOURCE, MPI_TAG, and MPI_ERROR. It is used to 
return the status of the MPI communications functions. 
For now just think of the status variable as the place where MPI puts the 
error codes for its functions.
<p>
Now we will start looking at the MPI functions used in this program.  The first
is MPI_Init which has the following definition:
<p>
 int MPI_Init(int *argc, char ***argv)<br>
<p>
MPI_Init must be the first MPI function call in your program, as it initializes
the state of the program for all other MPI calls.  Although the call to 
MPI_Init allows the passing of the command line parameters, their use is not
defined in the MPI standard.  This means that using them is not necessarily 
portable, and that they should be used with caution.  Their use is beyond the
scope of this class. Also note that for most installations, you can get 
information on the MPI functions by using the man command.
<p>
With the next call we begin to delve into some of the mechanisms of MPI 
programming.  
<p>

   /*Find out process rank*/
   MPI_Comm_rank(MPI_COMM_WORLD,&amp;my_rank);

   /*Find out the number of processes*/
   MPI_Comm_size(MPI_COMM_WORLD,&amp;p);

<p>
Both MPI_Comm_rank and MPI_Comm_size have as their first parameter 
the constant MPI_COMM_WORLD, which is of type MPI_Comm. 
An MPI_Comm is the data type used to reference a it communicator.  
A communicator indexcommunicator 
is a collection of processes.  As your programs and the underlying tasks 
become more complicated, you may need to set up different groups of processors
to accomplish different tasks and may need to create your own communicators.  
MPI by default sets up the 
global communicator MPI_COMM_WORLD.  The MPI_Comm_rank command returns the 
process number or "rank" within the communicator. 
MPI_Comm_size puts the size of the given communicator into the
location pointed to by the second parameter.  The rank of the process and the 
communicator's size are often used to control the local flow of the program.
We can see this in the next code segment.
<p>

 
if(my_rank !=0){
   /* Create Message */
   sprintf(message,"Hello from process %d!",my_rank);
   dest = 0;
   /*Use strlen+1 so that '\0' gets transmitted */
   MPI_Send(message,strlen(message)+1,
			MPI_CHAR,dest,tag,MPI_COMM_WORLD);
}else{
   for (source =1;source&lt;p;source++){
	  MPI_Recv(message,100,MPI_CHAR,
		 source,tag,MPI_COMM_WORLD,&amp;status);
	  printf("%s\n",message);
   }
}

<p>
In this snippet we see two very common MPI constructions.  First the it
if .. else construction where one (or more) process(es) are executing one
set of instructions, and another set of processes are executing another.  
In this case we see that each process other than the root process (rank = 0)
is sending a message.  Meanwhile, the root process is receiving messages
in a for loop which runs from 1 to the size of the communicator.  
Note that there is nothing in this code that forces an order on the 
communications from the non-root processes.  Rather they will each run the
code asynchronously.  One might ask how is it that the root process is able
to receive the messages in the correct order?  To answer that question, we
must take a more in depth look at the it MPI_Recv and it MPI_Send
commands. This is the subject of the next , but we will take a brief
look at these commands here, paying particular attention to the parameter 
types and what they mean.
<p>
The MPI_Send command has the definition:

int MPI_Send( void *buf, int count, 
			   MPI_Datatype datatype, int dest,
			   int tag, MPI_Comm comm )

<p>
begindescription
   <br> [buf] This is the initial address of the send buffer, in our case it is
the beginning of the string message.  
   <br> [count] Number of elements in send buffer. We set this to the length
of the string message +1 to account for the null terminator character. The 
count is not the number of bytes but number of <br>s.  This allows more
complex data types to be handled easily, and maintains portability.
   <br> [datatype] Datatype of each send buffer element.  Note that this is
not a standard C datatype but one of the data types defined by MPI. Although
MPI has many datatypes and functions for manipulating them, our examples will
draw from the basic types given in the following list.
   <br>
	  <br> MPI_CHAR
	  <br> MPI_SHORT
	  <br> MPI_INT
	  <br> MPI_LONG
	  <br> MPI_UNSIGNED_CHAR
	  <br> MPI_UNSIGNED_SHORT
	  <br> MPI_UNSIGNED
	  <br> MPI_UNSIGNED_LONG
	  <br> MPI_FLOAT
	  <br> MPI_DOUBLE
	  <br> MPI_LONG_DOUBLE
	  <br> MPI_BYTE
	  <br> MPI_PACKED
   
   <br> [dest] Rank of the destination.
   <br> [tag] The use of the message tag is left to the program. It may be 
used to classify messages. 
   <br> [comm] Communicator.
enddescription
<p>
The MPI_Recv has the definition:

int MPI_Recv( void *buf, int count, 
			   MPI_Datatype datatype, int source, int tag, 
			   MPI_Comm comm, MPI_Status *status )
 
<p>
Let's take a look at the differences between MPI_Send and MPI_Recv.
The it buf in the MPI_Recv call is the initial address of the receive buffer 
instead of the send buffer.  We have replaced dest or the destination of the
message with the source of the message.  MPI_Recv also returns a MPI_Status
variable as described above.  
<p>
With just a little more information we can now answer the question, how does 
the root process receive the messages in order. MPI buffers messages. Although
MPI does not guarantee that the messages from different processes arrive 
in any specific order, the root process reads them  from the buffer in 
processor number order.
MPI does however insure that messages from the same processor do
arrive in order.  We will explore other types of point-to-point
communications and their details in the following .
<p>
<br>Compiling and running MPI programs
To compile MPI programs you must make sure that the MPI_HOME/include 
directory is in your include path, and that the MPI_HOME/lib directory is in your library path.  On the lab machines this should be the default 
configuration. If these requirements are met 
the program above can be compiled with the command:

   gcc -o HelloWorld HelloWorld.c -lmpi

On most installations of MPI there is some script to make sure that you
have included the correct libraries and paths.  For our machines the command is
it mpicc.  So you can replace the above command with:

   mpicc -o HelloWorld HelloWorld.c

<p>
Once the program is compiled you can use the it mpirun command to run the
program.  The following example should work on your lab machine:

mpirun -np 4 -machinefile /usr/local/nodes HelloWorld

<p>
<br>A note on output
Getting output from an MPI program is not as straightforward as it is in
serial programming.  This is a topic of much discussion, and research.  In
general if you put a print statement into an MPI program, it will not 
necessarily print in some convenient fashion.  Remember that each node 
operates independently. Suppose that an MPI program has the following code,
and that the float value in the second printf is equal to the rank of the
process.

printf("my rank is %d\n",local_rank);   
...
processing code
...
printf("my value is %3.1f\n",val);

The output from this bit of code could be in any order.

my rank is 0
my rank is 1
my rank is 2
my value is 0.0 
my value is 1.0
my value is 2.0

or

my rank is 0
my value is 0.0
my rank is 2
my rank is 1
my value is 1.0
my value is 2.0

The only thing that you can be sure of is that the program will run sequentially
on each processor.  There are advanced methods of I/O for MPI, and there are
some parallel I/O calls that you may want to explore as you become more
familiar with MPI.  For now you should do one of two things.  If you want to 
have nice tidy output, send messages to the root process and print from
there, or make sure that each line of output includes the rank of the process
so that you can be sure of which process is doing the output.
<p>
<br>Exercises
label:basicMPIex
<br>
   <br> Hello Neighbor - Modify the Hello World Program so that each processor
sends a message to the processor of the next higher rank, and receives a 
message from the next lower rank.  The root process 0 should not receive a 
message, and the processor with the largest rank should not send a message.
After all of the messages are sent and received, all of the non root processes
should send the root process a message with their received message.  The 
output should look something like:

process 1 received message Hello from process 0
process 2 received message Hello from process 1
   ...

   <br> Advanced Exercise - On each non-root processor, define a one 
dimensional array of integers and fill it with the product of the processor
number and the <br> of the location in the array.  Send this vector to root. 
On the root processor receive the vectors into rows of a matrix.  For 
simplicity, assume that there are four processors and each vector has length
3.  Then print out the matrix from the root process. The output should look 
like:

   0 1 2
   0 2 4
   0 3 6


<p>
 Point to Point Communications
In this , we will examine point to point message passing
which allows data transfer from one process to another.  Unlike
collective communications, point to point message passing involves
only a pair of processes.
<p>
Sending a scalar variable or an array from one process requires
calling one of the MPI send subroutines. This initiates data transfer from
the user buffer to the system buffer. The corresponding receive
call, on the other hand, allows data to be copied from the system
buffer to the user buffer in the destination process. There is a
variety of MPI send subroutines allowing communication in
different modes  - synchronous, buffered, ready, standard blocking
and non-blocking.  Unlike send, there are only two types of
receive subroutines: standard blocking and nonblocking. Here, we
will focus on blocking and nonblocking communications, as they are
the most commonly used modes.
<p>
When we use the blocking send subroutine, MPI_Send, control does
not return to the program until the data transfer to the system
buffer is complete. Similarly, control returns from the blocking
receive subroutine, newline MPI_Recv, only after the data is copied to the
user buffer. On the other hand, nonblocking communication
subroutines, MPI_Isend and MPI_Irecv, indicate that the data
transfer has only begun. Control immediately returns to the
program while data transfer continues in the background. Note that
incorrect communications occur if we change the buffer content
before data transfer is complete. Therefore, somewhere in the
program, we have to make sure that the communication is over by
calling  MPI_Wait. The format for  MPI_Wait is as follows:
<p>

int MPI_Wait (
               MPI_Request  *request,
               MPI_Status   *status)

<p>
Now, let us take a look at the following example program where a
different scalar variable is initialized on each process.  The
processes communicate and calculate the sum of these variables.
To handle the boundry conditions we introduce the it MPI_PROC_NULL
constant.indexMPI_PROC_NULL This constant is the equivalent of
using /dev/null in normal programming.  Messages sent to MPI_PROC_NULL
are treated as if the send and receive completed immediately, but not
communications occur. 
<p>

/* This program shows the correct and incorrect use*/
/*of the MPI_Isend and MPI_Irecv functions */

#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;mpi.h&gt;


main(int argc, char* argv[]){
   int my_rank,root=0;
   int nprocs;
   int inext,iprev,result=0;
   int a=0,b=0,c=0,d=0,f=0,e=0;
   MPI_Status status;
   MPI_Request sendReq1,sendReq2,recvReq1,recvReq2;
   
   /*Start MPI*/
   MPI_Init(&amp;argc, &amp;argv);
   
   /*Find out process rank*/
   MPI_Comm_rank(MPI_COMM_WORLD, &amp;my_rank);
   
   /*Find out the number of processes*/
   MPI_Comm_size(MPI_COMM_WORLD, &amp;nprocs);
  
   /*Define next and previous processes in */
   /*terms of ranks*/
   inext=my_rank+1;
   iprev=my_rank-1;
   
   /* define proc null for falling off the end */
   if (my_rank == (nprocs-1)) inext=MPI_PROC_NULL; 
   
   if  (my_rank == root) iprev=MPI_PROC_NULL;

   /* Initialize a in process 0 and b in process 1 */
   a = my_rank;
   b = my_rank+10;
   d = my_rank;
   e = my_rank+10;
   /*    Data exchange    */
   MPI_Isend(&amp;a,1,MPI_INT,
      inext,0,MPI_COMM_WORLD,&amp;sendReq1);

   MPI_Isend(&amp;b,1,MPI_INT,
      inext,0,MPI_COMM_WORLD,&amp;sendReq2);

   MPI_Irecv(&amp;a,1,MPI_INT,
      iprev,0,MPI_COMM_WORLD,&amp;recvReq1);

   MPI_Irecv(&amp;b,1,MPI_INT,
      iprev,0,MPI_COMM_WORLD,&amp;recvReq2);


   /*   Calculate c  and d before MPI_WAIT */
   c=a+b;
   f=d+e; 
   printf("Process %d before wait c= %d f= %d\n",
			 my_rank,c,f);
	    
   MPI_Wait(&amp;sendReq1,&amp;status);
   MPI_Wait(&amp;sendReq2,&amp;status);
   MPI_Wait(&amp;recvReq1,&amp;status);
   MPI_Wait(&amp;recvReq2,&amp;status);

   /*   Calculate c after MPI_Wait */
   c=a+b;
   f=d+e; 
   printf("Process %d after wait c= %d f=%d\n", 
			   my_rank,c,f);

   /* Shutdown MPI */
   MPI_Finalize();

}

*********************************************
Output from program (Sorted for easy reading)
Process 0 before wait c= 10 f= 10
Process 0 after wait c= 10 f=10
Process 1 before wait c= 12 f= 12
Process 1 after wait c= 10 f=12
Process 2 before wait c= 14 f= 14
Process 2 after wait c= 12 f=14

<p>
This program generates a wrong value for c when it is calculated
before the MPI_WAIT call.  MPI_Wait is used to block both
sending and receiving processes until the communication is
complete. Since the variable f can be calculated correctly during
data exchange, it is safe to post MPI_Wait calls afterwards. On
the other hand, placing MPI_Wait just after the immediate Send
and Receive calls would be the same as using blocking
communications. Please note that, to reduce synchronization
overhead, we should post MPI_Wait in the program as late as
possible. Notice one other aspect of this program.  The values are
correct for Process 0, because it receives from MPI_PROC_NULL.  
So special care must be taken in debugging these situations.
<p>
We use immediate calls because they are faster than blocking
communications. Also, blocking calls may fail when the sent
message is too large.
<p>
<br>Transferring Data from Arrays
We specify the location of the first element to be sent or
received when we call MPI communication subroutines. For a
successful communication, the rest of the elements to be transferred
must be contiguous in the memory. Since C stores the
elements of a two-dimensional array in row major order, sending
a row of an array is straightforward.  Let us examine the
following program that computes the elapsed time for the data
exchange between two processes.
<p>

/* Demonstration of communicating an array */
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;mpi.h&gt;
#define ARRAY_SIZE 25
main(int argc,char* argv[]){
	
   int				my_rank;
   int				p;
   int				source;
   int				dest;
   int				tag=0;
   int			    i,j;
   double		    a[ARRAY_SIZE][ARRAY_SIZE]; 
   double		    b[ARRAY_SIZE][ARRAY_SIZE]; 
   double		    t1,t2,resol;
   char			    message[100];
   MPI_Status		status;

   /*Start MPI*/
   MPI_Init(&amp;argc,&amp;argv);
   /*Find out process rank*/
   MPI_Comm_rank(MPI_COMM_WORLD,&amp;my_rank);
   /*Find out the number of processes*/
   MPI_Comm_size(MPI_COMM_WORLD,&amp;p);

   resol=MPI_Wtick();
   printf("clock resolution %f\n",resol);

   if (my_rank==0){ 
      for(i=0;i&lt;ARRAY_SIZE;i++)
        for(j=0;j&lt;ARRAY_SIZE;j++){
           a[i][j]=1.0;
           b[i][j]=0.0;
        }
   }        
   if (my_rank==1){
      for(i=0;i&lt;ARRAY_SIZE;i++)
        for(j=0;j&lt;ARRAY_SIZE;j++){
           a[i][j]=0.0;
           b[i][j]=2.0;
        }
   }
   MPI_Barrier(MPI_COMM_WORLD);

   if (my_rank==0){
        t1=MPI_Wtime();

      MPI_Send(a,ARRAY_SIZE*2,
         MPI_DOUBLE,1,tag,MPI_COMM_WORLD);
      MPI_Recv(b,ARRAY_SIZE*2,
         MPI_DOUBLE,1,tag,MPI_COMM_WORLD,&amp;status);

      t2=MPI_Wtime();
	  printf("communication time %10.5f process %d\n",
			  t2-t1,my_rank);
   }
   if (my_rank==1){
      t1=MPI_Wtime();
      MPI_Send(b,ARRAY_SIZE*2,
         MPI_DOUBLE,0,tag,MPI_COMM_WORLD);
      MPI_Recv(a,ARRAY_SIZE*2,
         MPI_DOUBLE,0,tag,MPI_COMM_WORLD,&amp;status);
      t2=MPI_Wtime();
	  printf("communication time %10.5f process %d\n",
			  t2-t1,my_rank);
   }
   MPI_Finalize();
}

First, let us explain briefly some of the subroutines we have used
for the first time. MPI_Wtick is used to determine the clock
resolution and MPI_Wtime gives the elapsed time.
<p>
The MPI_Barrier subroutine blocks each process until each one has
called it. It is used for synchronization. In other words,  it
ensures that the processes start sending and receiving at the same
time. MPI_Barrier can be used for blocking multiple processes; it
is a collective communication subroutine.  We will see more about
collective communications in the next .
<p>
In this program, each process transfers the first and second
rows of the array initialized on them. If we increased the
number of rows to be transferred more and more, MPI_Send and
MPI_Recv would eventually fail. In this case, non-blocking
calls would still be working.
sub<br>Derived Data Types
Now, let us see how to transfer columns in an array.  A new data type
must be defined in order to send and receive columns because this
time the data is not contiguous in the memory. The following
Send/Receive pair can be used to transfer the first column of the
following array.

            1 2 3 4
            1 2 3 4
            1 2 3 4
            1 2 3 4

<p>

   MPI_Type_vector(4,1,4,MPI_INT,&amp;coltype);
   MPI_Type_commit(&amp;coltype);

   if (my_rank==0)
     MPI_Send(a,1,coltype,
         1,tag,MPI_COMM_WORLD);

   if (my_rank==1) 
      MPI_Recv(a,1,coltype,
         0,tag,MPI_COMM_WORLD,&amp;status);

   MPI_Type_free(&amp;coltype);

<p>
The MPI_Type_vector subroutine is used to define a new data type
representing equally spaced blocks.  It has the following format:
<p>

int MPI_Type_vector(
               int count,
               int blocklen,
               int stride,
               MPI_Datatype old_type,
               MPI_Datatype *newtype )

<p>
The MPI_Type_commit makes the new data type ready to be used in
communication. The new data type is freed by the MPI_Type_free
call. In other words, the MPI_Type_free routine sets the new
data type to newline MPI_DATATYPE_NULL.
<p>
<br>Exercises
<br>
<br> On each process, define a 4X4  array.  The first process
will have 1s in the first row, the second process will have 2s
in the second row and so on. The rest of the elements of the
array will be zero. Using blocking communication subroutines,
complete the array on each process. Assume that the number of
processes is 4. The output should look like as follows.
<p>

            1 1 1 1
            2 2 2 2
            3 3 3 3
            4 4 4 4

Rewrite the program using non-blocking communications.
<p>
<br> Advanced Exercise: Modify the array of the previous exercise
such that only one column should contain nonzero elements initially.
Complete the array on each process using derived data types and
point to point communications.

<p>
Collective Communications
label:collective_communications
Until this point we've only discussed communications coming from a single 
process and going to a single process.  MPI also contains it collective
communications indexcollective communications which loosely defined are 
those communications that involve multiple processors on the sending end, 
the receiving end or both.  The MPI_Barrier command that was described
in the last , is often included as a collective communication. In the
following s we will describe some of the most commonly used 
communications of this type.  The reader should be aware that this is a 
rich, complex and useful area within MPI, and these notes barely scratch the
surface.
<br>One to All
<br>:bcast
To send messages from one process to all other processes MPI contains the
function MPI_Bcast. indexMPI_Bcast The format for MPI_Bcast is
<p>

int MPI_Bcast ( void *buffer, int count, 
			   MPI_Datatype datatype, 
			   int root,MPI_Comm comm )

<p>
There are several differences between MPI_Bcast and the point-to-point 
communications that we have looked at in the previous s.  First both
the sender and the receivers use the same function call. Root in the case of
MPI_Bcast is the rank of the process sending and it must be the same on all
processes or there will be an error.  
<p>
It is important to understand that although you can 
accomplish the same data transfer by using individual sends and receives, 
using MPI_Bcast is more efficient.  This is generally true of the collective
communications.  Let's take a brief look at why.  When you do individual sends 
and receives, at each iteration of the loop there is one message being sent 
from the sending process to each of the receiving processes.  There are other
possible methods of getting the data to each process that are more efficient.
Let's assume for the sake of simplicity that the root process in our 
communication is process 0, and we have 8 nodes. In this case we could 
send the communications in the manner depicted in Figure reffig:bcastC.
<p>
beginfigure[b]
    centering
    epsffilebcast.eps
<p>
caption A more efficient communication scheme
    labelfig:bcastC
endfigure
<p>
Notice that this completes the communication in three timesteps instead of 
the eight needed by sending all messages from root.  This is not to say that
TCP/IP communications are this simple, or that there aren`t more efficient
schemes. The efficiency of such a scheme is dependent on the underlying 
network topology. Fortunately, we don't have to come up with an 
efficient scheme each
time we want to distribute data.  The implementations of MPI_Bcast handle the
details for us.  This is especially important when one considers portability
issues.  With out using the MPI built in routines, you would have to determine
the best scheme each time you wanted to run the program in a different 
environment, or even a different number of processors.
<p>
<br>All to one
<br>:reduce
So you're probably thinking that if MPI has an efficient method of broadcasting
from one process to all of the others, that there should be some way of 
collecting data from all of the nodes onto one.  There are several.  
The first ones that we will examine involve reduction operations.  For example,
suppose that you have a value on each processor and that you wanted to 
get a global sum of these values.  If we send all of the data to the root node
by using individual calls we have created several bottlenecks in the program.
Also, once
the data is collected the other processors must wait until the root processor
is finished with the calculation before they can proceed. (If they need the
result.)  The function MPI_Reduce takes care of the first of these problems. 
<p>

int MPI_Reduce ( void *sendbuf,void *recvbuf,
			    int count, MPI_Datatype datatype, 
			   MPI_Op op,int root,MPI_Comm comm )

<p>
Lets take a look at an example. Assume that all of the variables are define
appropriately.

MPI_Reduce(&amp;val,&amp;total,1,MPI_INT,MPI_SUM,0,
						MPI_COMM_WORLD);

<p>
Each process calls MPI_Reduce with the same parameters.  Even though
the sum is accumulated only on process 0. After the call above, the
variable total will contain the global sum.  What if you want to do some
other operation than summation? MPI predefines several operations for
the parameter MPI_Op. It can take on the following values. 
begindescription
    <br> [MPI_MAX] maximum
	<br> [MPI_MIN] minimum
	<br> [MPI_SUM] sum
	<br> [MPI_PROD] product
	<br> [MPI_LAND] logical and
	<br> [MPI_BAND] bit-wise and
	<br> [MPI_LOR] logical or
	<br> [MPI_BOR] bit-wise or
	<br> [MPI_LXOR] logical xor
	<br> [MPI_BXOR] bit-wise xor
	<br> [MPI_MAXLOC] max value and location of maximum
	<br> [MPI_MINLOC] min value and location of minimum
enddescription
<p>
The operators MPI_MAXLOC and MPI_MINLOC use predefined MPI datatypes to
be able to return the value and the location.
<p>
While MPI_Reduce solves the problem of efficiently communicating the data to
the root node it leaves open the question of how to distribute the result to 
all of the processes. We could follow the call to MPI_Reduce
with a call to MPI_Bcast, but this isn't the most efficient method. 
Instead MPI uses a communication structure called the it butterfly which
communicates the results of partial operations in such a way that at the 
end of the call 
all of the processors have the result.  The details of this communication scheme
are beyond the scope of these notes, but it should suffice at this stage to
assume that for doing one of the global reduction operations and having
the result be available to all of the processors, MPI_Allreduce is the
function of choice. The definition of MPI_Allreduce is given below.
<p>

int MPI_Allreduce ( void *sendbuf, void *recvbuf, 
					 int count, MPI_Datatype datatype, 
					 MPI_Op op, MPI_Comm comm )

<p>
<br>Scatter and Gather
<br>:scattergather
We have looked at collective communications that take the same data and 
communicate it to all processors, and those that reduce data from all processes
to one.  But these communication functions will not allow us send different 
information to each process.  Nor will they allow us to collect separate bits
of information from all of the processes onto one.  For these tasks we can
use MPI_Scatter and MPI_Gather.  
<p>

int MPI_Scatter (
               void *sendbuf,
               int sendcnt,
               MPI_Datatype sendtype,
               void *recvbuf,
               int recvcnt,
               MPI_Datatype recvtype,
               int root,
               MPI_Comm comm )

int MPI_Gather ( 
               void *sendbuf, 
               int sendcnt, 
               MPI_Datatype sendtype,
               void *recvbuf, int recvcount, 
               MPI_Datatype recvtype,
               int root, 
               MPI_Comm comm )

<p>
The meanings of these parameters are similar to previous commands that we have
studied.  The sendcnt is usually the same as the recvcnt, and the sendtype
is usually the same as the recvtype. We can look at the follow example to see
how MPI_Scatter works.  
<p>

	
main(int argc, char * argv[]){
     int i,vec[4]={0,0,0,0};
     // Initialize MPI 
     MPI_Init(&amp;argc,&amp;argv);
     MPI_Comm_size(MPI_COMM_WORLD,&amp;numOfProcessors);  
     MPI_Comm_rank(MPI_COMM_WORLD,&amp;localRank);
     if(localRank==0){
         for(i=0;i&lt;4;i++) vec[i]=i*2;
     }
     MPI_Scatter(&amp;vec,1,MPI_INT,&amp;vec,1,MPI_INT,0,
                          MPI_COMM_WORLD);

     MPI_Finalize();
}

<p>
This code will distribute each of the entries of vector vec to the
processes in MPI_COMM_WORLD.  Figure reffig:scatC shows the effects 
of the MPI_Scatter command on the memory for each process. Note that the
blank memory locations are really set to a value of 0.
beginfigure[h]
    centering
    epsffilescat.eps
<p>
caption The effects of MPI_Scatter
    labelfig:scatC
endfigure
<p>
MPI_Gather is in some sense the inverse operation to MPI_Scatter.  Instead 
of distributing values, it gathers them together.  The following code and
Figure reffig:gatherC show the effects of the MPI_Gather operation.
<p>

main(int argc, char * argv[]){
   int i,vec[4]={0,0,0,0};
	// Initialize MPI 
	MPI_Init(&amp;argc,&amp;argv);
	MPI_Comm_size(MPI_COMM_WORLD,&amp;numOfProcessors);  
	MPI_Comm_rank(MPI_COMM_WORLD,&amp;localRank);
	if(localRank!=0){
	   vec[0]=localRank*2;	
	}
    MPI_Gather(&amp;vec,1,MPI_INT,&amp;vec,1,MPI_INT,0,
								 MPI_COMM_WORLD);

	MPI_Finalize();

<p>
beginfigure[h]
    centering
    epsffilegather.eps
<p>
caption The effects of MPI_Gather
    labelfig:gatherC
endfigure
<br>Exercises
<br>:reductionEx
<br>
   <br> Write a program using MPI_Bcast and MPI_Reduce to calculate the
dot product of two vectors.  For simplicity, initialize and assign values to
the integer vectors of length 20 (Do this only the root process and 
assign the values as you wish). Assume the the the number of processors 
evenly divides 20.
   <br> Matrix-Vector multiplication. For this example assume that you will be using 4 processors.  On on the root process initialize a 4x4 matrix and 
a 4x1 vector.  Multiply the matrix by the 
vector using MPI_Scatter to distribute the matrix  and MPI_Bcast to distribute
the vector to the non-root processes.  
Then use MPI_Gather to collect the result.
   <br> Advanced Exercise - Write a program using newline MPI_AllReduce to 
calculate 
the integral of <IMG
 WIDTH="45" HEIGHT="15" ALIGN="MIDDLE" BORDER="0"
 SRC="img33.png"
 ALT="$ f(x)=x^2$">
 in the interval 0 to 12 using the Trapezoid rule. 
Assume  4 processors, you can use the intervals <!-- MATH
 $[a,b] = [p*3,(p+1)*3]$
 -->
<IMG
 WIDTH="70" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img34.png"
 ALT="$ [a,b] = [p*3,(p+1)*3]$">
. 
Can you extend this program to handle any interval with any number of processes?

Putting All Together: A Diffusion Problem in MPI
In this , we will apply what we have  learned to a two
dimensional diffusion problem for which  the prototype system is
shown in Figure reffig:proto. Here, the concentration of a solute in a liquid
is held constant at 1 at the boundaries of a square domain.
Initially, the concentration is  2 inside, except a small square
in the middle.
beginfigure[H]
centering
includegraphics figure1.eps
captionThe Prototype System
labelfig:proto
endfigure
As Fick's Law states, the solute will diffuse from high
concentration to  low concentration region. To determine the
concentration at any point and time, we need to solve the
following partial differential equation.Here, C stands for
concentration; t for time; x and y for space dimensions; and D for
diffusivity.
<p>
beginequation
 fracpartial Cpartial t  = Dbigg{fracpartial ^2 Cpartial x^2+fracpartial ^2 Cpartial y^2bigg}
endequation
linebreak
<p>
We will use the explicit finite difference method to solve
this equation. The formulation is as follows:
beginfigure[H]
centering
includegraphics[width=9cm] figure2.eps
captionDiscretization of the Domain
endfigure
<p>
Second order partial derivatives in the x and y
direction:
<p>
beginequation
fracpartial^2 Cpartial x^2  = frac1triangle x^2bigg{C^k_i+1,j-2C^k_i,j+C^k_i-1,j  bigg}
endequation
<p>
beginequation
fracpartial^2 Cpartial y^2  = frac1triangle y^2bigg{C^k_i,j+1-2C^k_i,j+C^k_i,j-1  bigg}
endequation
linebreak
<p>
Time derivative:
<p>
beginequation
 fracpartial Cpartial t  = frac1triangle tbigg{C^k+1_i,j-C^k_i,j  bigg}
endequation
linebreak
<p>
The partial differential equation in terms of finite difference
formulas
<p>
beginalign
labelequation:pde5
 frac1triangle tbig(C^k+1_i,j-C^k_i,jbig)  &amp;=D frac1triangle x^2bigg{C^k_i+1,j-2C^k_i,j+C^k_i-1,j bigg}notag 
<BR> &amp;+D frac1triangle y^2bigg{C^k_i,j+1-2C^k_i,j+C^k_i,j-1bigg}
endalign
linebreak linebreak linebreak
<p>
Rearranging Equation refequation:pde5
<p>
beginalign
C^k+1_i,j =C^k_i,j&amp;+D fractriangle ttriangle  x^2bigg{C^k_i+1,j-2C^k_i,j+C^k_i-1,jbigg}notag
<BR>&amp;+D fractriangle ttriangle
y^2bigg{C^k_i,j+1-2C^k_i,j+C^k_i,j-1bigg}
endalign
linebreak
<p>
The explicit finite difference method  requires small time steps in order to
overcome the stability problem which makes it   computationally
expensive. However, this method is simple and easy to parallelize. The
square domain is divided among two processes  in Figure reffig:fig3.
Each process is responsible for one rectangular grid and
the boundary elements are transferred to neighboring processes through message
passing. Note that, in columnwise distribution, the
boundary elements are contiguous in the memory and they can be
transferred without using derived data types.
beginfigure[H]
centering
includegraphics [width=6cm] figure3.eps
captionDividing the domain between two Processes 
labelfig:fig3
endfigure
<p>
Now let us examine the program to see how  to divide the domain
among the processes; express the initial and boundary conditions
in a parallel program; and also, generate output files.
<p>

#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;mpi.h&gt;
#include &lt;time.h&gt;
#include &lt;math.h&gt;

#define SIZE 500
#define SPIKE_SIZE 5
/* take the minimum of two integers */
int imin(int a,int b){
   if(a&lt;=b) return a; else return b;
}

main(int argc, char* argv[]){
   int my_rank,work1,work2;
   int nprocs,cnt,tag=0;
   int inext,iprev;
   int row,col,i,j;
   int N, M, ROOT,NT;
   int COUNT,I,J,JSTA,JSTA2,JEND,JEND1;
   int done;
   double FLD[SIZE][SIZE], WKSP[SIZE][SIZE];
   double DELTA,DELTAT,DELTAX;
   double DELTAY,DELTA1,DELTA2;
   double DF,L,T;
   char hostname[25],filename[50];
   int wallTime,starttime,endtime;
   time_t startSec,endSec;
   FILE * outf;
   MPI_Status status;
   MPI_Request send1,send2,recv1,recv2;
   
   /*Start MPI*/
   MPI_Init(&amp;argc, &amp;argv);
   
   /*Find out process rank*/
   MPI_Comm_rank(MPI_COMM_WORLD, &amp;my_rank);
   
   /*Find out the number of processes*/
   MPI_Comm_size(MPI_COMM_WORLD, &amp;nprocs);

   /*integer ISTATUS(MPI_STATUS_SIZE)*/

   /* Start the timer */
   if(my_rank==0) {
      startSec=time(NULL);
   }
   /* Initialize parameters */
   N=SIZE;
   M=SIZE;
   T=10.0;
   NT=100;
   DELTAT=T/NT;
   L=100.0;
   DELTAX=L/(N);
   DELTAY=L/(M);
   DELTA1=DELTAT/pow(DELTAX,2.0);
   DELTA2=DELTAT/pow(DELTAY,2.0);
   DF=(pow(10.0,-4.0))*25.0;
   done=0;

   MPI_Comm_size(MPI_COMM_WORLD,&amp;nprocs);
   MPI_Comm_rank(MPI_COMM_WORLD,&amp;my_rank);

   /*distribute the array onto the processes. */
   /*JSTA and JEND  stand for the first and */
   /*the last row mapped to each process. */
   
   work1=SIZE/nprocs;
   /*Determines the offset */
   work2=SIZE%nprocs;
   /*Distributes the extra columns to the */
   /*processes. If mod()is 2 for example, */
   /*an extra column will  be mapped to */
   /*process 0 and process 1*/
   JSTA=my_rank*work1+imin(my_rank,work2);
   if(my_rank&lt;work2) JEND=JSTA+work1;
   else JEND=JSTA+work1-1;   

   ROOT=(nprocs-1)/2;
   JSTA2=JSTA;
   JEND1=JEND;
   if (my_rank==0) JSTA2=1;
   if (my_rank==(nprocs-1)) JEND1=M-2;
   inext=my_rank+1;
   iprev=my_rank-1;
   if (my_rank==(nprocs-1))inext=MPI_PROC_NULL;
   if (my_rank==0) iprev=MPI_PROC_NULL;

   /*Initial Conditions */
   for(row=JSTA2;row&lt;=JEND1;row++)
     for(col=1;col&lt;SIZE-1;col++)
	  FLD[row][col]=2.0;
   /* for middle processor set spike value */
   if (my_rank==ROOT){ 
      for(i=0;i&lt;SPIKE_SIZE;i++)
        for(j=0;j&lt;SPIKE_SIZE;j++)
   	    FLD[(SIZE/2)-SPIKE_SIZE+i]
			   [SIZE/2-SPIKE_SIZE+j]=5.0;
   }
  /* Boundary Conditions*/
  for(row=JSTA;row&lt;=JEND;row++){
    FLD[row][0]=1.0;
    FLD[row][SIZE-1]=1.0;
  }
  if (my_rank==0) {
   for(col=0;col&lt;SIZE;col++)
      FLD[0][col]=1.0;
  }
  if(my_rank==(nprocs-1)){
      for(col=0;col&lt;SIZE;col++)
         FLD[JEND][col]=1.0;
  } 
  /*Main Processing Loop */
  /* set a counter to stop execution after some*/
  /* sane number of itterations*/
  cnt = 0;
  while((!done)&amp;&amp;(cnt&lt;2500)){  
   /* Transfer the boundary elements to  */
  /* neigboring processes */
   MPI_Isend(&amp;(FLD[JEND][0]),SIZE,MPI_DOUBLE,
      inext,tag,MPI_COMM_WORLD,&amp;send1);
   MPI_Isend(&amp;(FLD[JSTA][0]),SIZE,MPI_DOUBLE,
      iprev,tag,MPI_COMM_WORLD,&amp;send2);
   MPI_Irecv(&amp;(FLD[JSTA-1][0]),SIZE,MPI_DOUBLE,
      iprev,tag,MPI_COMM_WORLD,&amp;recv1);
   MPI_Irecv(&amp;(FLD[JEND+1][0]),SIZE,MPI_DOUBLE,
      inext,tag,MPI_COMM_WORLD,&amp;recv2);

   MPI_Wait(&amp;send1,&amp;status);
   MPI_Wait(&amp;send2,&amp;status);
   MPI_Wait(&amp;recv1,&amp;status);
   MPI_Wait(&amp;recv2,&amp;status);

   /*Updates the concentration using the finite*/  
   /*difference formula */
   for(row=JSTA2;row&lt;=JEND1;row++)
      for(col=1;col&lt;SIZE-1;col++)
         WKSP[row][col]=FLD[row][col]+
            DELTA1*DF*(FLD[row+1][col]
            -2.0*FLD[row][col]+FLD[row-1][col])
            +DELTA2*DF*(FLD[row][col+1]
            -2.0*FLD[row][col]+FLD[row][col-1]);

   for(row=JSTA2;row&lt;=JEND1;row++)
      for(col=1;col&lt;SIZE-1;col++)
         FLD[row][col]=WKSP[row][col];


   /*Check whether the concentration in the middle */ 
   /*of the small square is below 3.0. */
   /If so, change done to true */
   if (my_rank==ROOT){
      if(FLD[SIZE/2-SPIKE_SIZE/2]
         [SIZE/2-SPIKE_SIZE/2]&lt;=3.0){
         done=1;
      }
   }

   /*Collective Communication*/ 
   /*"done" is transferred from root*/
   /*to all processes*/
   MPI_Bcast(&amp;done,1,MPI_INT,ROOT,MPI_COMM_WORLD);
   cnt++;
  } 


   /*Opens a file for each process for I/O as*/ 
   /*"diffout.my_rank"*/

   sprintf(filename,"diffout.%d",my_rank); 
   if((outf=fopen(filename,"w+"))==NULL){
      printf("unable to open file %s for 
            process %d\n",filename,my_rank);
      exit(0);  
   }
   for(row=JSTA;row&lt;=JEND;row++){
      for(col=0;col&lt;SIZE;col++){
         fprintf(outf,"%7.4f ",FLD[row][col]);
      }
      fprintf(outf,"\n");
   }
   if(my_rank == 0){
      endSec=time(NULL);
      wallTime=(int) (endSec-startSec);
      fprintf(stderr, 
       "\n processing time %d seconds\n",wallTime);
   }

   MPI_Finalize();
}

<p>
The following figures show the initial and final states of the
system.
beginfigure[H]
includegraphics [width=13cm] result1.eps
captionSolute Concentration at t=0 
endfigure
beginfigure[H]
includegraphics [width=13cm] result2.eps
captionSolute Concentration after 366 iterations 
endfigure
<br>Exercises
<br>
<br> Modify the diffusion program for the following system.  Run
the code with 1,2,3 and 4 nodes and compare the execution times.
Try both non-blocking and blocking communications.
beginfigure[H]
centering
includegraphics [height=6cm]ex1.eps
captionThe Prototype System for Exercise1
endfigure
<br> Advanced Exercise. This time, there are two high
concentration regions as shown in the following Figure. Assume
that these regions are 10 grids away from the boundaries in the y
direction. The location of the squares in the x direction is the
same as before. Program should terminate when both of the
concentrations  go below 3.5. Again,run the program with different
numbers of nodes and compare the results.
beginfigure[H]
centering
includegraphics [height=6cm] ex2.eps
captionThe Prototype System for Exercise2 
endfigure

<p>
A Brief Introduction to PBS
label:pbs
PBS or it Portable Batch System is queuing software that controls the flow
of jobs on distributed systems.  PBS has the capability to have different queues
with different priorities.  One of the complications of running a parallel 
machine is that not all jobs require all of the resources.  This means that
system resources would be idle if the system could only run one job at a time.
On the other hand, if there were no job control none of the jobs would run 
efficiently.  PBS is flexible enough to handle many different configurations.
This  of the class is intended to give you an introduction to the basic
functions of PBS as we use them on our clusters. We will introduce the
several of the commands in the exercises at the end of this .
<br>Queues, qsub, and the submit scripts
Queues are at the heart of PBS.  There are several types and a long list of
possible settings for each type.  This can be used to sort jobs by various 
resources, and assign them different priorities.  On our cluster we run with 
only one queue called router. 
<p>
Jobs are submitted to the queuing system using the command it qsub. In an 
environment that has need of greater complexity, the command can be use to
control which queue a job enters, when the job is run, and controlling various
other aspects of a job.  
<p>
While much of the job control can be accomplished using qsub command line 
parameters, a preferable method is to use submission scripts.  These scripts
can set the environment and PBS variables to control the run. This is a 
preferable method since it is much simpler to rerun a job with small changes.
<p>
Below is a sample script to run the HelloWorld program on 4 nodes.

#!/bin/sh
#PBS -l nodes=4
mpirun -np 4 -machinefile $PBS_NODEFILE HelloWorld

If these lines were in a file called hello.pbs, then the program HelloWorld
could be submitted to the cluster with the command "qsub hello.pbs".
Although this is a simple file, there are a few points of interest.  The line
"#PBS -l nodes =4" is a PBS directive line.  Any line that starts with #PBS
is interpreted as a directive.  In this case it is telling PBS that it 
needs 4 nodes to execute the program.  Notice that this number matches the
parameter -np in the mpirun line.  The number allocated by PBS must be greater
than or equal to the number requested by the mpirun command.  In the
mpirun line we have a PBS environment variable.  Although this is a shell
script PBS makes certain changes to the environment.  In this case, the list of 
nodes that PBS is making available is stored in the $PBS_NODEFILE variable
and passed to mpirun. 
<p>
If the above program were submitted with the qsub program above we would see
the following sequence of events.

&gt;qsub hello.pbs
&gt;100.mimir

This would be telling you that your job was assigned the number 100. When
the job was finished you would have two new files in this directory with
the names "hello.pbs.e100" and "hello.pbs.o100". The .e file contains the
output from standard error, and any errors that PBS encounters.  The .o file
contains the output from the submit script and the program.
<p>
The list below show some of the common modifications to the submit script
that you might want to use.
<p>
begindescription
<br>[#PBS -joe] Combine the error and output files.
<br>[#PBS -N ] Set the name of the job. This affects the name of the output
files.
<br>[#PBS -l] Sets the resource lists for the jobs -l can be followed by
a resource list which often includes the following <br>s.
   begin<br>ize
	  <br> nodes - Specifies which may be followed by a formatted string specifying which nodes to use.
	  <br> ncpus - Sets how many cpus are needed. This is more important with
SMP nodes, or in a pure SMP environment.
	  <br> walltime - Sets an upper limit on the length of the job.
	  <br> cputime - Sets an upper limit on the cpu time used for the job.
   end<br>ize
<br> [cd] Change the directory.
enddescription
<p>
There are many other options available and the complete Users Guide is available
on line from ftp.acomp.usf.edu.
<br>Exercises
<br>
<br> Create a submission script to run the diffusion program from the last 
  on eight nodes and submit it by logging on to mimir and using qsub.
<br> While everyone is running their programs on mimir, log onto mimir and
investigate the commands qstat, pbs_stats, xpbsmon and xpbs. You may have to
rerun jobs if they all finish too fast.

<p>
Further Reading
label:addmat 
begin<br>ize
<br> Books
begin<br>ize
   <br> it MPI-The Complete Reference by Snir,Otto,Huss-Lederman,Walker,andDongarra
   <br> it Parallel Programming with MPI by Peter S. Pacheco
end<br>ize
<br> On-line Tutorials
   begin<br>ize
   <br> OSC - http://oscinfo.osc.edu/training (requires free registration)
   <br> USF - http://rocs.acomp.usf.edu/tut/mpi.php
   end<br>ize
end<br>ize
<p>


 
<p>Introduction to MPI Programming in Fortran
<p>
it MPI or it Message Passing Interface indexMPI is not a
programming language.  It is a standard for implementing message
passing parallel programming. It is in most implementations a
library of functions or subprograms that can be called from other
languages, most often C or Fortran. MPI grew out from the
frustrations of programmers and researchers whose parallel code
was not portable due to proprietary programming interfaces. It was
developed by a forum with members from academia, government, and
industry.  MPI allows development of portable and efficient
parallel code. These libraries make many of the lower level
intricacies of parallel programming transparent to the programmer.
<p>
MPI is a form of message passing parallel programming, which is a
very popular parallel programming model. In this paradigm, the
processes run independently, and use communications for
synchronizing their efforts. For those of you with an interest in
the theory behind parallel programming, MPI is a form of SIMD or
SPMD parallel architecture.  This means that all of the processors
run the same program, but on different data.  Unlike a pure SIMD
architecture MPI does not force the program to run exactly the
same on all processors, but allows for differential execution.
<p>
These notes are not intended to be an in depth
exposition of even the introductory topics that are covered.
Rather they are intended to lead you through the examples which
should give you adequate exposure to begin using MPI. For more in
depth materials please see  ref:addmat which
lists additional pedagogical materials.
<p>
There are several commercial and free versions of these libraries.
The two most popular non-commercial implementations are it
LAM-MPI and it MPICH.  We will be using MPICH for this class.
<p>
 A first program
label:afp We will start with the traditional "Hello
World" program.  This will allow us to take a detailed look at the
minimal code necessary to run an MPI program. The code for this
and the other examples in this <p> are included in the bccode
directory on the lab machines.

    PROGRAM HELLOWORLD

    INCLUDE 'mpif.h'

    integer my_rank
    integer NPROCS
    integer source
    integer dest
    integer tag
    character*100 message
    character*10  rank
    integer status(MPI_STATUS_SIZE)
    integer ierr

c   Start MPI
    CALL MPI_Init(ierr)

c   Find out process rank
    CALL MPI_Comm_rank(MPI_COMM_WORLD, my_rank,ierr)

c   Find out the number of processes
    CALL MPI_Comm_size(MPI_COMM_WORLD, NPROCS,ierr)

    if (my_rank.ne.0) then

c   Create Message
    write(rank,100) my_rank
100 format(I1)
    message = 'Hello from process ' //  rank
  &amp;             // '!'
    dest = 0
    tag = 0
    CALL MPI_SEND(message, 100, MPI_CHARACTER,
  &amp;             dest, tag, MPI_COMM_WORLD, ierr)
    else
    do 200 source = 1, NPROCS-1
        tag = 0
    CALL MPI_RECV(message, 100, MPI_CHARACTER,
  &amp;     source,tag, MPI_COMM_WORLD, status, ierr)
    write(*,*) message
200 continue
    endif

c   Shutdown MPI
    CALL MPI_FINALIZE(IERR)
    end

In Fortran, MPI programs require the include file mpif.h. Status is
an array of integers of length MPI_STATUS_SIZE which is defined
in the file mpif.h. It is used to return the status of the MPI
communication subroutines. Information on MPI_SOURCE, MPI_TAG,
and MPI_ERROR is obtainable from Status. For now, just think of
the status variable as the place where MPI puts the error codes
for its subroutines.
<p>
Now we will start looking at the MPI subroutines used in this
program. The first is MPI_INIT which has the following
definition:

    CALL MPI_INIT(ierr)
    integer ierr

 MPI_INIT must be the first MPI  call in your program, as it initializes the state of the
program for all other MPI calls. Although the call to MPI_INIT
allows the passing of the command line parameters, their use is
not defined in the MPI standard. This means that using them is not
necessarily portable, and that they should be used with caution.
Their use is beyond the scope of this class. Also note that for
most installations, you can get information on the MPI functions
by using the man command.
<p>
With the next call we begin to delve into some of the mechanisms
of MPI programming.
<p>

c   Find out process rank
    CALL MPI_COMM_RANK(MPI_COMM_WORLD,my_rank,ierr)

c   Find out the number of processes
    CALL MPI_COMM_SIZE(MPI_COMM_WORLD,NPROCS,ierr)

<p>
Both MPI_COMM_RANK and MPI_COMM_SIZE have as their first
parameter the constant MPI_COMM_WORLD, which is of type
MPI_COMM. An MPI_COMM is the data type used to reference a it
communicator. A communicator indexcommunicator is a collection
of processes. As your programs and the underlying tasks become
more complicated, you may need to set up different groups of
processors to accomplish different tasks and may need to create
your own communicators. MPI by default sets up the global
communicator MPI_COMM_WORLD.  The MPI_COMM_RANK command
returns the process number or "rank" within the communicator.
MPI_COMM_SIZE puts the size of the given communicator into the
location pointed to by the second parameter.  The rank of the
process and the communicator's size are often used to control the
local flow of the program. We can see this in the next code
segment.
<p>

    if (my_rank.ne.0) then
c   Create Message
    write(rank,100) my_rank
100     format(I1)
    message = 'Hello from process ' //  rank
  &amp;             // '!'
    dest = 0
    tag = 0
    CALL MPI_SEND(message, 100, MPI_CHARACTER,
  &amp;             dest, tag, MPI_COMM_WORLD, ierr)
    else
    do 200 source = 1, NPROCS-1
        tag = 0
    CALL MPI_RECV(message, 100, MPI_CHARACTER,
  &amp;     source,tag, MPI_COMM_WORLD, status, ierr)
    write(*,*) message

<p>
In this snippet we see two very common MPI constructions.  First
the it if .. else construction where one (or more) process(es)
are executing one set of instructions, and another set of
processes are executing another. In this case we see that each
process other than the root process (rank = 0) is sending a
message.  Meanwhile, the root process is receiving messages in a
DO loop which runs from 1 to the size of the communicator. Note
that there is nothing in this code that forces an order on the
communications from the non-root processes.  Rather they will each
run the code asynchronously.  One might ask how is it that the
root process is able to receive the messages in the correct order?
To answer that question, we must take a more in depth look at the
it MPI_RECV and it MPI_SEND commands. This is the subject
of the next , but we will take a brief look at these
commands here, paying particular attention to the parameter types
and what they mean.
<p>
The MPI_Send command has the definition:

    call MPI_SEND (buf,icount,MPI_Datatype,idest,
  &amp;                  itag,MPI_COMM,ierr)
    integer icount,idest,itag,ierr

<p>
begindescription
   <br> [buf] This is initial address of the send buffer, in our case it is
the beginning of the string message.
   <br> [icount] number of elements in send buffer. The count is not the number of bytes
but number of <br>s.  This allows more complex data types to be
handled easily, and maintains portability.
   <br> [datatype] Datatype of each send buffer element.  Note that this is
not a standard Fortran datatype but one of the data types defined
by MPI. Although MPI has many datatypes and functions for
manipulating them, our examples will draw from the basic types
given in the following list.
   <br>
      <br> MPI_CHARACTER
      <br> MPI_REAL
      <br> MPI_INTEGER
      <br> MPI_DOUBLE_PRECISION
      <br> MPI_LOGICAL
      <br> MPI_REAL8
      <br> MPI_REAL4
      <br> MPI_INTEGER4
      <br> MPI_BYTE
      <br> MPI_PACKED
      <br> MPI_COMPLEX
      <br> MPI_DOUBLE_COMPLEX
   
   <br> [idest] Rank of the destination
   <br> [itag] The use of the message tag is left to the program. It may be
used to classify messages
   <br> [comm] Communicator
   <br> [ierr] MPI error number (0=no error)
enddescription
<p>
The MPI_RECV has the  definition:

    call MPI_RECV (buf,icount,MPI_Datatype,isource,
 &amp;                   itag,MPI_COMM,istatus,ierr)
    integer  icount,isource,itag,ierr

Let's take a look at the differences between MPI_SEND and
MPI_RECV. The it buf in the MPI_RECV call is the initial
address of the receive buffer instead of the send buffer.  We have
replaced idest or the destination of the message with the source
of the message. MPI_RECV also returns a Status variable as
described above.
<p>
With just a little more information we can now answer the
question, how does the root process receive the messages in order.
MPI buffers messages. Although MPI does not guarantee that the
messages from different processes arrive in any specific order,
the root process reads them  from the buffer in processor number
order. MPI does however ensure that messages from the same
processor do arrive in order.  We will explore other types of
point-to-point communications and their details in the following
.
<p>
<br>Compiling and running MPI programs
To compile MPI programs you must make sure that the
MPI_HOME/include directory is in your include path, and that the
MPI_HOME/lib directory is in your library path.  On the lab
machines this should be the default configuration. If these
requirements are met the program above can be compiled with the
command:

   g77 -L/usr/local/mpich/lib hello.o 
		 -ffree-form -o hello  -lmpich

On most installations of MPI there is some script to make sure
that you have include the correct libraries an paths.  For our
machines the command is it mpif77.  So you can replace the above
command with:

   mpif77 -ffree-form -o HelloWorld HelloWorld.f

<p>
Once the program is compiled you can use the it mpirun command
to run the program.  The following example should work on your lab
machine:

mpirun -np 4 -machinefile /usr/local/nodes HelloWorld

<p>
<br>A note on output
Getting output from an MPI program is not as straightforward as it
is in serial programming.  This is a topic of much discussion, and
research.  In general if you put a print statement into an MPI
program, it will not necessarily print in some convenient fashion.
Remember that each node operates independently. Suppose that an
MPI program has the following code
<p>

    write(*,*) 'my rank is',myrank
 ... processing code ...
    write(*,*) 'my value is',REAL(myrank)

The output from this bit of code could be in any order.

my rank is 0
my rank is 1
my rank is 2
my value is 0.0
my value is 1.0
my value is 2.0

or

my rank is 0
my value is 0.0
my rank is 2
my rank is 1
my value is 1.0
my value is 2.0

The only thing that you can be sure of is that the program will
run sequentially on each processor.  There are advanced methods of
I/O for MPI, and there are some parallel I/O calls that you may
want to explore as you become more familiar with MPI.  For now you
should do one of two things.  If you want to have some nice tidy
output send messages to the root process and print from there, or
make sure that each line of output includes the rank of the
process so that you can be sure of which process is doing the
output.
<p>
<br>Exercises
label:basicMPIex
<br>
   <br> Hello Neighbor - Modify the Hello World Program so that each processor
sends a message to the processor of the next highest rank, and
receives a message from the next lower rank.  The root process 0
should not receive a message, and the processor with the largest
rank should not send a message. After all of the messages are sent
and received all of the non root processes should send the root
process a message with their received message.  The output should
look something like:

process 1 received message Hello from process 0
process 2 received message Hello from process 1
   ...

   <br> Advanced Exercise - On each non-root processor define a one
dimensional array of integers and fill it with the product of the
processor number and the <br> of the location in the array.  Send
this vector to root. On the root processor receive the vectors
into rows of a matrix.  For simplicity, assume that there are four
processors and each vector has length 3.  Then print out the
matrix from the root process. The output should look like:

   0 1 2
   0 2 4
   0 3 6


<p>
 Point-to-Point Communications
In this , we will examine point-to-point message passing
which allows data transfer from one process to another.  Unlike
collective communications, point-to-point message passing involves
only a pair of processes.
<p>
Sending a scalar variable or an array from one process requires
calling an mpi send subroutine. This initiates data transfer from
the user buffer to the system buffer. The corresponding receive
call, on the other hand, allows data to be copied from the system
buffer to the user buffer in the destination process. There is a
variety of mpi send subroutines allowing communication in
different modes  - synchronous, buffered, ready, standard blocking
and non-blocking.  Unlike send, there are only two types of
receive subroutines: standard blocking and nonblocking. Here, we
will focus on blocking and nonblocking communications, as they are
the most commonly used modes.
<p>
When we use the blocking send subroutine, MPI_SEND, control does
not return to the program until the data transfer to the system
buffer is complete. Similarly, control returns from the blocking
receive subroutine, MPI_RECV only after the data is copied to the
user buffer. On the other hand, nonblocking communication
subroutines, MPI_ISEND and MPI_IRECV, indicate that the data
transfer has only begun. Control immediately returns to the
program while data transfer continues in the background. Note that
incorrect communications occur if we change the buffer content
before data transfer is complete. Therefore, somewhere in the
program, we have to make sure that the communication is over by
calling  MPI_WAIT. The format for  MPI_WAIT is as follows:
<p>

    CALL MPI_WAIT(IREQUEST,ISTATUS,IERR)
    Irequest    Request Identifier
    Istatus     Status Object
    Ierr        Fortran Return code

<p>
Now, let us take a look at the following example program where a
different scalar variable is initialized on each process.  The
processes communicate and calculate the sum of these variables.
<p>

        PROGRAM COMMUNICATION
        INCLUDE  'mpif.h'
        DOUBLE PRECISION a,b,c
c       Start MPI
        CALL MPI_INIT(IERR)
c       Number of processes
        CALL MPI_COMM_SIZE(MPI_COMM_WORLD,
     &amp;                       NPROCS,IERR)
c       Process rank
        CALL MPI_COMM_RANK(MPI_COMM_WORLD,
     &amp;                        MYRANK,IERR)
c       Define next and previous processes
        INEXT=MYRANK+1
        IPREV=MYRANK-1
c        Define Null processes
        IF (MYRANK.EQ.(NPROCS-1))
     &amp;       INEXT=MPI_PROC_NULL
        IF(MYRANK.EQ.0)IPREV=MPI_PROC_NULL

c       Initialize a in process 0 and b in process 1
        if (myrank.eq.0) a=1.50d0
        if (myrank.eq.1) b=2.50d0

c       Data exchange
         CALL MPI_ISEND(a,1,MPI_DOUBLE_PRECISION,
     &amp;  INEXT,1,MPI_COMM_WORLD,ISEND1,IERR)

        CALL MPI_ISEND(b,1,MPI_DOUBLE_PRECISION,
     &amp;  IPREV,1,MPI_COMM_WORLD,ISEND2,IERR)

        CALL MPI_IRECV(a,1,MPI_DOUBLE_PRECISION,
     &amp;  IPREV,1,MPI_COMM_WORLD,IRECV1,IERR)

        CALL MPI_IRECV(b,1,MPI_DOUBLE_PRECISION,
     &amp;  INEXT,1,MPI_COMM_WORLD,IRECV2,IERR)

c       Calculate c  and d before MPI_WAIT
        d=(5.0d0)**(2.d0)+5.0d0
        c=a+b+d
        write(*,*) c,d,myrank

        CALL MPI_WAIT(ISEND1,ISTATUS,IERR)
        CALL MPI_WAIT(ISEND2,ISTATUS,IERR)
        CALL MPI_WAIT (IRECV1,ISTATUS,IERR)
        CALL MPI_WAIT (IRECV2,ISTATUS,IERR)

c       Calculate c after MPI_WAIT
        c=a+b+d

        write(*,*) c,d, '**',myrank

c       Shutdown MPI
        CALL MPI_FINALIZE(IERR)

        END

<p>
This program generates a wrong value for c when it is calculated
before the MPI_WAIT call.  MPI_WAIT is used to block both
sending and receiving processes until the communication is
complete. Since the variable d can be calculated correctly during
data exchange, it is safe to post MPI_WAIT calls afterwards. On
the other hand, placing MPI_WAIT just after the immediate Send
and Receive calls would be the same as using blocking
communications. Please note that, to reduce synchronization
overhead, we should post MPI_WAIT in the program as late as
possible.
<p>
We use immediate calls because they are faster than blocking
communications. Also, blocking calls may fail when the sent
message is too large.
<p>
<br>Transferring Data from Arrays
We specify the location of the first element to be sent or
received when we call MPI communication subroutines. For a
successful communication, rest of the elements to be transferred
must be contiguous in the memory. Since   Fortran stores the
elements of a two-dimensional array in column major order, sending
a column of an array is straightforward.  Let us examine the
following program that computes the elapsed time for the data
exchange between two processes.
<p>

        PROGRAM COMMUNICATION2
        INCLUDE 'mpif.h'
        DOUBLE PRECISION A,B,T1,T2,RESON
        DIMENSION A(2500,2500),B(2500,2500)

        CALL MPI_INIT(IERR)

        CALL MPI_COMM_SIZE(MPI_COMM_WORLD,
     &amp;                       NPROCS,IERR)

        CALL MPI_COMM_RANK(MPI_COMM_WORLD,
     &amp;                       MYRANK,IERR)

        RESON=MPI_Wtick()
        write(*,*) reson, 'clock resolution'

        IF (MYRANK.EQ.0) THEN
        DO I=1,2500
        DO j=1,2500
        A(I,J)=1.0
        ENDDO
        ENDDO
        ENDIF

        IF (MYRANK.EQ.1) THEN
        DO I=1,2500
        DO J=1,2500
        B(I,J)=2.0
        ENDDO
        ENDDO
        ENDIF

        CALL MPI_BARRIER(MPI_COMM_WORLD,ierr)
        IF (MYRANK.EQ.0) THEN
        T1=MPI_WTIME()

        CALL MPI_SEND(A(1,1),5000,
   &amp;    MPI_DOUBLE_PRECISION, 1,1,
   &amp;    MPI_COMM_WORLD,IERR)
        CALL MPI_RECV(B(1,1),5000,
   &amp;    MPI_DOUBLE_PRECISION,1,1,
   &amp;     MPI_COMM_WORLD,IRECV1,IERR)

        T2=MPI_WTIME()
        write(*,*) t2-t1,myrank,'t2'
        ENDIF

        IF (MYRANK.EQ.1) THEN
        T1=MPI_WTIME()

        CALL MPI_SEND(B(1,1),5000,
  &amp;     MPI_DOUBLE_PRECISION, 0,1,
  &amp;     MPI_COMM_WORLD,IERR)
        CALL MPI_RECV(A(1,1),5000,
  &amp;     MPI_DOUBLE_PRECISION,0,1,
  &amp;     MPI_COMM_WORLD,IRECV2,IERR)
        T2=MPI_WTIME()
        write(*,*) t2-t1,myrank
        ENDIF
        CALL MPI_FINALIZE(IERR)
        END

First, let us explain briefly some of the subroutines we have used
for the first time. MPI_WTICK is used to determine the clock
resolution and MPI_WTIME gives the elapsed time.
<p>
The MPI_BARRIER subroutine blocks each process until each one has
called it. It is used for synchronization. In other words,  it
ensures that the processes start sending and receiving at the same
time. MPI_BARRIER can be used for blocking multiple processes; it
is a collective communication subroutine.  We will see more about
collective communications in the next .
<p>
In this program, each process transfers the first and second
columns of the array initialized on them. If we increased the
number of columns to be transferred more and more, MPI_SEND and
MPI_RECEIVE would eventually fail. In this case, non-blocking
calls would still be working.
sub<br>Derived Data Types
Now, let us see how to transfer rows in an array.  A new data type
must be defined in order to send and receive rows because this
time the data is not contiguous in the memory. The following
Send/Receive pair can be used to transfer the first row of the
following array.

            1 2 3 4
            1 2 3 4
            1 2 3 4
            1 2 3 4

<p>

        CALL MPI_TYPE_VECTOR(4,1,4,
   &amp;    MPI_DOUBLE_PRECISION,rowtype,ierr)
        CALL MPI_TYPE_COMMIT(ROWTYPE,IERR)

        IF (MYRANK.EQ.0) THEN
         CALL MPI_SEND(A(1,1),1,rowtype,
   &amp;     1,1,MPI_COMM_WORLD,ISEND2,IERR)
        ENDIF

        IF (MYRANK.EQ.1) THEN
        CALL MPI_RECV(A(1,1),1,rowtype,
   &amp;    0,1,MPI_COMM_WORLD,IRECV2,IERR)

        CALL MPI_TYPE_FREE(rowtype,ierr)

<p>
The MPI_TYPE_VECTOR subroutine is used to define a new data type
representing equally spaced blocks.  It has the following format:
<p>

    CALL MPI_TYPE_VECTOR(COUNT, BLOCKLENGTH,
  &amp; STRIDE, OLDTYPE,NEWTYPE,IERROR)

COUNT           Number of blocks
BLOCKLENGTH     Number of elements in each block
STRIDE          Number of elements between
                two succesive blocks
OLDTYPE         Old data type
NEWTYPE         New data type
IERR            Fortran return code

<p>
The MPI_TYPE_COMMIT makes the new data type ready to be used in
communication. The new data type is freed by the MPI_TYPE_FREE
call. In other words, the MPI_TYPE_FREE routine sets the new
data type to MPI_DATATYPE_NULL.
<p>
<br>Exercises
<br>
<br> On each process, define a 4X4  array.  The first process
will have 1s in the first column, the second process will have 2s
in the second column and so on. The rest of the elements of the
array will be zero. Using blocking communication subroutines,
complete the array on each process. Assume that the number of
processes is 4. The output should look like as follows.
<p>

            1 2 3 4
            1 2 3 4
            1 2 3 4
            1 2 3 4

Rewrite the program using non-blocking communications.
<p>
<br> Advanced Exercise: Modify the array of the previous exercise
such that only one row should contain nonzero elements initially.
Complete the array on each process using derived data types and
point to point communications.

<p>
Collective Communications
label:collective_communications Until this point we've
only discussed communications coming from a single process and
going to a single process.  MPI also contains it collective
communications indexcollective communications which loosely
defined are those communications that involve multiple processors
on the sending end, the receiving end or both. The MPI_Barrier
command that was described in the last , is often included
as a collective communication. In the following s we will
describe some of the most commonly used communications of this
type.  The reader should be aware that this is a rich, complex and
useful area within MPI, and these notes barely scratch the
surface.
<p>
<br>One to All
<br>:bcast To send messages from one process to all
other processes MPI contains the function MPI_BCAST.
indexMPI_BCAST The format for MPI_BCAST is
<p>

    CALL MPI_BCAST(buffer,icount,MPI_DATATYPE,
  &amp; iroot,MPI_COMM,ierr)
    integer icount,iroot,ierr

<p>
There are several differences between MPI_BCAST and the
point-to-point communications that we have looked at in the
previous s.  First both the sender and the receivers use
the same function call. Root in the case of MPI_BCAST is the rank
of the process sending and it must be the same on all processes or
there will be an error.
<p>
It is important to understand that although you can accomplish the
same data transfer by using individual sends and receives, using
MPI_BCAST is more efficient.  This is generally true of the
collective communications.  Let's take a brief look at why.  When
you do individual sends and receives, at each iteration of the
loop there is one message being sent from the sending process to
each of the receiving processes.  There are other possible methods
of getting the data to each process that are more efficient. Let's
assume for the sake of simplicity that the root process in our
communication is process 0, and we have 8 nodes. In this case we
could send the communications in the manner depicted in Figure
reffig:bcast.
<p>
beginfigure[H]
    centering
    epsffilebcast.eps
<p>
caption A more efficient communication scheme
    labelfig:bcast
endfigure
<p>
Notice that this completes the communication in three time steps
instead of the eight needed by sending all messages from root.
This is not to say that TCP/IP communications are this simple, or
that there are not more efficient schemes. The efficiency of such
a scheme is dependent on the underlying network topology.
Fortunately, we don't have to come up with an efficient scheme
each time we want to distribute data.  The implementations of
MPI_BCAST handle the details for us.  This is especially
important when one considers portability issues.  Without using
the MPI built in routines, you would have to determine the best
scheme each time you wanted to run the program in a different
environment, or even a different number of processors.
<p>
<br>All to one
<br>:reduce So you are probably thinking that if MPI
has an efficient method of broadcasting from one process to all of
the others that there should be some way of collecting data from
all of the nodes into one.  And there are several. The first ones
that we will look at involve reduction operations.  For example
suppose that you have a value on each processor and that you
wanted to get a global sum of these values.  If we send all of the
data to the root node by using individual calls we have created
several bottlenecks in the program. The communications must be
received sequentially at the root node.  Also, once the data is
collected the other processors must wait until the root processor
is finished with the calculation before they can proceed. (If they
need the result.)  The function MPI_Reduce takes care of the
first of these problem.
<p>

    CALL MPI_REDUCE(sendbuf,recvbuf,icount,
  &amp; MPI_Datatype,MPI_Op,iroot,MPI_Comm,ierr)
    integer icount,iroot,ierr

<p>
Lets take a look at an example. Assume that all of the variables
are defined appropriately.

    CALL MPI_REDUCE(val,total,1,MPI_INTEGER,
  &amp; MPI_SUM,0,MPI_COMM_WORLD,ierr)

<p>
Each process calls MPI_Reduce with the same parameters.  Even
though the sum is accumulated only on process 0. After the call
above, the variable total will contain the global sum.  What if
you want to do some other operation than summation? MPI predefines
several operations for the parameter MPI_Op. It can take on the
following values.
begindescription
    <br> [MPI_MAX] maximum
    <br> [MPI_MIN] minimum
    <br> [MPI_SUM] sum
    <br> [MPI_PROD] product
    <br> [MPI_LAND] logical and
    <br> [MPI_BAND] bit-wise and
    <br> [MPI_LOR] logical or
    <br> [MPI_BOR] bit-wise or
    <br> [MPI_LXOR] logical xor
    <br> [MPI_BXOR] bit-wise xor
    <br> [MPI_MAXLOC] max value and location of maximum
    <br> [MPI_MINLOC] min value and location of minimum
enddescription
<p>
The operators MPI_MAXLOC and MPI_MINLOC have predefined MPI
datatypes to be able to return the value and the location.
<p>
While MPI_Reduce solves the problem of efficiently communicating
the data to the root node it leaves open the question of how to
distribute the result if it is needed on all of the processors. We
could follow the call to MPI_Reduce with a call to MPI_Bcast,
but this isn't the most efficient method. Instead MPI uses a
communication structure called the it butterfly which
communicates the results of partial operations in such a way that
at the end of the call all of the processors have the result.  The
details of this communication scheme are beyond the scope of these
notes, but it should suffice at this stage to just assume that for
doing one of the global reduction operations and having the result
be available to all of the processors, MPI_Allreduce is the
function of choice. The definition of MPI_Allreduce is given
below.
<p>

    call MPI_ALLREDUCE(sendbuf,recvbuf,icount,
  &amp;     MPI_Datatype,MPI_Op,MPI_Comm,ierr)
    integer icount,ierr

<p>
<br>Scatter and Gather
<br>:scattergather We have looked at collective
communications that take the same data and communicate it to all
processors, and those that reduce data from all processes to one.
But these communication functions will not allow us to send different
information to each process.  Nor will they allow us to collect
separate bits of information from all of the processes onto one.
For these tasks we can use MPI_SCATTER and MPI_GATHER.
<p>

    CALL MPI_SCATTER(SNDBUF,ISCOUNT,MPI_DATATYPE,
  &amp; RECVBUF,IRCOUNT,MPI_DATATYPE,IROOT,MPI_COM,
  &amp;                  IERR)
    integer  iscount,ircount,iroot,ierr

    CALL MPI_GATHER(SNDBUF,ISCOUNT,MPI_DATATYPE,
  &amp; RECVBUF,IRCOUNT,MPI_DATATYPE,IROOT,MPI_COM,
  &amp;                 IERR)
    integer  iscount,ircount,iroot,ierr

<p>
The meanings of these parameters are similar to previous commands
that we have studied.  The iscount is usually the same as the
ircount, and the send datatype is usually the same as the receive
datatype. We can look at the following example to see how
MPI_Scatter works.
<p>

        PROGRAM SCATTER
        INCLUDE 'mpif.h'

        real vec
        dimension vec(4)
        CALL MPI_INIT(ierr)
        CALL MPI_COMM_SIZE(MPI_COMM_WORLD,NPROCS,
     &amp;                      IERR)

        CALL MPI_COMM_RANK(MPI_COMM_WORLD,MYRANK,
     &amp;                       IERR)

        if (myrank.eq.0) then
        do i=0,3
        vec(i+1)=i*2.0
        enddo
        endif
        CALL MPI_SCATTER(vec,1,MPI_INTEGER,vec,1,
     &amp;    MPI_INTEGER,0,MPI_COMM_WORLD,IERR)

        write(*,*) vec(1),myrank
        CALL    MPI_FINALIZE(ierr)
        end

<p>
This code will distribute the each of the entries of vector vec to
the processes in MPI_COMM_WORLD.  Figure reffig:scat shows
the effects of the MPI_Scatter command on the memory for each
process.
<p>
beginfigure[H]
    centering
    epsffilescat.eps
<p>
caption The effects of MPI_Scatter
    labelfig:scat
endfigure
MPI_Gather is in some sense the inverse operation to
MPI_Scatter.  Instead of distributing values, it gathers them
together.  The following code and Figure reffig:gather show the
effects of the MPI_Gather operation.
<p>

        PROGRAM GATHER
        INCLUDE 'mpif.h'

        real vec
        dimension vec(4)
        CALL MPI_INIT(ierr)
        CALL MPI_COMM_SIZE(MPI_COMM_WORLD,
    &amp;                        NPROCS,IERR)

        CALL MPI_COMM_RANK(MPI_COMM_WORLD,
    &amp;                        MYRANK,IERR)

        if (myrank.ne.0) vec(1)= myrank*2.0

        CALL MPI_GATHER(vec,1,MPI_INTEGER,vec,1,
     &amp;    MPI_INTEGER,0,MPI_COMM_WORLD,IERR)

        if (myrank.eq.0) then
        do i=0,3
        write(*,*) vec(i+1)
        enddo
        endif
        CALL  MPI_FINALIZE(ierr)
        end

<p>
beginfigure[H]
    centering
    epsffilegather.eps
<p>
caption The effects of MPI_Gather
    labelfig:gather
endfigure
<br>Exercises
<br>:reductionEx
<br>
   <br> Write a program using MPI_Bcast and MPI_Reduce to calculate the
dot product of two vectors.  For simplicity, initialize and assign
values to the integer vectors of length 20 (Do this only the root
process and assign the values as you wish). Assume the the the
number of processors evenly divides 20.
   <br> Matrix-Vector multiplication. For this example assume that you will be using 4 processors.  On on the root process initialize a 4x4 matrix and 
a 4x1 vector.  Multiply the matrix by the 
vector using MPI_Scatter to distribute the matrix  and MPI_Bcast to distribute
the vector to the non-root processes.  
Then use MPI_Gather to collect the result.
   <br> Advanced Exercise - Write a program using newline MPI_AllReduce to
calculate the integral of <IMG
 WIDTH="45" HEIGHT="15" ALIGN="MIDDLE" BORDER="0"
 SRC="img33.png"
 ALT="$ f(x)=x^2$">
 in the interval 0 to 12 using
the Trapezoid rule. Assume  4 processors, you can use the
intervals <!-- MATH
 $[a,b] = [p*3,(p+1)*3]$
 -->
<IMG
 WIDTH="70" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img34.png"
 ALT="$ [a,b] = [p*3,(p+1)*3]$">
. Can you extend this program to
handle any interval with any number of processes?

Putting All Together: A Diffusion Problem in MPI
In this , we will apply what we have  learned to a two
dimensional diffusion problem for which  the prototype system is
shown in Figure reffig:figure1.
Here, the concentration of a
solute in a liquid is held constant at 1 at the boundaries of a
square domain. Initially, the concentration is  2 inside, except a
small square in the middle.
beginfigure[H]
centering
includegraphics figure1.eps
captionThe Prototype System
labelfig:figure1
endfigure
As Fick's Law states, the solute will diffuse from high
concentration to  low concentration region. To determine the
concentration at any point and time, we need to solve the
following partial differential equation.Here, C stands for
concentration; t for time; x and y for space dimensions; and D for
diffusivity.
<p>
beginequation
 fracpartial Cpartial t  = Dbigg{fracpartial ^2 Cpartial x^2+fracpartial ^2 Cpartial y^2bigg}
endequation
linebreak
<p>
We will use the explicit finite difference method to solve
this equation. The formulation is as follows:
beginfigure[H]
centering
includegraphics[width=9cm] figure2.eps
captionDiscretization of the Domain
endfigure
<p>
Second order partial derivatives in the x and y
direction:
<p>
beginequation
fracpartial^2 Cpartial x^2  = frac1triangle x^2bigg{C^k_i+1,j-2C^k_i,j+C^k_i-1,j  bigg}
endequation
<p>
beginequation
fracpartial^2 Cpartial y^2  = frac1triangle y^2bigg{C^k_i,j+1-2C^k_i,j+C^k_i,j-1  bigg}
endequation
linebreak
<p>
Time derivative:
<p>
beginequation
 fracpartial Cpartial t  = frac1triangle tbigg{C^k+1_i,j-C^k_i,j  bigg}
endequation
linebreak
<p>
The partial differential equation in terms of finite difference
formulas:
<p>
beginalign
 frac1triangle tbig(C^k+1_i,j-C^k_i,jbig)  &amp;=D frac1triangle x^2bigg{C^k_i+1,j-2C^k_i,j+C^k_i-1,j bigg}notag 
<BR> &amp;+D frac1triangle y^2bigg{C^k_i,j+1-2C^k_i,j+C^k_i,j-1bigg}
endalign
<p>
Rearranging Eq.5.5,
<p>
beginalign
C^k+1_i,j =C^k_i,j&amp;+D fractriangle ttriangle  x^2bigg{C^k_i+1,j-2C^k_i,j+C^k_i-1,jbigg}notag
<BR>&amp;+D fractriangle ttriangle
y^2bigg{C^k_i,j+1-2C^k_i,j+C^k_i,j-1bigg}
endalign
linebreak
<p>
The explicit finite difference method  requires small time steps in order to
 overcome the stability problem which makes it   computationally
 expensive . However, this method is simple and easy to parallelize. The
 square domain is divided among two processes  in Figure reffig:figure3.
  Each process is responsible for one rectangular grid and
 the boundary elements are transferred to neighboring processes through message
 passing. Note that, in columnwise distribution, the
 boundary elements are contiguous in the memory and they can be
 transferred without using derived data types.
beginfigure[H]
centering
includegraphics [width=6cm] figure3.eps
captionDividing the Domain between Two Processes 
labelfig:figure3
endfigure
<p>
Now let us examine the program to see how  to divide the domain
among the processes; express the initial and boundary conditions
in a parallel program; and also, generate output files.
<p>

    PROGRAM DIFFUSION

    include  'mpif.h'

c   N,M: Number of grids in the x and y
c   directions
c   root: Rank of the process to which array
c   elements with an initial value of 5 are
c   mapped
c   FLD,WKSP:Concentration of the solute
c   DF:Diffusivity
c   L:Length of one side of the square domain

    integer N, M, root,NT
    integer COUNT,I,J
    double precision FLD, WKSP
    double precision DELTA,DELTAT,DELTAX
    double precision DELTAY,DELTA1,DELTA2
    double precision DF,L,T

c   Variables used for opening output files
    character*12 hostname
    integer status, hostnm
    character*32 filename

c   Variables used for timing
    double precision FUNCTION DSECNDS
    double precision XL, Y1,Y2

c   Control parameter
    logical  done

    dimension   FLD(2401,2401),WKSP(2401,2401)
    integer ISTATUS(MPI_STATUS_SIZE)

c   Start the timer
    XL=0.0
    Y1=DSECNDS(XL)

    CALL MPI_INIT(IERR)

c   Initializing the parameters
    N=2401
    M=2401
    T=10.0d0
    NT=100
    DELTAT=T/NT
    L=100.0d0
    DELTAX=L/(N-1)
    DELTAY=L/(M-1)
    DELTA1=DELTAT/(DELTAX**2.0d0)
    DELTA2=DELTAT/(DELTAY**2.0d0)
    DF=(10.0d0**(-4.0d0))*25.0d0
    DONE=.FALSE.

    CALL MPI_COMM_SIZE(MPI_COMM_WORLD,NPROCS,
  &amp; IERR)
    CALL MPI_COMM_RANK(MPI_COMM_WORLD,MYRANK,
  &amp; IERR)

c   The subroutine  RANGE distributes the array
c   onto the processes.
c   JSTA and JEND stand for the first and the
c   last column mapped to each process.
    CALL RANGE (1,M,NPROCS,MYRANK,JSTA,JEND)

    ROOT=(NPROCS-1)/2
    JSTA2=JSTA
    JEND1=JEND
    IF (MYRANK.EQ.0) JSTA2=2
    IF (MYRANK.EQ.(NPROCS-1)) JEND1=M-1
    INEXT=MYRANK+1
    IPREV=MYRANK-1
    IF (MYRANK.EQ.(NPROCS-1))INEXT=MPI_PROC_NULL
    IF(MYRANK.EQ.0) IPREV=MPI_PROC_NULL

c   Initial Conditions
    DO J=JSTA2,JEND1
    DO I=2,N-1
    FLD(I,J)=2.0d0
    ENDDO
    ENDDO
    IF (MYRANK.EQ.ROOT) THEN
    DO J= 1190,1200
    DO I=1190,1200
    FLD(I,J)=5.0d0
    ENDDO
    ENDDO
    ENDIF

c   Boundary Conditions
    DO J=JSTA,JEND
    FLD(1,J)=1.0d0
    FLD(N,J)=1.0d0
    ENDDO
    IF (MYRANK.EQ.0) THEN
    DO I=1,N
    FLD(I,JSTA)=1.0d0
    ENDDO
    ENDIF
    IF (MYRANK.EQ.(NPROCS-1)) THEN
    DO I=1,N
    FLD(I,JEND)=1.0d0
    ENDDO
    ENDIF

c   Main Loop

    DO  COUNT=1,500
c   Transfer the boundary elements to
c   neigboring processes
    CALL MPI_ISEND(FLD(1,JEND),N,MPI_DOUBLE_
  &amp; PRECISION, inext,1,MPI_COMM_WORLD,ISEND1,IERR)
    CALL MPI_ISEND(FLD(1,JSTA),N,MPI_DOUBLE_
  &amp; PRECISION,iprev,1,MPI_COMM_WORLD,ISEND2,IERR)
    CALL MPI_IRECV(FLD(1,JSTA-1),N,MPI_DOUBLE_
  &amp; PRECISION,iprev,1,MPI_COMM_WORLD,IRECV1,IERR)
    CALL MPI_IRECV(FLD(1,JEND+1),N,MPI_DOUBLE_
  &amp;  PRECISION,inext,1,MPI_COMM_WORLD,IRECV2,IERR)

    CALL MPI_WAIT(ISEND1,ISTATUS,IERR)
    CALL MPI_WAIT(ISEND2,ISTATUS,IERR)
    CALL MPI_WAIT(IRECV1,ISTATUS,IERR)
    CALL MPI_WAIT(IRECV2,ISTATUS,IERR)

c   Updates the concentration using the finite
c   difference formula
    DO J=JSTA2,JEND1
    DO I=2,N-1
    WKSP(I,J)=FLD(I,J)+DELTA1*DF*(FLD(I+1,J)
  &amp; -2.0*FLD(I,J)+FLD(I-1,J))
  &amp; +DELTA2*DF*(FLD(I,J+1)-2.0*FLD(I,J)+FLD(I,J-1))
    ENDDO
    ENDDO

    DO J=JSTA2,JEND1
    DO I=2,N-1
    FLD(I,J)=WKSP(I,J)
    ENDDO
    ENDDO

c   Check whether the concentration in the middle
c   of the small square is below 2.5. If so, change
c   the control parameter "done" to true

    IF (MYRANK.EQ.ROOT) THEN
    WRITE(*,*) FLD(1195,1195), 'mdfld'
    IF (FLD(1195,1195).le.2.5) THEN
    WRITE(*,*) fld(1195,1195), 'mdfld'
    DONE=.TRUE.
    WRITE (*,*) done , myrank, 'done'
    ENDIF
    ENDIF

c   Collective Communication ("done" is transferred
c   from root to all processes)

    CALL MPI_BCAST(done,1,MPI_LOGICAL,root,
 &amp;  MPI_COMM_WORLD,ierr)
    IF (DONE) GO TO 100

    ENDDO

c   Prints the number of iterations necessary for
c   the concentration to go down to 2.5
100 WRITE(*,*) count, myrank, 'count'
c   Opens a file for each process for I/O as
c   "mpi.hostname"
    STATUS=HOSTNM(HOSTNAME)
    WRITE(*,*) HOSTNAME,MYRANK
    FILENAME='mpi.' // hostname
    OPEN(unit=myrank,file=filename,
  &amp; status='unknown')

    DO J=JSTA,JEND
    DO I=1,N
    WRITE(myrank,*) fld(i,j)
    ENDDO
    ENDDO

c   Terminates the parallel execution
    CALL MPI_FINALIZE(IERR)

c   Prints the elapsed time.
    Y2=DSECNDS(XL)
    Y2=Y2-Y1
    WRITE(*,*) Y2, 'seconds'

    END

    SUBROUTINE RANGE (n1,n2,nprocs,irank,ista,iend)
c   Number of columns that will be mapped to
c   each process
    iwork1=(n2-n1+1)/nprocs
c   Determines the offset
    iwork2=MOD(n2-n1+1,nprocs)
c   Distributes the extra columns to the processes
c   If mod()is 2 for example, an extra column will
c   be mapped to process 0 and process 1
    ista=irank*iwork1+n1+MIN(irank,iwork2)
    iend=ista+iwork1-1
    if(iwork2.gt.irank) iend=iend+1
    return
    end

<p>
The following figures show the initial and final states of the
system.
beginfigure[H]
includegraphics [width=13cm] result1.eps
captionSolute Concentration at t=0 
endfigure
beginfigure[H]
includegraphics [width=13cm] result2.eps
captionSolute Concentration after 366 iterations 
endfigure
<br>Exercises
<br>
<br> Modify the diffusion program for the following system.  Run
the code with 1,2,3 and 4 nodes and compare the execution times.
Try both non-blocking and blocking communications.
beginfigure[H]
centering
includegraphics [height=6cm]ex1.eps
captionThe Prototype System for Exercise1
endfigure
<br> Advanced Exercise. This time, there are two high
concentration regions as shown in the following Figure. Assume
that these regions are 10 grids away from the boundaries in the y
direction. The location of the squares in the x direction is the
same as before. Program should terminate when both of the
concentrations  go below 3.5. Again, run the program with different
numbers of nodes and compare the results.
beginfigure[H]
centering
includegraphics [height=6cm] ex2.eps
captionThe Prototype System for Exercise2 
endfigure

<p>
High Performance Fortran
HPF has been developed as a set of extensions to Fortran 90, to
provide parallel array and loop operations. Although HPF is
simpler than writing message passing code, it is not well-suited
to handle irregular data structures or control parallel programs.
In  an HPF program, each processor runs the same program on part
of the overall data. HPF directives are used for the distribution
of data among the processors. HPF supports both of the source
formatting (free and fixed forms) provided by Fortran.  An HPF
directive may have any of the following forms:
<p>

CHPF$   directive
!HPF$   directive
*HPF$   directive

<p>
The only form that can be used in  free source format for the HPF
directives is the !HPF$. If you use the !HPF$, your program will
be universally valid. Note that, these directives will only be
executed by HPF compilers. For a standard Fortran compiler, they
are just  comments and will be ignored. Now let us go over some of
the basic HPF directives used for data distribution and parallel
data execution.
<p>
<br>Data Distribution
 We will examine three important directives:
 PROCESSORS, DISTRIBUTE and ALIGN.
 sub<br> The PROCESSORS Directive
The PROCESSORS directive declares the number and rank of a
processor arrangement. The default is the number of processors
specified by the runtime command-line options.
 The syntax of a PROCESSORS definition is similar to the syntax of an array definition in Fortran. For example,
<p>

!HPF$   PROCESSORS  Example(4)

declares a set of 4 processors, named Example.

!HPF$  PROCESSORS Example(2,2)
\end {verbatim}
specifies 4 abstract processors in a 2x2 array. Note that the
processors-name can not be the same as a variable or constant
name. It is possible to inquire the number of processors from the
system using the HPF intrinsic functions NUMBER\_OF\_PROCESSORS()
and PROCESSOR\_SHAPE()

\sub<br> {The DISTRIBUTE Directive}

DISTRIBUTE specifies the mapping of data onto processors. The
syntax for this directive is as follows:

\begin{verbatim}
!HPF$   DISTRIBUTE ARRAY [BLOCK /CYCLIC]
c       [ONTO PROCESSORS]

<p>
HPF provides two different options to distribute arrays: BLOCK and
CYCLIC. BLOCK places successive elements in the same processors,
whereas  CYCLIC distributes the elements  in a round-robin
fashion. Block distributions are suitable for problems that have a
regular domain decomposition. Cyclic is good where the work load
is not the same for each  of the array. Note that
distributions must be selected in such a way to minimize
communications.  Now, let us take a look at the  following
examples:
<p>
Example 1: Assume  A is a  one dimensional array of 8 elements.

!HPF$   PROCESSORS P(4)
!HPF$   DISTRIBUTE A(BLOCK)  onto P

The array will be distributed  in the following fashion:
beginfigure[H]
centering
includegraphics  block1.eps
caption1-D Block Distribution 
endfigure
Example 2:

 !HPF$  PROCESSORS P(4)
 !HPF$  DISTRIBUTE A(CYCLIC)  onto P

beginfigure[H]
centering
includegraphics  cyclic.eps
caption1-D Cyclic Distribution 
endfigure
<p>
Example 3: B is a two
dimensional array B(4,4)

!HPF$ PROCESSORS P(4)
!HPF$  DISTRIBUTE  B(*,BLOCK)  onto P

beginfigure[H]
centering
 epsffileblock2.eps
caption(*,Block) Distribution 
endfigure
Example 4:

!HPF$   PROCESSORS P(2,2)
!HPF$   DISTRIBUTE B(BLOCK,BLOCK) onto P

beginfigure[H]
includegraphics  block3.eps
caption (Block,Block) Distribution 
endfigure
sub<br> The ALIGN Directive
<p>
The ALIGN directive specifies that a data object is to be mapped
in the same way as another one. Elements of arrays that are
aligned are mapped to the same abstract processor. The aim of the
alignment is to reduce communications between processors. Assume
that A, B are one dimensional arrays and each has 8 elements.

 !HPF$  PROCESSORS P(4)
 !HPF$  DISTRIBUTE A(BLOCK) ONTO P
 !HPF$  ALIGN  B(:) WITH A(:)

<p>
The alignment says that A(i) and B(i) must
be placed on the same processor. Now let us take a look at the
following example:

        REAL, DIMENSION(4)   :: A
        REAL, DIMENSION(8)   :: B
!HPF$   ALIGN A(i) WITH B(i*2)

<p>
In this case, A(1)
and B(2); A(2) and B(4); ...A(4) and B(8) will reside on the same
processor.
<p>
<br>Data Parallel Execution
 HPF provides data parallel execution by special array assignments.
 Here we will only examine
the FORALL statement.
<p>
sub<br>The FORALL Statement
FORALL is introduced as an alternative to the DO-loop. The
contents of a FORALL statement can be executed in any order.
FORALL gives identical results whether applied sequentially or in
parallel. Using FORALL is especially useful where assignments are
based on array indices.

    FORALL (I=1:1000,J=1:1000) A(I,J)=I+J

We can also use FORALL in the following form:

    FORALL (I=1:1000, J=1:1000)
    A(I,J)=I+J
    B(I,J)=A(I,J)
    END FORALL

<p>
<br>Additional Information on  HPF
<p>
Here, we will  look at the extrinsic procedures which are used to
call different languages and library functions in HPF.
<p>
sub<br>The HPF EXTRINSIC Routine
The HPF Extrinsic routine is provided to call a procedure beyond
the HPF language or style. This is called a local procedure
because HPF compiler calls it as though it were a serial routine
on each node. A local procedure can be written in F77, F90, C, C++
for example. It may be written even in HPF . An
EXTRINSIC(HPF_LOCAL) subroutine can call library modules such as
HPF_LOCAL_LIBRARY. This module includes some system inquiry
procedures such as  MY_PROCESSOR which returns the identifying
number of the calling physical processor.
<p>
Now let us examine the HPF version of the diffusion program.

    PROGRAM DIFFUSIONHPF
    IMPLICIT NONE

c   N,M: Number of grids in the x and y direction
c   FLD,WKSP:Concentration of the solute
c   DF: Diffusivity
c   L:Length of one side of the square domain

    INTEGER, PARAMETER :: N=2401, M=2401,
  &amp;                  MAXCOUNT=10000 ,no=10
    INTEGER   :: COUNT,I,nt,mynode
    DOUBLE PRECISION, DIMENSION(N,M) ::BOUND,
  &amp;          FLD,WKSP,CHANGE
    DOUBLE PRECISION ::DIFF,DF,L,T
    LOGICAL, DIMENSION (N,M) :: MASK
    DOUBLE PRECISION :: delta1,delta2,
  &amp;          deltat, deltax,deltay

c   Variables used for opening an output file
    INTEGER            :: unit_number
    CHARACTER (LEN=32) :: file_name

c    Variables used for timing
    CHARACTER (LEN = 12) :: DATE,TIMED,TIMED2
    INTEGER :: STARTTIME,COUNTRATE,ENDTIME
    REAL :: ELAPSEDTIME

c   Declares number of processors
!HPF$   PROCESSORS PRNUM(2,2)

c   Specifies the distribution of data
c   onto processes
!HPF$   DISTRIBUTE FLD(block,block) onto prnum

c   Alignment of data objects
!HPF$   ALIGN CHANGE(:,:) WITH FLD(:,:)
!HPF$   ALIGN WKSP(:,:) WITH FLD(:,:)
!HPF$   ALIGN MASK(:,:) WITH FLD(:,:)

c   Starts the timer
    CALL DATE_AND_TIME (DATE,TIMED)
    CALL SYSTEM_CLOCK(starttime, countrate)

c   Initialization of the parameters
    L=100.0d0
    T=10.0d0
    DF=(10.0d0**(-4.0d0))*25.0d0
    NT=100
    DELTAT=T/NT
    DELTAX=L/(N-1)
    DELTAY=L/(M-1)
    DELTA1=DELTAT/(DELTAX**2.0d0)
    DELTA2=DELTAT/(DELTAY**2.0d0)


c   OPEN (NO,FILE='dif.EX')

c   Boundary Values
    FLD=2.0d0
    FLD(UBOUND(FLD,DIM=1),:)=1.0d0  !S
    FLD(LBOUND(FLD,DIM=1),:)=1.0d0  !N
    FLD(:,UBOUND(FLD,DIM=2))=1.0d0  !E
    FLD(:,LBOUND(FLD,DIM=2))=1.0d0  !W

    FLD(1190:1200,1190:1200)=5.0d0

c   Assigns false to the Logical Variable
c   Mask at the boundaries
    MASK=.FALSE.
    MASK(2:N-1,2:M-1)=.TRUE.

c   Main Loop
    COUNT=0

    DO
    WHERE(MASK)
c   Updates the concentration using finite
c   difference formula and  CSHIFT
    WKSP=FLD+DELTA1*DF*(CSHIFT(FLD,1,1)+
     &amp;  CSHIFT(FLD,-1,1)-2.0*FLD)
     &amp;   +DELTA2*DF*(CSHIFT(FLD,1,2)+
     &amp;  CSHIFT(FLD,-1,2)-2.0*FLD)

    CHANGE=WKSP-FLD
    ENDWHERE
    FLD=FLD+CHANGE
        write(*,*) fld(1195,1195)

        COUNT=COUNT+1

c   Checks whether the concentration in the
c   middle of the small square is below 2.5

    IF (FLD(1195,1195).LE.2.5) go to 5500
    ENDDO
    PRINT*, TIMED,TIMED2
5500    PRINT*, count
    PRINT*, FLD(1195,1195),'MIDFLD'

c   Calls extrinsic subroutine to open
c   an output file for each process

500  call p_open (fld)
1922    FORMAT (F17.15)

c   Alternatively,  all data can be
c   written in a single file

    write(no,1922) fld

c   Stop the timer
    CALL DATE_AND_TIME (DATE,TIMED2)
    CALL SYSTEM_CLOCK(EndTime)
        ElapsedTime = REAL(EndTime -
     &amp;  StartTime) / REAL(CountRate)
        write(*,*) 'Wall clock time =
     &amp;  ', ElapsedTime, ' seconds'
        write (*,*) 'Start Time = ',TIMED
        write (*,*) 'End Time = ',TIMED2

    END PROGRAM DIFFUSIONHPF

    extrinsic(hpf_local) subroutine p_open (UD)

c   Allows using HPF_LOCAL_LIBRARY procedures
c   such as MY_PROCESSOR
    USE HPF_LOCAL_LIBRARY

    integer  :: mynode
    double precision, dimension(:,:),
 &amp;              intent(in) :: UD
    character(LEN=12) :: hostname
    character(LEN=32)  ::filename
    INTEGER  ::  hostnm, status

c   The library "lib3f.h" should be
c   included to be able to use the
c   function hostnm in an HPF code
    INCLUDE 'lib3f.h'

!hpf$   distribute *(block,block) :: UD

    mynode = MY_PROCESSOR()
    status=hostnm(hostname)
    write(*,*) hostname,mynode

c   Each processor opens an output file
c   as "filehpf.hostname
         filename='filehpf.' // hostname
1923     FORMAT (F17.15)

    OPEN(unit=mynode,file=filename,
     &amp;          status='unknown')
    write(mynode,1923)UD

    end subroutine p_open

<p>
Note that CSHIFT(array,shift,dimension) performs a circular shift
along a given dimension of an array
<p>

    1 2 3                  4 5 6
M=  4 5 6   CSHIFT(M,1,1)= 7 8 9
    7 8 9                  1 2 3
                   2 3 1
    CSHIFT(M,1,2)= 5 6 4
                   8 9 7

<br>Exercises
<br>
<br> Work on the problems of the previous  using HPF. Try
different distributions such as (*,Block), (Block,*) and
(Block,Block) on different numbers of processors. Compare the
execution times.

<p>
A Brief Introduction to PBS
label:pbs
PBS or it Portable Batch System is queuing software that controls the flow
of jobs on distributed systems.  PBS has the capability to have different queues
with different priorities.  One of the complications of running a parallel
machine is that not all jobs require all of the resources.  This means that
system resources would be idle if the system could only run one job at a time.
On the other hand, if there were no job control none of the jobs would run
efficiently.  PBS is flexible enough to handle many different configurations.
This  of the class is intended to give you an introduction to the basic
functions of PBS as we use them on our clusters. We will introduce the
several of the commands in the exercises at the end of this .
<br>Queues, qsub, and the submit scripts
Queues are at the heart of PBS.  There are several types and a long list of
possible settings for each type.  This can be used to sort jobs by various
resources, and assign them different priorities.  On our cluster we run with
only one queue called router.
<p>
Jobs are submitted to the queuing system using the command it qsub. In an
environment that has need of greater complexity, the command can be use to
control which queue a job enters, when the job is run, and controlling various
other aspects of a job.
<p>
While much of the job control can be accomplished using qsub command line
parameters, a preferable method is to use submission scripts.  These scripts
can set the environment and PBS variables to control the run. This is a
preferable method since it is much simpler to rerun a job with small changes.
<p>
Below is a sample script to run the HelloWorld program on 4 nodes.

#!/bin/sh
#PBS -l nodes=4
mpirun -np 4 -machinefile $PBS_NODEFILE HelloWorld
       -pgf77

If these lines were in a file called hello.pbs, then the program HelloWorld
could be submitted to the cluster with the command "qsub hello.pbs".
Although this is a simple file, there are a few points of interest.  The line
"#PBS -l nodes =4" is a PBS directive line.  Any line that starts with #PBS
is interpreted as a directive.  In this case it is telling PBS that it
needs 4 nodes to execute the program.  Notice that this number matches the
parameter -np in the mpirun line.  The number allocated by PBS must be greater
than or equal to the number requested by the mpirun command.  In the
mpirun line we have a PBS environment variable.  Although this is a shell
script PBS makes certain changes to the environment.  In this case, a list of
nodes that PBS is making available is stored in the $PBS_NODEFILE variable
and passed to mpirun.
<p>
If the above program were submitted with the qsub program above we would see
the following sequence of events.

&gt;qsub hello.pbs
&gt;100.mimir

This would be telling you that your job was assigned the number 100. When
the job was finished you would have two new files in this directory with
the names "hello.pbs.e100" and "hello.pbs.o100". The .e file contains the
output from standard error, and any errors that PBS encounters.  The .o file
contains the output from the submit script and the program.
<p>
The list below show some of the common modifications to the submit script
that you might want to use.
<p>
begindescription
<br>[#PBS -joe] Combine the error and output files.
<br>[#PBS -N ] Set the name of the job. This affects the name of the output
files.
<br>[#PBS -l] Sets the resource lists for the jobs -l can be followed by
a resource list which often includes the following <br>s.
   begin<br>ize
      <br> nodes - Specifies which may be followed by a formatted string specifying which nodes to use.
      <br> ncpus - Sets how many cpus are needed. This is more important with
SMP nodes, or in a pure SMP environment.
      <br> walltime - Sets an upper limit on the length of the job.
      <br> cputime - Sets an upper limit on the cpu time used for the job.
   end<br>ize
<br> [cd] Change the directory.
enddescription
<p>
There are many other options available and the complete Users Guide is available
on line from ftp.acomp.usf.edu.
<br>Exercises
<br>
<br> Create a submission script to run the diffusion program from the last
  on eight nodes and submit it by logging on to mimir and using qsub.
<br> While everyone is running their programs on mimir, log onto mimir and
investigate the commands qstat, pbs_stats,xpbsmon and xpbs. You may have to
rerun jobs if they all finish too fast.

<p>
Further Reading
label:addmat
begin<br>ize
<br> Books
begin<br>ize
   <br> it Using MPI:Portable Parallel Programming with the Message-Passing Interface by
   William Gropp, Ewing Lusk and Anthony Skjellum
   <br> it The High Performance Fortran Handbook by Charles H. Koelbel
end<br>ize
<br> On-line Tutorials
   begin<br>ize
   <br>  MPI - http://www.tacc.utexas.edu/ newline
			resources/user_guides/mpi/<br>.php
   <br>  HPF - http://www.jics.utk.edu/ newline
			HPF/HPFguide.html
   end<br>ize
end<br>ize
<p>


appendix

<p>VIM QUICK REFERENCE CARD
indexvi reference
label<p>:vimqrc
begintabular*4in rl
multicolumn2lbf Basic Movement
<BR>
hline
&amp;
<BR>
h l k j&amp;character left, right; line up, down
<BR>
b w&amp;word/token left, right
<BR>
ge e&amp;end of word/token left, right
<BR>{ } &amp;beginning of previous, next paragraph
<BR>( )&amp;beginning of previous, next sentence
<BR>
0 gm&amp;beginning, middle of line
<BR>^ $&amp;first, last character of line
<BR><IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
G <IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
gg&amp;line <IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
, default the last, first
<BR><IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
%&amp;percentage <IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
 of the file it(<IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
 must be provided)
<BR><IMG
 WIDTH="11" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img36.png"
 ALT="$ n\vert$">
&amp;column <IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
 of current line
<BR>%&amp;match of next brace, bracket, comment, tt#define
<BR><IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
H <IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
L&amp;line <IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
 from start, bottom of window
<BR>
M&amp;middle line of window
<BR>&amp;
<BR>
multicolumn2lbf Insertion &amp; replace <IMG
 WIDTH="16" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img37.png"
 ALT="$ to$">
 insert mode
<BR>
hline
&amp;
<BR>
i a&amp;insert before, after cursor
<BR>
I A&amp;insert at beginning, end of line
<BR>
gI&amp;insert text in first column
<BR>
o O&amp;open a new line below, above the current line
<BR>
r<IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
&amp;replace character under cursor with <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">

<BR>
gr<IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
&amp;like tt r, but without affecting layout
<BR>
R&amp;replace characters starting at the cursor
<BR>
gR&amp;like tt R, but without affecting layout
<BR>
c<IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">
&amp;change text of movement command <IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">

<BR>
cc or S&amp;change current line
<BR>
C&amp;change to the end of line
<BR>
s&amp;change one character and insert
<BR> ~<br>&amp; switch case and advance cursor
<BR>
g ~<br><IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">
 &amp; switch case of movement command <IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">

<BR>
endtabular*
newpage
begintabular*4in rl
multicolumn2lbf Insertion &amp; replace <IMG
 WIDTH="16" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img37.png"
 ALT="$ to$">
 insert mode (continued)
<BR>
hline
&amp;
<BR>
gu<IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">
 gU<IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">
&amp;lowercase, uppercase text of movement <IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">

<BR><IMG
 WIDTH="16" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img40.png"
 ALT="$ &lt;$">
<IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">
 <IMG
 WIDTH="14" HEIGHT="22" ALIGN="MIDDLE" BORDER="0"
 SRC="img41.png"
 ALT="$ &gt;$">
<IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">
&amp;shift left, right text of movement <IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">

<BR><IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
<IMG
 WIDTH="16" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img40.png"
 ALT="$ &lt;$">
kern-3pt<IMG
 WIDTH="16" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img40.png"
 ALT="$ &lt;$">
 <IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
<IMG
 WIDTH="14" HEIGHT="22" ALIGN="MIDDLE" BORDER="0"
 SRC="img41.png"
 ALT="$ &gt;$">
kern-3pt<IMG
 WIDTH="14" HEIGHT="22" ALIGN="MIDDLE" BORDER="0"
 SRC="img41.png"
 ALT="$ &gt;$">
&amp;shift <IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
 lines left, right
<BR>&amp;
<BR>
multicolumn2lbf Deletion
<BR>
hline
&amp;
<BR>
x X &amp; delete character under, before cursor
<BR>
d<IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">
 &amp; delete text of movement command <IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">

<BR>
dd D &amp; delete current line, to the end of line
<BR>
J gJ &amp; join current line with next, without space
<BR>:<IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
d<!-- MATH
 $hookleftarrow$
 -->
&amp; delete range <IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
 lines
<BR>:<IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
d<IMG
 WIDTH="9" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img43.png"
 ALT="$ x$">
<!-- MATH
 $hookleftarrow$
 -->
&amp; delete range <IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
 lines into register <IMG
 WIDTH="9" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img43.png"
 ALT="$ x$">

<BR>
<p>
&amp;
<BR>
multicolumn2lbf Insert Mode
<BR>
hline
&amp;
<BR>
rmchar94kern-1ptV<IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
 rmchar94kern-1ptV<IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
&amp;insert char <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
 literally, decimal value <IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">

<BR>
rmchar94kern-1ptV<IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
&amp;insert decimal value of character
<BR>
rmchar94kern-1ptA&amp;insert previously inserted text
<BR>
rmchar94kern-1pt@&amp;same as ttrmchar94kern-1ptA and stop 
<BR>&amp;insert <IMG
 WIDTH="16" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img37.png"
 ALT="$ to$">
 command mode
<BR>
rmchar94kern-1ptR<IMG
 WIDTH="9" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img43.png"
 ALT="$ x$">
 rmchar94kern-1ptRrmchar94kern-1ptR<IMG
 WIDTH="9" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img43.png"
 ALT="$ x$">
&amp;insert content of register <IMG
 WIDTH="9" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img43.png"
 ALT="$ x$">
, literally
<BR>
rmchar94kern-1ptN rmchar94kern-1ptP&amp;text completion before, after cursor
<BR>
rmchar94kern-1ptW&amp;delete word before cursor
<BR>
rmchar94kern-1ptU&amp;delete all inserted character in current line
<BR>
rmchar94kern-1ptD rmchar94kern-1ptT&amp;shift left, right one shift width
<BR>
rmchar94kern-1ptK<IMG
 WIDTH="11" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img44.png"
 ALT="$ c_1$">
<IMG
 WIDTH="16" HEIGHT="18" ALIGN="MIDDLE" BORDER="0"
 SRC="img45.png"
 ALT="$ c_2$">
 or <IMG
 WIDTH="11" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img44.png"
 ALT="$ c_1$">
kern-1pt<IMG
 WIDTH="16" HEIGHT="18" ALIGN="MIDDLE" BORDER="0"
 SRC="img45.png"
 ALT="$ c_2$">
&amp;enter digraph <!-- MATH
 $\{c_1,c_2\}$
 -->
<IMG
 WIDTH="16" HEIGHT="18" ALIGN="MIDDLE" BORDER="0"
 SRC="img46.png"
 ALT="$ \{c_1,c_2\}$">

<BR>
rmchar94kern-1ptO<IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
&amp;execute <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
 in temporary command mode
<BR>
rmchar94kern-1ptXrmchar94kern-1ptE rmchar94kern-1ptXrmchar94kern-1ptY&amp;scroll up, down
<BR><IMG
 WIDTH="106" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img2.png"
 ALT="$ langle$">
rmitesc<IMG
 WIDTH="45" HEIGHT="27" ALIGN="MIDDLE" BORDER="0"
 SRC="img3.png"
 ALT="$ rangle$">
 or rmchar94kern-1pt[ &amp; abandon edition <IMG
 WIDTH="16" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img37.png"
 ALT="$ to$">
 command mode
<BR>&amp;
<BR>
multicolumn2lbf Copying
<BR>
hline
&amp;
<BR>"<IMG
 WIDTH="9" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img43.png"
 ALT="$ x$">
&amp;use register <IMG
 WIDTH="9" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img43.png"
 ALT="$ x$">
 for next delete, yank, put
<BR>:reg<!-- MATH
 $hookleftarrow$
 -->
&amp;show the content of all registers
<BR>:reg <IMG
 WIDTH="9" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img43.png"
 ALT="$ x$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;show the content of registers <IMG
 WIDTH="9" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img43.png"
 ALT="$ x$">

<BR>
y<IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">
&amp;yank the text of movement command <IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">

<BR>
yy or Y&amp;yank current line into register
<BR>
p P&amp;put register after, before cursor position
<BR>]p [p&amp;like tt p, tt P with indent adjusted
<BR>
gp gP&amp;like tt p, tt P leaving cursor after new text
<BR>
endtabular*
newpage
<p>
begintabular*4in rl
multicolumn2lbf Advanced insertion
<BR>
hline
&amp;
<BR>
g?<IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">
&amp;perform rot13 encoding on movement <IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">

<BR><IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
rmchar94kern-1ptA <IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
rmchar94kern-1ptX&amp;<IMG
 WIDTH="53" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img47.png"
 ALT="$ +n$">
, <IMG
 WIDTH="23" HEIGHT="24" ALIGN="MIDDLE" BORDER="0"
 SRC="img48.png"
 ALT="$ -n$">
 to number under cursor
<BR>
gq<IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">
&amp;format lines of movement <IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">
 to fixed width
<BR>:<IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
ce <IMG
 WIDTH="23" HEIGHT="21" ALIGN="BOTTOM" BORDER="0"
 SRC="img49.png"
 ALT="$ w$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;center lines in range <IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
 to width <IMG
 WIDTH="23" HEIGHT="21" ALIGN="BOTTOM" BORDER="0"
 SRC="img49.png"
 ALT="$ w$">

<BR>:<IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
le <IMG
 WIDTH="13" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img50.png"
 ALT="$ i$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;left align lines in range <IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
 with indent <IMG
 WIDTH="13" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img50.png"
 ALT="$ i$">

<BR>:<IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
ri <IMG
 WIDTH="23" HEIGHT="21" ALIGN="BOTTOM" BORDER="0"
 SRC="img49.png"
 ALT="$ w$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;right align lines in range <IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
 to width <IMG
 WIDTH="23" HEIGHT="21" ALIGN="BOTTOM" BORDER="0"
 SRC="img49.png"
 ALT="$ w$">

<BR>!<IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">
<IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;filter lines of movement <IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">
 through command <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">

<BR><IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
!!<IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;filter <IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
 lines through command <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">

<BR>:<IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
!<IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;filter range <IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
 lines through command <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">

<BR>&amp;
<BR>
multicolumn2lbf Visual mode
<BR>
hline
&amp;
<BR>
v V rmchar94kern-1ptV&amp;start/stop highlighting characters, lines, block
<BR>
o&amp;exchange cursor position with start of
<BR>&amp; highlighting
<BR>
gv&amp;start highlighting on previous visual area
<BR>
aw as ap&amp;select a word, a sentence, a paragraph
<BR>
ab aB&amp;select a block ( ), a block ttchar123  ttchar125 
<BR>
<p>
&amp;
<BR>
multicolumn2lbf Undoing,repeating, &amp; registers
<BR>
hline
&amp;
<BR>
u U&amp;undo last command, restore last changed line
<BR>.thinspacethinspacermchar94kern-1ptR&amp;repeat last changes, redo last undo
<BR><IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
. &amp;repeat last changes with count replaced by <IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">

<BR>
q<IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
 q<IMG
 WIDTH="7" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img51.png"
 ALT="$ C$">
&amp;record, append typed characters in register <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">

<BR>
q&amp;stop recording
<BR>@<IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
&amp;execute the content of register <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">

<BR>@@&amp;repeat previous tt @ command
<BR>:@<IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;execute register <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
 as an it Ex command
<BR>:<IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
g/<IMG
 WIDTH="14" HEIGHT="16" ALIGN="MIDDLE" BORDER="0"
 SRC="img52.png"
 ALT="$ p$">
/<IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;execute it Ex command <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
 on range <IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">

<BR>&amp;where pattern <IMG
 WIDTH="14" HEIGHT="16" ALIGN="MIDDLE" BORDER="0"
 SRC="img52.png"
 ALT="$ p$">
 matches
<BR>&amp;
<BR>
multicolumn2lbf Complex Movement
<BR>
hline
&amp;
<BR>- +&amp;line up, down on first non-blank character
<BR>
B W&amp;space-separated word left, right
<BR>
gE E&amp;end of space-separated word left, right
<BR><IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
_&amp;down <IMG
 WIDTH="11" HEIGHT="18" ALIGN="MIDDLE" BORDER="0"
 SRC="img53.png"
 ALT="$ n-1$">
 line on first non-blank character
<BR>
g0&amp;beginning of it screen line
<BR>
g^ g$&amp;first, last character of it screen line
<BR>
gk gj&amp;it screen line up, down
<BR>&amp;
<BR>
endtabular*
newpage
begintabular*4in rl
multicolumn2lbf Complex Movement (continued)
<BR>
hline
&amp;
<BR>
f<IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
 F<IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
&amp;next, previous occurence of character <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">

<BR>
t<IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
 T<IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
&amp;before next, previous occurence of <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">

<BR>; ,&amp;repeat last tt fFtT, in opposite direction
<BR> [[ ]]<br>&amp;start of  backward, forward
<BR> [] ][<br>&amp;end of  backward, forward
<BR> [( ])<br> &amp;unclosed (, ) backward, forward
<BR> [<br>char123   ]<br>char125 &amp;unclosed ttchar123 , ttchar125  backward, forward
<BR> [m ]m<br> &amp;start of backward, forward it Java method
<BR> [<br>#  ]<br># &amp;unclosed tt#if, tt#else, tt#endif backward,
<BR>&amp; forward
<BR>
<p>
 [* ]*<br>&amp;start, end of tt/* */ backward, forward
<BR>&amp;
<BR>
multicolumn2lbf Search &amp; substitution
<BR>
hline
&amp;
<BR>/<IMG
 WIDTH="39" HEIGHT="26" ALIGN="BOTTOM" BORDER="0"
 SRC="img54.png"
 ALT="$ s$">
<!-- MATH
 $hookleftarrow$
 -->
 ?<IMG
 WIDTH="39" HEIGHT="26" ALIGN="BOTTOM" BORDER="0"
 SRC="img54.png"
 ALT="$ s$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;search forward, backward for <IMG
 WIDTH="39" HEIGHT="26" ALIGN="BOTTOM" BORDER="0"
 SRC="img54.png"
 ALT="$ s$">

<BR>/<IMG
 WIDTH="39" HEIGHT="26" ALIGN="BOTTOM" BORDER="0"
 SRC="img54.png"
 ALT="$ s$">
/<IMG
 WIDTH="9" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img55.png"
 ALT="$ o$">
<!-- MATH
 $hookleftarrow$
 -->
 ?<IMG
 WIDTH="39" HEIGHT="26" ALIGN="BOTTOM" BORDER="0"
 SRC="img54.png"
 ALT="$ s$">
?<IMG
 WIDTH="9" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img55.png"
 ALT="$ o$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;search fwd, bwd for <IMG
 WIDTH="39" HEIGHT="26" ALIGN="BOTTOM" BORDER="0"
 SRC="img54.png"
 ALT="$ s$">
 with offset <IMG
 WIDTH="9" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img55.png"
 ALT="$ o$">

<BR>
n or /<!-- MATH
 $hookleftarrow$
 -->
&amp;repeat forward last search
<BR>
N or ?<!-- MATH
 $hookleftarrow$
 -->
&amp;repeat backward last search
<BR># *&amp;search backward, forward for word 
<BR>&amp;under cursor
<BR>
g# g*&amp;same, but also find partial matches
<BR>
gd gD&amp;local, global definition of symbol 
<BR>&amp;under cursor
<BR>:<IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
s/<IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img56.png"
 ALT="$ f$">
/<IMG
 WIDTH="11" HEIGHT="28" ALIGN="BOTTOM" BORDER="0"
 SRC="img57.png"
 ALT="$ t$">
/<IMG
 WIDTH="9" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img43.png"
 ALT="$ x$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;substitute <IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img56.png"
 ALT="$ f$">
 by <IMG
 WIDTH="11" HEIGHT="28" ALIGN="BOTTOM" BORDER="0"
 SRC="img57.png"
 ALT="$ t$">
 in range <IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">

<BR>&amp;<IMG
 WIDTH="7" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img58.png"
 ALT="$ x:$">
 tt g--all occurrences, tt c--confirm changes
<BR>:<IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
s <IMG
 WIDTH="9" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img43.png"
 ALT="$ x$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;repeat substitution with new <IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
 &amp; <IMG
 WIDTH="9" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img43.png"
 ALT="$ x$">

<BR>&amp;
<BR>
multicolumn2lbf Special characters in search patterns
<BR>
hline
&amp;
<BR>.thinspacethinspacethinspacermchar94kern-1pt $&amp;any single character, start, end of line
<BR>
char92 <IMG
 WIDTH="16" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img40.png"
 ALT="$ &lt;$">
 char92 <IMG
 WIDTH="14" HEIGHT="22" ALIGN="MIDDLE" BORDER="0"
 SRC="img41.png"
 ALT="$ &gt;$">
&amp;start, end of word
<BR> [<br><IMG
 WIDTH="20" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img59.png"
 ALT="$ c_1..c_2$">
 ]<br>&amp;a single character in range <IMG
 WIDTH="20" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img59.png"
 ALT="$ c_1..c_2$">

<BR> [<br>rmchar94kern-1pt<IMG
 WIDTH="20" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img59.png"
 ALT="$ c_1..c_2$">
 ]<br>&amp;a single character not in range
<BR>
char92 i char92 k char92 I char92 K&amp;an identifier, keyword; excl. digits
<BR>
char92 f char92 p char92 F char92 P&amp;a file name, printable char.; excl. digits
<BR>
char92 s char92 S&amp;a white space, a non-white space
<BR>
char92 e char92 t char92 r char92 b&amp;<IMG
 WIDTH="106" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img2.png"
 ALT="$ langle$">
rmit esc<IMG
 WIDTH="45" HEIGHT="27" ALIGN="MIDDLE" BORDER="0"
 SRC="img3.png"
 ALT="$ rangle$">
, <IMG
 WIDTH="106" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img2.png"
 ALT="$ langle$">
rmit tab<IMG
 WIDTH="45" HEIGHT="27" ALIGN="MIDDLE" BORDER="0"
 SRC="img3.png"
 ALT="$ rangle$">
, <IMG
 WIDTH="106" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img2.png"
 ALT="$ langle$">
rmit<!-- MATH
 $hookleftarrow$
 -->
<IMG
 WIDTH="45" HEIGHT="27" ALIGN="MIDDLE" BORDER="0"
 SRC="img3.png"
 ALT="$ rangle$">
, <IMG
 WIDTH="106" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img2.png"
 ALT="$ langle$">
rmit<IMG
 WIDTH="38" HEIGHT="18" ALIGN="MIDDLE" BORDER="0"
 SRC="img60.png"
 ALT="$ gets$">
<IMG
 WIDTH="45" HEIGHT="27" ALIGN="MIDDLE" BORDER="0"
 SRC="img3.png"
 ALT="$ rangle$">

<BR>
char92 = * char92 +&amp;match <IMG
 WIDTH="30" HEIGHT="24" ALIGN="BOTTOM" BORDER="0"
 SRC="img61.png"
 ALT="$ 0..1$">
, <IMG
 WIDTH="26" HEIGHT="14" ALIGN="MIDDLE" BORDER="0"
 SRC="img62.png"
 ALT="$ 0..infty$">
, <IMG
 WIDTH="57" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img63.png"
 ALT="$ 1..infty$">
 of preceding atoms
<BR>
endtabular*
newpage
begintabular*4in rl
multicolumn2lbf Special characters in search patterns (continued)
<BR>
hline
&amp;
<BR>
char92 <IMG
 WIDTH="57" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img64.png"
 ALT="$ \vert$">
&amp;separate two branches (<IMG
 WIDTH="6" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img65.png"
 ALT="$ equiv$">
 it or)
<BR>
char92 ( char92 )&amp;group patterns into an atom
<BR>
char92 &amp; char92 <IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
&amp;the whole matched pattern, <IMG
 WIDTH="40" HEIGHT="26" ALIGN="BOTTOM" BORDER="0"
 SRC="img66.png"
 ALT="$ n^{th}$">
 tt() group
<BR>
char92 u char92 l&amp;next character made upper, lowercase
<BR>
<p>
multicolumn2lbf Offsets in search commands
<BR>
hline
&amp;
<BR><IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
 or +<IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
&amp;<IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
 line downward in column 1
<BR>-<IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
&amp;<IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
 line upward in column 1
<BR>
e+<IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
 e-<IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
&amp;<IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
 characters right, left to end of match
<BR>
s+<IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
 s-<IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
&amp;<IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
 characters right, left to start of match
<BR>;<IMG
 WIDTH="24" HEIGHT="18" ALIGN="BOTTOM" BORDER="0"
 SRC="img67.png"
 ALT="$ sc$">
&amp;execute search command <IMG
 WIDTH="24" HEIGHT="18" ALIGN="BOTTOM" BORDER="0"
 SRC="img67.png"
 ALT="$ sc$">
 next
<BR>&amp;
<BR>
multicolumn2lbf Marks &amp; motions
<BR>
hline
&amp;
<BR>
m<IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
&amp;mark current position with mark <IMG
 WIDTH="16" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img68.png"
 ALT="$ cin[a..Z]$">

<BR>`<IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
 `<IMG
 WIDTH="7" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img51.png"
 ALT="$ C$">
&amp;go to mark <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
 in current, <IMG
 WIDTH="7" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img51.png"
 ALT="$ C$">
 in any file
<BR>`<IMG
 WIDTH="62" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img69.png"
 ALT="$ 0..9$">
&amp;go to last exit position
<BR>``  `"&amp;go to position before jump, at last edit
<BR>`[ `]&amp;go to start, end of previously operated text
<BR>:marks<!-- MATH
 $hookleftarrow$
 -->
&amp;print the active marks list
<BR>:jumps<!-- MATH
 $hookleftarrow$
 -->
&amp;print the jump list
<BR><IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
rmchar94kern-1ptO&amp;go to <IMG
 WIDTH="40" HEIGHT="26" ALIGN="BOTTOM" BORDER="0"
 SRC="img66.png"
 ALT="$ n^{th}$">
 older position in jump list
<BR><IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
rmchar94kern-1ptI&amp;go to <IMG
 WIDTH="40" HEIGHT="26" ALIGN="BOTTOM" BORDER="0"
 SRC="img66.png"
 ALT="$ n^{th}$">
 newer position in jump list
<BR>&amp;
<BR>
multicolumn2lbf Key mapping &amp; abbreviations 
<BR>
hline
&amp;
<BR>
<p>
:map <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
 <IMG
 WIDTH="26" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img70.png"
 ALT="$ e$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;map <IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img71.png"
 ALT="$ cmapsto e$">
 in normal &amp; visual mode
<BR>:map! <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
 <IMG
 WIDTH="26" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img70.png"
 ALT="$ e$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;map <IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img71.png"
 ALT="$ cmapsto e$">
 in insert &amp; cmd-line mode
<BR>:unmap! <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;remove mapping <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">

<BR>:mk <IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img56.png"
 ALT="$ f$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;write current mappings, settings... to file <IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img56.png"
 ALT="$ f$">

<BR>:ab <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
 <IMG
 WIDTH="26" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img70.png"
 ALT="$ e$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;add abbreviation for <IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img71.png"
 ALT="$ cmapsto e$">

<BR>:ab <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;show abbreviations starting with <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">

<BR>:una <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;remove abbreviation <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">

<BR>&amp;
<BR>
multicolumn2lbf Tags 
<BR>
hline
&amp;
<BR>:ta <IMG
 WIDTH="11" HEIGHT="28" ALIGN="BOTTOM" BORDER="0"
 SRC="img57.png"
 ALT="$ t$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;jump to tag <IMG
 WIDTH="11" HEIGHT="28" ALIGN="BOTTOM" BORDER="0"
 SRC="img57.png"
 ALT="$ t$">

<BR>:<IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
ta<!-- MATH
 $hookleftarrow$
 -->
&amp;jump to <IMG
 WIDTH="40" HEIGHT="26" ALIGN="BOTTOM" BORDER="0"
 SRC="img66.png"
 ALT="$ n^{th}$">
 newer tag in list
<BR>
rmchar94kern-1pt] rmchar94kern-1ptT&amp;jump to the tag under cursor, return 
<BR>&amp;from tag
<BR>:ts <IMG
 WIDTH="11" HEIGHT="28" ALIGN="BOTTOM" BORDER="0"
 SRC="img57.png"
 ALT="$ t$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;list matching tags and select one for jump
<BR>
endtabular*
<p>
begintabular*4in rl
multicolumn2lbf Tags (continued)
<BR>
hline
&amp;
<BR>:tj <IMG
 WIDTH="11" HEIGHT="28" ALIGN="BOTTOM" BORDER="0"
 SRC="img57.png"
 ALT="$ t$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;jump to tag or select one if multiple
<BR>&amp; matches
<BR>:tags<!-- MATH
 $hookleftarrow$
 -->
&amp;print tag list
<BR>:<IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
po<!-- MATH
 $hookleftarrow$
 -->
 :<IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
rmchar94kern-1ptT<!-- MATH
 $hookleftarrow$
 -->
&amp;jump back from, to <IMG
 WIDTH="40" HEIGHT="26" ALIGN="BOTTOM" BORDER="0"
 SRC="img66.png"
 ALT="$ n^{th}$">
 older tag
<BR>:<IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
po<!-- MATH
 $hookleftarrow$
 -->
&amp;jump back from <IMG
 WIDTH="40" HEIGHT="26" ALIGN="BOTTOM" BORDER="0"
 SRC="img66.png"
 ALT="$ n^{th}$">
 older tag in tag list
<BR>:tl<!-- MATH
 $hookleftarrow$
 -->
&amp;jump to last matching tag
<BR>
rmchar94kern-1ptWchar125  :pt <IMG
 WIDTH="11" HEIGHT="28" ALIGN="BOTTOM" BORDER="0"
 SRC="img57.png"
 ALT="$ t$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;preview tag under cursor, tag <IMG
 WIDTH="11" HEIGHT="28" ALIGN="BOTTOM" BORDER="0"
 SRC="img57.png"
 ALT="$ t$">

<BR>
rmchar94kern-1ptW ]<br>&amp;split window and show tag under cursor
<BR>
rmchar94kern-1ptWz or :pc<!-- MATH
 $hookleftarrow$
 -->
&amp;close tag preview window
<BR>&amp;
<BR>
multicolumn2lbf Scrolling &amp; multi-windowing 
<BR>
hline
&amp;
<BR>
rmchar94kern-1ptE rmchar94kern-1ptY&amp;scroll line up, down
<BR>
rmchar94kern-1ptD rmchar94kern-1ptU&amp;scroll half a page up, down
<BR>
rmchar94kern-1ptF rmchar94kern-1ptB&amp;scroll page up, down
<BR>
zt or z<!-- MATH
 $hookleftarrow$
 -->
&amp;set current line at top of window
<BR>
zz or z. &amp;set current line at center of window
<BR>
zb or z-&amp;set current line at bottom of window
<BR>
zh zl&amp;scroll one character to the right, left
<BR>
zH zL&amp;scroll half a screen to the right, left
<BR>
rmchar94kern-1ptWs or :split<!-- MATH
 $hookleftarrow$
 -->
&amp;split window in two
<BR>
rmchar94kern-1ptWn or :new<!-- MATH
 $hookleftarrow$
 -->
&amp;create new empty window
<BR>
rmchar94kern-1ptWo or :on<!-- MATH
 $hookleftarrow$
 -->
&amp;make current window one on screen
<BR>
rmchar94kern-1ptWj or rmchar94kern-1ptWk&amp;move to window below, above
<BR>
rmchar94kern-1ptWw or rmchar94kern-1ptWrmchar94kern-1ptW&amp;move to window below, above (wrap)
<BR>&amp;
<BR>
multicolumn2lbf Ex commands (<!-- MATH
 $hookleftarrow$
 -->
) 
<BR>
hline
&amp;
<BR>:e <IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img56.png"
 ALT="$ f$">
&amp;edit file <IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img56.png"
 ALT="$ f$">
, unless changes have 
<BR>&amp;been made
<BR>:e! <IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img56.png"
 ALT="$ f$">
&amp;edit file <IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img56.png"
 ALT="$ f$">
 always 
<BR>&amp;(by default reload current)
<BR>:wn :wN&amp;write file and edit next, previous one
<BR>:n :N&amp;edit next, previous file in list
<BR>:<IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
w&amp;write range <IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
 to current file
<BR>:<IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
w <IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img56.png"
 ALT="$ f$">
&amp;write range <IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
 to file <IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img56.png"
 ALT="$ f$">

<BR>:<IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
w<IMG
 WIDTH="14" HEIGHT="22" ALIGN="MIDDLE" BORDER="0"
 SRC="img41.png"
 ALT="$ &gt;$">
kern-3pt<IMG
 WIDTH="14" HEIGHT="22" ALIGN="MIDDLE" BORDER="0"
 SRC="img41.png"
 ALT="$ &gt;$">
<IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img56.png"
 ALT="$ f$">
&amp;append range <IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
 to file <IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img56.png"
 ALT="$ f$">

<BR>:q :q!&amp;quit and confirm, quit and discard 
<BR>&amp;changes
<BR>:wq or :x or ZZ&amp;write to current file and exit
<BR>
endtabular*
newpage
begintabular*4in rl
multicolumn2lbf Ex commands (continued)
<BR>
hline
&amp;
<BR><IMG
 WIDTH="106" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img2.png"
 ALT="$ langle$">
rmit up<IMG
 WIDTH="45" HEIGHT="27" ALIGN="MIDDLE" BORDER="0"
 SRC="img3.png"
 ALT="$ rangle$">
 <IMG
 WIDTH="106" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img2.png"
 ALT="$ langle$">
rmit down<IMG
 WIDTH="45" HEIGHT="27" ALIGN="MIDDLE" BORDER="0"
 SRC="img3.png"
 ALT="$ rangle$">
&amp;recall commands starting with current
<BR>:r <IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img56.png"
 ALT="$ f$">
&amp;insert content of file <IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img56.png"
 ALT="$ f$">
 below cursor
<BR>:r! <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
&amp;insert output of command <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
 below
<BR>&amp; cursor
<BR>:all&amp;open a window for each file in the 
<BR>&amp;argument list
<BR>:args&amp;display the argument list
<BR>&amp;
<BR>
multicolumn2lbf Ex ranges 
<BR>
hline
&amp;
<BR>, ; &amp;separates two lines numbers, set to first line
<BR><IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
&amp;an absolute line number <IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">

<BR>.thinspacethinspacethinspace$&amp;
	the current line, the last line in file
<BR>% *&amp;entire file, visual area
<BR>'<IMG
 WIDTH="11" HEIGHT="28" ALIGN="BOTTOM" BORDER="0"
 SRC="img57.png"
 ALT="$ t$">
&amp;position of mark <IMG
 WIDTH="11" HEIGHT="28" ALIGN="BOTTOM" BORDER="0"
 SRC="img57.png"
 ALT="$ t$">

<BR>/<IMG
 WIDTH="14" HEIGHT="16" ALIGN="MIDDLE" BORDER="0"
 SRC="img52.png"
 ALT="$ p$">
/ ?<IMG
 WIDTH="14" HEIGHT="16" ALIGN="MIDDLE" BORDER="0"
 SRC="img52.png"
 ALT="$ p$">
?&amp;the next, previous line where <IMG
 WIDTH="14" HEIGHT="16" ALIGN="MIDDLE" BORDER="0"
 SRC="img52.png"
 ALT="$ p$">
 matches
<BR>+<IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
 -<IMG
 WIDTH="172" HEIGHT="29" ALIGN="BOTTOM" BORDER="0"
 SRC="img35.png"
 ALT="$ n$">
&amp;<IMG
 WIDTH="53" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img47.png"
 ALT="$ +n$">
, <IMG
 WIDTH="23" HEIGHT="24" ALIGN="MIDDLE" BORDER="0"
 SRC="img48.png"
 ALT="$ -n$">
 to the preceding line number
<BR>
<p>
&amp;
<BR>
multicolumn2lbf Folding 
<BR>
hline
&amp;
<BR>
zf<IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">
&amp;create fold of movement <IMG
 WIDTH="8" HEIGHT="11" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.png"
 ALT="$ m$">

<BR>:<IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">
fo&amp;create fold for range <IMG
 WIDTH="14" HEIGHT="22" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$ r$">

<BR>
zd zE&amp;delete fold at cursor, all in window
<BR>
zo zc zO zC&amp;open, close one fold; recursively
<BR> [z ]z<br>&amp;move to start, end of current open fold
<BR>
zj zk&amp;move down, up to start, end of next fold
<BR>&amp;
<BR>
multicolumn2lbf Miscellaneous 
<BR>
hline
&amp;
<BR>:sh<!-- MATH
 $hookleftarrow$
 -->
 :!<IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;start shell, execute command <IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.png"
 ALT="$ c$">
 in shell
<BR>
K&amp;lookup keyword under cursor with tt man
<BR>:make<!-- MATH
 $hookleftarrow$
 -->
&amp;start tt make, read errors and jump to first
<BR>:cn<!-- MATH
 $hookleftarrow$
 -->
 :cp<!-- MATH
 $hookleftarrow$
 -->
&amp;display the next, previous error
<BR>:cl<!-- MATH
 $hookleftarrow$
 -->
 :cf<!-- MATH
 $hookleftarrow$
 -->
&amp;list all errors, read errors from file
<BR>
rmchar94kern-1ptL rmchar94kern-1ptG&amp;redraw screen, show filename and position
<BR>
grmchar94kern-1ptG&amp;show cursor column, line, and 
<BR>&amp;character position
<BR>
ga&amp;show ASCII value of character under cursor
<BR>
gf&amp;open file which filename is under cursor
<BR>:redir<IMG
 WIDTH="14" HEIGHT="22" ALIGN="MIDDLE" BORDER="0"
 SRC="img41.png"
 ALT="$ &gt;$">
<IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img56.png"
 ALT="$ f$">
<!-- MATH
 $hookleftarrow$
 -->
&amp;redirect output to file <IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img56.png"
 ALT="$ f$">

<BR>:mkview <IMG
 WIDTH="67" HEIGHT="24" ALIGN="MIDDLE" BORDER="0"
 SRC="img72.png"
 ALT="$ [f]$">
&amp;save view configuration [to file <IMG
 WIDTH="9" HEIGHT="11" ALIGN="MIDDLE" BORDER="0"
 SRC="img56.png"
 ALT="$ f$">
]
<BR>
endtabular*
<p>


 
<p>BASH REFERENCE
indexbash reference
label<p>:bashref
begintabular*4in rl
multicolumn2lbf File Attribute Operators
<BR>
hline
-d it file&amp;it file exists and is a directory
<BR>-e it file&amp;it file exists
<BR>-f it file&amp;it file exists and is a regular it file 
<BR>-r it file&amp;You have read permissions on the it file 
<BR>-s it file&amp;it file exists and is not empty 
<BR>-w it file&amp;You have write permissions on the it file
<BR>-x it file&amp;You have execute permissions on the it file
<BR>-O it file&amp;You own the it file
<BR>-G it file&amp;Files group id matches one of yours
<BR>
it file1 -nt it file2&amp;it file1 is newer than it file2
<BR>
it file1 -ot it file2&amp;it file1 is older than it file2
<BR>
endtabular*
begintabular*4in rl
multicolumn2lbf Arithmetic Test Operators
<BR>
hline
-lt&amp;Less than
<BR>-le&amp;Less than or equal
<BR>-eq&amp;Equal
<BR>-ge&amp;Greater than or equal
<BR>-gt&amp;Greater than
<BR>-ne&amp;Not equal
<BR>
endtabular*
<p>
begintabular*4in rl
multicolumn2lbf Relational Operators 
<BR>
hline
 &lt;<br>&amp;Less than
<BR> &gt;<br>&amp;Greater than
<BR> &lt;<br>=&amp;Less than or equal to
<BR> &gt;<br>=&amp;Greater than or equal to
<BR>== &amp;Equal to
<BR>!= &amp;Not equal to
<BR>
endtabular*
begintabular*4in rl
multicolumn2lbf String Comparison Operators 
<BR>
hline
it str1 = it str2&amp;it str1 equals it str2
<BR>
it str1 != it str2&amp;it str1 does not equal it str2
<BR>
it str1  &lt;<br> it str2&amp;it str1 is less than it str2
<BR>
it str1  &gt;<br> it str2&amp;it str1 is greater than it str2
<BR>-n it str1 &amp;it str1 is not null
<BR>-z it str1 &amp;it str1 is null
<BR>
endtabular*
begintabular*4in rl
multicolumn2lbf Arithmetic Operators 
<BR>
hline
+&amp;Plus
<BR>-&amp;Minus
<BR> *<br>&amp;Multiplication
<BR>/&amp;Division
<BR> %<br>&amp;Remainder
<BR> &lt;&lt;<br>&amp;Bit-shift left
<BR> &gt;&gt;<br>&amp;Bit-shift right
<BR> &amp;<br>&amp;Bitwise and
<BR> |<br>&amp;Bitwise or
<BR> ~<br>&amp;Bitwise not
<BR> !<br>&amp;Bitwise not
<BR> ^<br>&amp;Bitwise exclusive or 
<BR>
endtabular*
begintabular*4in rl
multicolumn2lbf Pattern-Matching Operators
<BR>
hline
 ${variable#pattern}<br>&amp;If the pattern matches the
<BR>
multicolumn2lbeginning of the variables value, delete the shortest part
<BR>
multicolumn2lthat matches and return the rest.
<BR> ${variable##pattern}<br>&amp;If the pattern matches the 
<BR>
multicolumn2lbeginning of the variables value, delete the longest part
<BR>
multicolumn2lthat matches and return the rest.
<BR> ${variable%pattern}<br>&amp;If the pattern matches the 
<BR>
multicolumn2lend of the variables value, delete the shortest part
<BR>
multicolumn2lthat matches and return the rest.
<BR> ${variable%%pattern}<br>&amp;If the pattern matches the 
<BR>
multicolumn2lend of the variables value, delete the longest part
<BR>
multicolumn2lthat matches and return the rest.
<BR> ${variable/pattern/string}<br>&amp;
<BR> ${variable//pattern/string}<br>&amp;The longest match 
<BR>
multicolumn2lto pattern in variable is replaced by string. 
<BR>
multicolumn2lIn the first form only the first match replaced.
<BR>
multicolumn2lIn the second form all matches are replaced. If 
<BR>
multicolumn2lThe pattern begins with # the match must 
<BR>
multicolumn2lbe at the begining. % means the match 
<BR>
multicolumn2lmust be at the end.  If the string is null
<BR>
multicolumn2lthen the matches are deleted.
<BR>
endtabular*
<p>


<p>
 <footer class="foot">

                                <h5>HPC Training - University of South Florida</h5>
                        </footer>

</BODY>
</HTML>
