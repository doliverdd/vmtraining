<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<HTML>
<HEAD>
<TITLE>USF HPC Training</TITLE>
<meta name="viewport" content="width=device-width, initial-scale=1">


</HEAD>

<body >


	<h3>HPC Hardware at USF</h3>

	There are too many types of advanced hardware to possibly list them all in this book.  In this  we will discuss the types of hardware that are available for HPC from the Research Computing Core at USF. In particular, we will be looking
	at the aspects of the different systems that will allow you to make a good 
	choice as to which environment is best for the type of problem you are solving. 


	<h4>Beowulf Cluster</h4>

	Traditionally High Performance computing was only done on large proprietary 
	systems. These systems (also known as Big Iron, or super computers)indexBig Iron were expensive,
	highly specialized computers that were capable of very fast computations.  They
	had several problems.  The programming languages and libraries were not usually
	portable, which meant that it was quite expensive to switch between vendors. 
	Each machine required personnel trained specifically for that machine,
	which added to the expense of switching computer companies.  Also when the speed of these machines was no longer fast, they had to be replaced again at great
	expense.  
	<p>
	Beowulf Cluster Computing uses commodity off the shelf hardware (COTS)
	indexCOTS to achieve speeds near those of the Big Iron machines at a much
	lower cost.  To achieve this, standard desktop quality computers are connected
	together over a network.  These computers then work in parallel to speed 
	computation. These cluster computers generally use special 
	libraries to handle communications.  
	<p>
	
	The systems that you will be working on will use the MPICH libraries. 
	Not all computations work well on a cluster computer.  To understand which 
	problems lend themselves to the cluster environment we need to take a look at
	a property of computer programs called granularity.indexgranularity  
	Simply stated, granularity is the ratio between the amount of 
	communications and the amount of computation
	necessary to solve a problem. When this ratio is low, or when there is not
	too much communication, the problem is coarse grained.  If there is a lot of
	communication relative to the amount of computation the program is called fine
	grained.  The cluster environment is best for medium and coarse grained 
	applications. Granularity thus helps us define how effectively 
	a program can be subdivided for parallel execution. Coarse grained programs are often referred to as "embarrassingly 
	parallel". 
	<p>
	The Research Computing Core at USF has a 48 node Beowulf Cluster available for
	students and faculty.  

	
</BODY>
</HTML>
